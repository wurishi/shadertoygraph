{"ver":"0.1","info":{"id":"mlXGW8","date":"1671669265","viewed":71,"name":"backward_mapping_lkg_quilt _2","username":"holophone3d","description":"Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\nClick and hold mouse on the image and move left/right to view synthesized intermediate views\n\n","likes":0,"published":1,"flags":0,"usePreview":0,"tags":["backwardmapping"],"hasliked":0,"parentid":"dls3Dr","parentname":"backward_mapping_lkg_quilt"},"renderpass":[{"inputs":[],"outputs":[{"id":"4dfGRr","channel":0}],"code":"/*\nLOAD THE FOUR TEXTURES BELOW TO RUN THE SHADER:\n\nThe idea is to call directly the SetTexture function found in Shadertoy js code \n(https://www.shadertoy.com/view/lsGGDd)\n\nHere is how to loads the three textures needed for this shader:\n - Open the javascript console of your browser:\n\t\t\t\t   Mac      /     Windows\n\tChrome:  cmd + opt + J  /  ctrl + shift J\n\tFirefox: cmd + opt + K  /  ctrl + shift K\n    Edge:          na         /  ctrl + shift J   \n\n- Then copy the following lines in the console:\n\nHigh depth:\ngShaderToy.SetTexture(0, {mSrc:'https://i.imgur.com/9qZnje5.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(1, {mSrc:'https://i.imgur.com/6dUpegD.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(2, {mSrc:'https://i.imgur.com/l0IuNLP.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(3, {mSrc:'https://i.imgur.com/5WO7KYh.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\n\n- hit return to execute and load the textures.\n\n*/\n\n// DETAILS:\n// Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\n// POC shader to enable 3DS emulator Citra to natively render into Looking Glass hologram display\n// Uses a depth based warping and blending approach to create virtual frames\n// backward mapping - loosely based on the paper below\n// https://www.cc.gatech.edu/conferences/3DPVT08/Program/Papers/paper213.pdf\n\n// USAGE:\n// Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\n// Click and hold mouse on the image and move left/right to view synthesized intermediate views\n\n// NEED SOME MATH HELP/BRAINSTORMING HERE:\n// As you can see, I'm trying to determine the disparity between the rectified RGB+D stereo images\n// In theory, I have enough data to do this with the depth maps because the formula is: disparity = (focalLength * baseline) / depth\n// However, I think the normalized z data (0-1) isn't enough, I think I need it in world space\n// I think I just need a little math insight or brainstormin (maybe guessing at a projection matrix?)\n// Any thoughts or ideas are very welcome!\n\n// pixel depth offset values for high depth images\n// brute force mapping of offsets, need to determine how to generate these from depth information\nconst vec2 highdepthDisparity[9] = vec2[](vec2(0.0, -25.0), vec2(0.58, -20.0), vec2(0.1, -12.0), vec2(0.168, 0.0), vec2(0.196, 12.0), vec2(0.321, 12.0),vec2(0.34, 30.0), vec2(0.82, 57.0), vec2(1.0, 90.0));\n\nfloat get_depth_shift(float depth)\n{   \n    float offset = 0.0;\n   depth = (1.0 - depth);\n   \n     for(int i=0; i < highdepthDisparity.length(); i++)\n    {\n        if(depth <= highdepthDisparity[i].x)\n        {\n            float depth_delta = highdepthDisparity[i-1].x-highdepthDisparity[i].x;\n            float disparity_delta = highdepthDisparity[i-1].y-highdepthDisparity[i].y;\n            offset = highdepthDisparity[i-1].y + (highdepthDisparity[i].x-depth)*(depth_delta/disparity_delta);\n            offset /= iChannelResolution[0].x;\n            break;\n        }\n        i++;\n       }\n   return offset;\n}\n\nvec3 get_Im(vec2 normalized_coords, float alpha)\n{\n    float depth_l = texture(iChannel1, normalized_coords).x;\n    float depth_r = texture(iChannel3, normalized_coords).x;\n    float depth_shifted_x_l = normalized_coords.x - alpha*get_depth_shift(depth_l);\n    float depth_shifted_x_r = normalized_coords.x + (1.0-alpha)*get_depth_shift(depth_r);\n    vec3 left_rgb = (1.0-alpha)*texture(iChannel0, vec2(depth_shifted_x_l, normalized_coords.y)).xyz;\n    vec3 right_rgb = alpha*texture(iChannel2, vec2(depth_shifted_x_r, normalized_coords.y)).xyz;\n    return left_rgb + right_rgb;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{        \n    vec2 normalized_coords = fragCoord.xy/iResolution.xy;\n    float alpha = iMouse.x/iResolution.x; // coefficient 0-1 of camera angle (L-R) DEBUG: use mouse to see warping and blending\n    \n    bool generate_quilt = true; // Looking Glass Mode    \n    bool use_expanded_alpha = true; // compresses the middle field of view to have more depth and creates more virtual frames outside of normal alpha range\n    \n    if (generate_quilt)\n    {\n        // quilt config\n        float cols = 8.0; \n        float rows = 6.0;\n        \n        // alter virtual camera angle for each quilt image\n        float alpha_delta = 1.0/(rows*cols); //step per quilt view\n        float active_row = floor(normalized_coords.y/(1.0/rows));\n        float active_col = floor(normalized_coords.x/(1.0/cols));\n        alpha = active_row*cols*alpha_delta + active_col*alpha_delta; // update alpha virtual camera angle based on quilt view\n              \n        // update coordinates for quilt space\n        normalized_coords = vec2(normalized_coords.x*cols - active_col, normalized_coords.y * rows);\n    }\n    \n    if(use_expanded_alpha)\n    {\n        alpha *= 4.0;\n        alpha -= 2.0;\n    }\n    \n    fragColor = vec4(get_Im(normalized_coords, alpha), 1.0);\n}\n","name":"Image","description":"","type":"image"}]}