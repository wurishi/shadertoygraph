{"ver":"0.1","info":{"id":"3lsfzH","date":"1596016681","viewed":408,"name":"Common Camera Distortions","username":"sdfgeoff","description":"Originally developed for a drone game I was planning to develop. Includes lens distortion, chromatic aberration, sensor noise (static and dynamic) and brightness/sharpness/saturation","likes":6,"published":1,"flags":0,"usePreview":0,"tags":["lens","distort","camera","drone","sensor"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[{"id":"4sX3Rn","filepath":"/media/a/c3a071ecf273428bc72fc72b2dd972671de8da420a2d4f917b75d20e1c24b34c.ogv","previewfilepath":"/media/ap/c3a071ecf273428bc72fc72b2dd972671de8da420a2d4f917b75d20e1c24b34c.ogv","type":"video","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dfGRr","channel":0}],"code":"// Designed to simulate the distortions of cheap cameras\n// such as those found in drones and older action cameras\n//\n// See https://www.youtube.com/watch?v=ypAcqtLELNo \n// \n// License MIT\n\n\n// Lens parameters\nconst vec3 lens_vignette = vec3(0.0, 0.0, 0.0); // r2, r4, r6, same as openCV\nconst vec3 lens_chromatic_aberation = vec3(-0.02, 0.01, 0.0); // r2, r4, r6\nconst vec3 lens_radial_distortion = vec3(0.15, -0.05, 0.0); // r2, r4, r6, same as openCV\nconst vec2 lens_tangental_distortion = vec2(0.0, 0.0);\nconst float lens_zoom_factor = 0.9;\n\n// Sensor parameters\nconst vec2 sensor_resolution = vec2(320.0, 240.0);\nconst vec3 sensor_pixel_noise = vec3(0.02, 0.02, 0.1); // static noise, dynamic noise, dynamic noise speed\nconst float sensor_exposure = 1.0;\nconst float sensor_gain = 1.0;\nconst float sensor_dynamic_range = 0.5;\n\n// Camera board parameters\nconst vec3 processor_sharpness_functions = vec3(2.0, 0.8, 2.0); // sharpness, denoise, radius\nconst float processor_brightness = 0.0;\nconst float processor_gamma = 1.0;\nconst float processor_contrast = 1.0;\nconst float processor_saturation = 1.0;\nconst vec4 processor_white_balance_color = vec4(1.0, 1.0, 1.0, 1.0);\n\n\nfloat rand(vec2 co){\n\t// Generate a random number. This is the well known magic random number generator\n\t// that seems to appear everywhere in GLSL code. In this case, from stack overflow\n\treturn fract(sin(dot(co.xy ,vec2(12.9898,78.233))) * 43758.5453);\n}\n\nvec2 center_coords(vec2 uv){\n\t// It's much nicer to work with the center of the screen being 0,0\n\t// rather than the edge of the screen.\n\treturn (uv - vec2(0.5)) * 2.0;\n}\nvec2 uncenter_coords(vec2 uv){\n\t// Undo center coords\n\treturn (uv * 0.5) + vec2(0.5);\n}\n\n\nvec2 lens_distort_coords(vec2 input_coords){\n\t// Radial distortion. See https://docs.opencv.org/3.4.3/dc/dbb/tutorial_py_calibration.html\n\t// This code is based on modules/calib3d/src/undistort.cpp in the function\n\t// cvUndistortPointsInternal where they do an iterative undistort that compares the\n\t// real values with computed distorted ones. The changes are mostly to try make\n\t// it obvious to the GLSL compiler what we are doing in the hope it can optimize the\n\t// multiplies more effectively. This should be disassembled/tested at some point.\n\tfloat x = input_coords.x;\n\tfloat y = input_coords.y;\n\t\n\tfloat r2 = dot(input_coords, input_coords);  // u.u = |u|^2\n\tfloat r4 = r2 * r2;\n\tfloat r6 = r2 * r4;\n\t\n\tfloat a1 = 2.0 * x * y;\n\tfloat a2 = r2 + 2.0 * x * x;\n\tfloat a3 = r2 + 2.0 * y * y;\n\tfloat cdist = 1.0 + dot(lens_radial_distortion, vec3(r2, r4, r6));\n\tvec2 tangental_dist = vec2(\n\t\tdot(lens_tangental_distortion, vec2(a1, a2)),\n\t\tdot(lens_tangental_distortion, vec2(a3, a1))\n\t);\n\tvec2 distort_amt = vec2(cdist) + tangental_dist;\n\n\treturn input_coords * distort_amt * lens_zoom_factor;\n}\n\n\nvec4 sample_photons(sampler2D tex, vec2 coords){\n\t/* Samples the photons as they arrive at the enterance\n\tto the lens. Not quite true because we don't use a \n\treal lens model, but close enough.... */\n\tvec2 texture_coords = uncenter_coords(coords);\n\treturn texture(tex, texture_coords, 1.0);\n}\n\nvec4 sample_lens(sampler2D tex, vec2 coords){\n\t/* Sample the light at a particular point inside the lens\n\tThis includes all lens artifacts such as vignetting, \n\tchromatic aberation, lens distortion, and in the future jello. */\n\tvec4 outp;\n\tfloat r2 = dot(coords, coords);  // u.u = |u|^2\n\tfloat r4 = r2 * r2;\n\tfloat r6 = r4 * r2;\n\tvec3 r2r4r6 = vec3(r2, r4, r6);\n\t\n\tfloat vignette_factor = dot(lens_vignette, r2r4r6);\n\t\n\t// TODO: add jello\n\t\n\tif (length(lens_chromatic_aberation) == 0.0){\n\t\toutp = sample_photons(tex, lens_distort_coords(coords));\n\t} else {\n\t\tfloat abr = dot(lens_chromatic_aberation, r2r4r6);// * rand(UV);\n\t\toutp = vec4(\n\t\t\tsample_photons(tex, lens_distort_coords(coords*(1.0 - abr*3.0))).r,\n\t\t\tsample_photons(tex, lens_distort_coords(coords*(1.0 - abr*2.0))).g,\n\t\t\tsample_photons(tex, lens_distort_coords(coords*(1.0 - abr*1.0))).b,\n\t\t\t1.0\n\t\t);\n\t}\n\toutp.rgb *= (1.0 + vignette_factor);\n\treturn outp;\n}\n\nvec4 sample_sensor(sampler2D tex, vec2 coords){\n\t// Rounding sample positions technically isn't needed because the sensor viewport \n\t// is the resolution of the camera. I thought I'd leave this in just in case \n\t// something upstream tries to sample in the wrong place.\n\tvec2 pixel_position = coords;\n\tpixel_position.x -= mod(coords.x, 2.0 / sensor_resolution.x);\n\tpixel_position.y -= mod(coords.y, 2.0 / sensor_resolution.y);\n\t\n\t// TODO: improve noise type and include hue variation in noise\n\tvec4 raw_sample = sample_lens(tex, pixel_position);\n\traw_sample *= sensor_exposure;\n\traw_sample += dot(sensor_pixel_noise.xy, vec2(\n\t\trand(coords) - 0.5,  // Static Noise\n\t\trand(coords+vec2(0.0, iTime*sensor_pixel_noise.z)) - 0.5 // Dynamic noise\n\t));\n\t\n\t// TODO: Should this happen before or after bit limiting? I think it's before\n\t// because I think it's done in the analog stage of the camera\n\traw_sample *= sensor_gain;\n\t\n\t// ARRGH Godot!!!!\n\t//raw_sample = (raw_sample - 0.5) * sensor_dynamic_range + 0.5;\n\t\n\t// This set of operations removes the precision from the sensor\n\t// Most cameras transfer data in RGB565 or YUV422 or some other\n\t// varient that reduces the 3 bytes of color to two bytes.\n\t// So this is the stage where HDR is lost\n\t// In this case, for simplicity I assume all channels have 5 bits (RGB555)\n\traw_sample = clamp(raw_sample, 0.0, 1.0);\n\traw_sample -= mod(raw_sample, 1.0 / pow(2.0, 5.0));\n\t\n\treturn raw_sample;\n}\n\n\n\n\nvec4 sample_camera_board(sampler2D tex, vec2 coords){\n\tfloat blur_pixel_radius = processor_sharpness_functions.z;\n\tfloat sharpness = processor_sharpness_functions.x;\n\tfloat denoise = processor_sharpness_functions.y;\n\tvec2 pixel_size = 2.0 / sensor_resolution * blur_pixel_radius;\n\tvec4 sample_1 = sample_sensor(tex, coords);\n\tvec4 blurred = vec4(0.0);\n\t\n\t// Four blur samples - with CA this gives 15 samples total\n\t// Four is the minimum to get a \"full\" sharpen effect where\n\t// all sides of an edge are lightened/darkened.\n\t// In theory you may be able to get it with three, but in\n\t// practice it still missed some edges.\n\tblurred += sample_sensor(tex, coords + pixel_size * vec2(1.0, 1.0));\n\tblurred += sample_sensor(tex, coords + pixel_size * vec2(-1.0, 1.0));\n\tblurred += sample_sensor(tex, coords + pixel_size * vec2(-1.0, -1.0));\n\tblurred += sample_sensor(tex, coords + pixel_size * vec2(1.0, -1.0));\n\tblurred /= 4.0;\n\t\n\t// Three blur samples - with CA this gives 12 samples total\n\t// This is one I wouldn't use. It adds little over two blur samples\n\t//blurred += sample_sensor(tex, coords + pixel_size * vec2(1.0, 0.0));\n\t//blurred += sample_sensor(tex, coords + pixel_size * vec2(1.0, 1.0));\n\t//blurred += sample_sensor(tex, coords + pixel_size * vec2(-1.0, 1.0));\n\t//blurred /= 3.0;\n\t\n\t// Two blur samples - with CA this gives 9 samples total\n\t//blurred += sample_sensor(tex, coords + pixel_size * vec2(1.0, -1.0));\n\t//blurred += sample_sensor(tex, coords + pixel_size * vec2(1.0, 1.0));\n\t//blurred /= 2.0;\n\t\n\t// The basic of a sharken is to find the edges and \"increase\" them\n\tvec4 edges = sample_1 - blurred;\n\tvec4 sharpened = mix(sample_1, sample_1 + edges, sharpness);\n\t\n\t// This is used to control a selective blur. Where it /isn't/ an\n\t// edge, blur it. Where it is an edge, sharpen it. This denoises\n\t// areas of roughly the same color, but leaves edges sharp.\n\tfloat sharpen_or_blur = (1.0 - length(abs(edges))) * denoise;\n\tvec4 processed = mix(sharpened, blurred, sharpen_or_blur);\n\t\n\tprocessed += processor_brightness;\n\tprocessed = pow(processed, vec4(1.0/processor_gamma));\n\tprocessed = (processed - 0.5) * processor_contrast + 0.5;\n\tprocessed = mix(vec4(dot(processed.rgb, vec3(0.2125, 0.7154, 0.0721))), processed, vec4(processor_saturation));\n\tprocessed = processed / processor_white_balance_color;\n\t\n\treturn processed;\n}\n\n\nvoid mainImage(out vec4 fragColor, vec2 fragCoord){\n    vec2 uv = fragCoord/iResolution.xy;\n    \n    if (uv.x > 0.5) {\n        vec2 coords = center_coords(uv);\n        fragColor = sample_camera_board(iChannel0, coords);\n        fragColor.a = 1.0;\n    } else {\n        fragColor = texture(iChannel0, uv);\n    }\n}","name":"Image","description":"","type":"image"}]}