{"ver":"0.1","info":{"id":"msySDy","date":"1681529888","viewed":93,"name":"3ds reverse projected lightfield","username":"holophone3d","description":"Synthesizes a lightfield all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\nClick and hold mouse on the image and move left/right to view synthesized intermediate views in the lightfield\n","likes":0,"published":1,"flags":0,"usePreview":0,"tags":["backwardmapping"],"hasliked":0,"parentid":"dsySWy","parentname":"backmap_matrix_2"},"renderpass":[{"inputs":[],"outputs":[{"id":"4dfGRr","channel":0}],"code":"/*\nINSTRUCTIONS:\nLOAD THE FOUR TEXTURES BELOW TO RUN THE SHADER:\n\nThe idea is to call directly the SetTexture function found in Shadertoy js code \n(https://www.shadertoy.com/view/lsGGDd)\n\nHere is how to loads the three textures needed for this shader:\n - Open the javascript console of your browser:\n\t\t\t\t   Mac      /     Windows\n\tChrome:  cmd + opt + J  /  ctrl + shift J\n\tFirefox: cmd + opt + K  /  ctrl + shift K\n    Edge:          na         /  ctrl + shift J   \n\n- Then copy the following lines in the console:\n\nSuper Mario 3D world (3DS) - RGB+D\n\ngShaderToy.SetTexture(0, {mSrc:'https://i.imgur.com/WyfxrfJ.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(1, {mSrc:'https://i.imgur.com/knCRQt5.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(2, {mSrc:'https://i.imgur.com/v961sp9.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(3, {mSrc:'https://i.imgur.com/xcgUn8T.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\n\n- hit return to execute and load the textures.\n\n*/\n\n// DETAILS:\n// Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\n// POC shader to enable 3DS emulator Citra to natively render into Looking Glass hologram display\n// Uses a depth based warping and blending approach to create virtual frames\n// backward mapping - loosely based on the paper below\n// https://www.cc.gatech.edu/conferences/3DPVT08/Program/Papers/paper213.pdf\n\n// USAGE:\n// Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\n// Click and hold mouse on the image and move left/right to view synthesized intermediate views\n// Set 'generate_quilt' in Main to use with a LookingGlass\n\n// NEED SOME MATH HELP/BRAINSTORMING HERE:\n// As you can see, I'm trying to determine the disparity between the rectified RGB+D stereo images\n// In theory, I have enough data to do this with the depth maps because the formula is: disparity = (focalLength * baseline) / depth_distance\n// There's some artifacts, likely due to some depth compression issues, but overall it seems like it might work\n// Any thoughts or ideas are very welcome!\n\n\nfloat get_depth_shift(vec2 normalized_coords, float depth_value) {\n    \n    // Define a reference resolution and scale factor\n    vec2 reference_resolution = vec2(800.,450.);\n    vec2 scale_factor = iResolution.xy / reference_resolution;\n    \n    // Set the parameters based on the reference resolution in meters and scale them\n    // These parameters are kind of reasonable but were just guessed based on observation of what looks good at 800x450\n    float iNear = 30.0 * scale_factor.x;\n    float iFar = 80.0 * scale_factor.x;\n    float focal_length = 35.0 * scale_factor.x;\n    float baseline = 0.07 * scale_factor.x;\n    \n    // Compute the distance between the left and right pixels in pixels\n    float depth_distance = depth_value * (iFar - iNear) + iNear;\n    float disparity = (focal_length * baseline) / depth_distance;\n    float distance_pixels = (2.0 * focal_length * baseline) / (disparity * iResolution.x);\n\n    // Return the distance between the left and right pixels in pixels\n    return distance_pixels;\n}\n\nvec3 get_Im(vec2 normalized_coords, float alpha, float focus_offset)\n{\n\n    // default depth is inverted so fix that\n    float depth_l = 1. - texture(iChannel1, normalized_coords).x;\n    float depth_r = 1. - texture(iChannel3, normalized_coords).x;\n    \n    // compute normalized shifted position    \n    float depth_shifted_x_l = normalized_coords.x - alpha*get_depth_shift(normalized_coords, depth_l) + focus_offset;\n    float depth_shifted_x_r = normalized_coords.x + (1.0-alpha)*get_depth_shift(normalized_coords, depth_r) + focus_offset;\n    \n    // fix screen edge occlusion\n    if (depth_shifted_x_l < 0.0)\n    {\n        alpha = 1.0;\n    }\n    else if (depth_shifted_x_r > 1.0)\n    {\n        alpha = 0.0;\n    }\n    \n    // sample alpha mixed pixels at shifted positions \n    vec3 left_rgb = (1.0-alpha)*texture(iChannel0, vec2(depth_shifted_x_l, normalized_coords.y)).xyz;\n    vec3 right_rgb = alpha*texture(iChannel2, vec2(depth_shifted_x_r, normalized_coords.y)).xyz;\n    return left_rgb + right_rgb;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{        \n    vec2 normalized_coords = fragCoord.xy/iResolution.xy;\n    float alpha = iMouse.x/iResolution.x; // coefficient 0-1 of camera angle (L-R) DEBUG: use mouse to see warping and blending\n    float focus_offset = 0.0;\n    \n    bool generate_quilt = false; // Looking Glass Mode    \n    bool use_expanded_alpha = false; // compresses the middle field of view to have more depth and creates more virtual frames outside of normal alpha range\n    \n    if (generate_quilt)\n    {\n        // quilt config\n        float cols = 8.0; \n        float rows = 6.0;\n        float view_count = rows*cols;\n        \n        // alter virtual camera angle for each quilt image\n        float alpha_delta = 1.0/(view_count); //step per quilt view\n        float active_row = floor(normalized_coords.y/(1.0/rows));\n        float active_col = floor(normalized_coords.x/(1.0/cols));\n        alpha = active_row*cols*alpha_delta + active_col*alpha_delta; // update alpha virtual camera angle based on quilt view\n        \n        // EXPERIMENTAL FOCUS\n        float focus_val = (iMouse.y/iResolution.y)/5.; // coefficient 0-1 of focus\n        \n        // map the view_num to -1 to 1 (0 is leftmost n is righmost)\n        float view_num = active_row*cols + active_col;\n        float focusAdjust = view_num / view_count * 2.0f - 1.0f;\n        // multiply by the focus value (normally a user accessable slider)\n        focusAdjust *= focus_val;\n        // multiply by the width to get the xOffset for where to sample from the current view image\n        focus_offset = focusAdjust * (1./iResolution.x); // not needed since we're already in normalized coordinates\n        \n        // update coordinates for quilt space\n        normalized_coords = vec2((normalized_coords.x*cols - active_col) + focusAdjust, normalized_coords.y * rows);\n\n    }    \n    if(use_expanded_alpha)\n    {\n        alpha *= 2.0;\n        alpha -= 0.5;\n    }\n    \n    fragColor = vec4(get_Im(normalized_coords, alpha, focus_offset), 1.0);\n}","name":"Image","description":"","type":"image"}]}