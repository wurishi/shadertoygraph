{"ver":"0.1","info":{"id":"l3ySzc","date":"1720698910","viewed":60,"name":"Inverse Sqrt Approximation","username":"TheTurk","description":"Improved version of the fast inverse square root algorythm using differentiable programming. The PyTorch code used to generate the approximations can be found under the \"Common\" tab.","likes":5,"published":1,"flags":0,"usePreview":0,"tags":["gradient","approximation","square","sqrt","inverse","float","quake","root","programming","descent","floatingpoint","differentiable"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[],"outputs":[{"id":"4dfGRr","channel":0}],"code":"/* \nOriginal Quake 3 version with magic constant 0x5f3759df \nRelative Error: 0.00092950\nMaximum Relative Error: 0.00175222\n*/\n\nfloat inverseSquareRoot1(float x) {\n    float y = intBitsToFloat(0x5f3759df - (floatBitsToInt(x) >> 1));\n    y = y * (1.5 - 0.5 * x * y * y);\n    return y;\n}\n\n/*\nMagic constant adjusted via differential programming, very close to the original constant with similar accuracy\nRelative Error: 0.00092826\nMaximum Relative Error: 0.00175125\n*/\n\nfloat inverseSquareRoot2(float x) {\n    float y = intBitsToFloat(1597463153 - (floatBitsToInt(x) >> 1));\n    y = y * (1.5 - 0.5 * x * y * y);\n    return y;\n}\n\n/*\nMagic constant and additional parameters in the Newton step adjusted via differential programming for improved accuracy\nRelative Error: 0.00040869\nMaximum Relative Error: 0.00066020\n*/\n\nfloat inverseSquareRoot3(float x) {\n    float y = intBitsToFloat(1596238211 - (floatBitsToInt(x) >> 1));\n    y = y * (1.642174020409584 - 0.6552134603261948 * x * y * y);\n    return y;\n}\n\nfloat scale = 0.1;\n\nfloat segment(vec2 position, vec2 start, vec2 end) {\n    vec2 p = position - start;\n    vec2 e = end - start;\n    float h = clamp(dot(p, e) / dot(e, e), 0.0, 1.0);\n    return length(p - e * h);\n}\n\nfloat plot1(vec2 position) {\n    float stepSize = (1.0 / iResolution.y) / scale;\n    float d = float(0xffffffffu);  \n    for (int i = -10; i < 10; i++) {\n        float x1 = round(position.x / stepSize) * stepSize + stepSize * float(i) + 0.001;\n        if (x1 < 0.0) {\n            continue;\n        }\n        float y1 = 1.0 / sqrt(x1);\n        float x2 = x1 + stepSize;\n        float y2 = 1.0 / sqrt(x2);\n        d = min(d, segment(position, vec2(x1, y1), vec2(x2, y2)));\n    }\n    return d;\n}\n\nfloat plot2(vec2 position) {\n    float stepSize = (1.0 / iResolution.y) / scale;\n    float d = float(0xffffffffu);  \n    for (int i = -10; i < 10; i++) {\n        float x1 = round(position.x / stepSize) * stepSize + stepSize * float(i) + 0.001;\n        if (x1 < 0.0) {\n          continue;\n        }\n        float y1 = inverseSquareRoot3(x1);\n        float x2 = x1 + stepSize;\n        float y2 = inverseSquareRoot3(x2);\n        d = min(d, segment(position, vec2(x1, y1), vec2(x2, y2)));\n    }\n    return d;\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord ) {\t\n    vec2 uv = fragCoord / iResolution.y;\n    uv /= scale;\n    uv -= 1.0;\n    vec3 color = vec3(1.0);\n        color -= max(step(floor(mod(fragCoord.x, iResolution.y * scale)), 0.5), step(floor(mod(fragCoord.y, iResolution.y * scale)), 0.5)) * 0.2;\n    if (iMouse.z > 0.0) {\n        float d1 = (plot1(uv) - 0.005 / scale) * scale;\n        color = mix(color, vec3(0.0, 0.0, 1.0), 1.0 - smoothstep(0.0, 3.0, d1 * iResolution.y));\n    } else {\n        float d2 = (plot2(uv) - 0.005 / scale) * scale;\n        color = mix(color, vec3(1.0, 0.0, 0.0), 1.0 - smoothstep(0.0, 3.0, d2 * iResolution.y));\n    }\n    fragColor = vec4(color, 1.0);\n}","name":"Image","description":"","type":"image"},{"inputs":[],"outputs":[],"code":"/*\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass EvaluationMetrics:\n    \n    @staticmethod\n    def relative_error(output, target):\n        relative_error = torch.abs(target - output) / torch.abs(target)  \n        return torch.mean(relative_error)\n    \n    @staticmethod\n    def relative_squared_error(output, target):\n        relative_squared_error = ((target - output) ** 2) / (target ** 2)\n        return torch.mean(relative_squared_error)\n    \n    @staticmethod\n    def maximum_relative_error(output, target):\n        relative_error = torch.abs(target - output) / torch.abs(target)\n        return torch.max(relative_error)\n        \n# Since the bit-level hack is not differentiable, we approximate the gradient during the backward pass using a straight-through estimator.\n\nclass DifferentiableBitHack(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, weight):\n        return (0x5f400000 - (input.view(torch.int32) >> 1) + torch.round(weight * 4194304.0).int()).view(torch.float32)\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output, grad_output\n\ndifferentiable_bit_hack = DifferentiableBitHack.apply\n\n# The expression intBitsToFloat(0x5f400000 - (floatBitsToInt(x) >> 1)) provides an initial estimate for 1 / sqrt(x). It is derived as follows:\n# We assume the integer representation of a float x is given by:\n# floatBitsToInt(x) = (exponent + 127) * pow(2, 23) + mantissa * pow(2, 23)\n# For simplicity, we ignore the mantissa. \n# The exponent can then be calculated as:\n# exponent = floatBitsToInt(x) / pow(2, 23) - 127\n# To compute 1 / sqrt(x) we adjust the exponent by multiplying by -0.5:\n# newExponent = -0.5 * exponent\n# newExponent = -0.5 * floatBitsToInt(x) / pow(2, 23) + 63.5\n# We then compute the new float with the new exponent:\n# y = intBitsToFloat((newExponent + 127) * pow(2, 23))\n# y = intBitsToFloat(190.5 * pow(2, 23) - 0.5 * floatBitsToInt(x))\n# y = intBitsToFloat(0x5f400000 - (floatBitsToInt(x) >> 1))\n\nclass InverseSquareRoot(nn.Module):\n    def __init__(self):\n        super(InverseSquareRoot, self).__init__()\n        self.weight1 = nn.Parameter(torch.tensor([0.0], dtype=torch.float32))\n        self.weight2 = nn.Parameter(torch.tensor([0.0], dtype=torch.float32))\n        self.weight3 = nn.Parameter(torch.tensor([0.0], dtype=torch.float32))\n \n    def forward(self, x):\n        y = differentiable_bit_hack(x, self.weight1)\n        y = y * ((1.5 + self.weight2) - (0.5 + self.weight3) * x * y * y)\n        return y \n\ndef train(model, data_loader, optimizer, scheduler):\n    for input, target in data_loader:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = EvaluationMetrics.maximum_relative_error(output, target)\n        # loss = EvaluationMetrics.relative_squared_error(output, target)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()        \n\ndef evaluate(model, data_loader):\n    model.eval()\n    relative_error = 0.0\n    relative_squared_error = 0.0\n    maximum_relative_error = 0.0\n    with torch.no_grad():\n        for input, target in data_loader:\n            output = model(input)\n            maximum_relative_error = max(maximum_relative_error, EvaluationMetrics.maximum_relative_error(output, target).item())\n            relative_error += EvaluationMetrics.relative_error(output, target).item()\n            relative_squared_error += EvaluationMetrics.relative_squared_error(output, target).item()\n    return relative_error / len(data_loader), relative_squared_error / len(data_loader), maximum_relative_error\n\ndef plot(model):\n    model.eval()\n    input = torch.tensor(np.linspace(0.01, pow(2.0, 8.0), 2000), dtype=torch.float32)\n    actual = torch.rsqrt(input)\n    predicted = model(input).detach().numpy()\n    plt.plot(input.numpy(), actual.numpy(), \"bo\", markersize=3.0, label=\"Actual\")\n    plt.plot(input.numpy(), predicted, \"ro\",  markersize=3.0, label=\"Predicted\")\n    plt.legend()\n    plt.show()\n    \ndef create_dataset(size, minimum_value, maximum_value):\n    input = torch.rand(size) * (maximum_value - minimum_value) + minimum_value\n    target = torch.rsqrt(input)\n    return TensorDataset(input, target)\n\ntraining_size = 1000000\nvalidation_size = 100000\nbatch_size = 2048\nlearning_rate = 0.1\ntraining_dataset = create_dataset(training_size, pow(2.0, -64.0), pow(2.0, 64.0))\nvalidation_dataset = create_dataset(validation_size, pow(2.0, -126.0), pow(2.0, 127.0))\ntraining_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\nminimumError = float(\"inf\")\nfor _ in range(1):\n    model = InverseSquareRoot()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(training_loader), epochs=1)\n    train(model, training_loader, optimizer, scheduler)\n    relative_error, relative_squared_error, maximum_relative_error = evaluate(model, validation_loader)\n    if maximum_relative_error > minimumError:\n        continue\n    minimumError = maximum_relative_error    \n    print(f\"Training Samples: {len(training_loader.dataset)}\")\n    print(f\"Batch Size: {training_loader.batch_size}\")\n    print(f\"Learning Rate: {learning_rate}\")  \n    print(f\"Weight 1: {model.weight1.item()}\")\n    print(f\"Weight 2: {model.weight2.item()}\")\n    print(f\"Weight 3: {model.weight3.item()}\")\n    print(f\"Relative Error: {relative_error:.8f}\")\n    print(f\"Relative Squared Error: {relative_squared_error:.8f}\")\n    print(f\"Maximum Relative Error: {maximum_relative_error:.8f}\")\n    print(\"--\")\n    constant = 0x5f400000 + round(model.weight1.item() * 4194304.0)\n    print(f\"float y = intBitsToFloat({constant} - (floatBitsToInt(x) >> 1));\")\n    print(f\"y = y * ({1.5 + model.weight2.item()} - {0.5 + model.weight3.item()} * x * y * y);\")\n    print(\"return y;\")\n    print(\"--\")\nplot(model)\n*/","name":"Common","description":"","type":"common"}]}