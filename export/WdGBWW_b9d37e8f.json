{"ver":"0.1","info":{"id":"WdGBWW","date":"1607069355","viewed":158,"name":"Sphere Fractal Cave Test 3","username":"sdfgeoff","description":"Same as SphereFractalCave but with iterative cone marching. Designed to run at any resolution where the pixel count is divisible by 8 and still an integer (eg 1920*1080). Click to see the progressive buffers. ","likes":3,"published":1,"flags":48,"usePreview":0,"tags":["interactive","cave","fly","conemarch"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"XsXGR8","filepath":"/media/previz/buffer01.png","previewfilepath":"/media/previz/buffer01.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dfGRr","channel":0}],"code":"/// Convert from a buffer of normals/depth into something pretty\n#define BUFFER_WORLD iChannel0\n#define BUFFER_STATE iChannel1\n\n\n\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    \n    //vec2 screen_coords = (fragCoord/iResolution.xy);\n    //vec4 world = texture(WORLD_BUFFER, screen_coords);\n    vec2 uv = (fragCoord/iResolution.xy);\n    \n    // sample previous tile\n    vec4 previous = sample_tile(BUFFER_WORLD, iResolution.xy, uv, 1.0);\n    \n    \n    mat4 camera_transform = quat_to_transform(\n        read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION),\n        read_data(BUFFER_STATE, ADDR_CAMERA_POSITION).xyz\n    );\n    \n\n    // Output to screen\n    vec4 world = raymarch(\n        iResolution.xy,\n        uv,\n        previous,\n        camera_transform\n    );\n    \n    \n    vec3 surface_normal = world.rgb;\n    float dist = world.a;\n    \n    float lighting = dot(surface_normal, vec3(0.5));\n    \n    float fog = world.a * 0.25;//pow(dist, 2.0);\n    \n    vec3 col = vec3(0.5, 0.6, 0.8);\n    col *= lighting;\n    col = mix(col, vec3(1.0, 0.9, 0.8), fog);\n    \n    fragColor = vec4(col, 1.0);\n    \n    if (iMouse.z > 0.0) {\n        //fragColor = sample_tile(iChannel0, iResolution.xy, uv, 1.0);\n        fragColor = texture(BUFFER_WORLD, uv);\n    }\n}\n","name":"Image","description":"","type":"image"},{"inputs":[{"id":"4dXGRr","filepath":"/presets/tex00.jpg","previewfilepath":"/presets/tex00.jpg","type":"keyboard","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dXGR8","channel":0}],"code":"// STATE\n\n#define BUFFER_STATE iChannel0\n#define BUFFER_KEYBOARD iChannel1\n\n\n// Return the state of a key\nfloat get_key(int key_code) {\n    return texelFetch(BUFFER_KEYBOARD, ivec2(key_code,0), 0).x;\n}\n\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    ivec2 address = ivec2(fragCoord);\n    \n    if (address == ADDR_CAMERA_POSITION) {\n        // Move the camera based on keypress\n    \tvec4 camera_position = read_data(BUFFER_STATE, ADDR_CAMERA_POSITION);\n        \n        if (iTime < 0.1) {\n            camera_position = vec4(0.0, 0.0, 7.0, 0.0);\n        }\n        \n        \n        vec3 translation = read_data(BUFFER_STATE, ADDR_CAMERA_LIN_VELOCITY).xyz;\n        \n        // Convert to local coordinate system\n        mat4 orientation = quat_to_transform(\n            read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION),\n            vec3(0.0)\n        );\n        translation = (orientation * vec4(translation, 0.0)).xyz;\n        translation *= iTimeDelta;\n        \n        camera_position.xyz += translation;\n        fragColor = camera_position;\n        return;\n    }\n    \n    \n    if (address == ADDR_CAMERA_ORIENTATION) {\n        // Rotate the camera based on keypress\n        vec4 camera_orientation = read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION);\n        \n        if (iTime < 0.1) {\n            camera_orientation = quat_from_axis_angle(vec3(0.0, 1.0, 0.0), 1.5);\n        }\n        \n        vec4 velocity = read_data(BUFFER_STATE, ADDR_CAMERA_ANG_VELOCITY);\n        velocity *= iTimeDelta;\n        \n        \n        \n        vec4 pan = quat_from_axis_angle(vec3(0.0, 1.0, 0.0), velocity.x);\n        vec4 tilt = quat_from_axis_angle(vec3(1.0, 0.0, 0.0), velocity.y);\n        vec4 roll = quat_from_axis_angle(vec3(0.0, 0.0, 1.0), velocity.z);\n        \n        \n        camera_orientation = quat_mul(pan, camera_orientation); \n        camera_orientation = quat_mul(tilt, camera_orientation); \n        camera_orientation = quat_mul(roll, camera_orientation); \n        \n        fragColor = camera_orientation;\n        return;\n    }\n    if (address == ADDR_CAMERA_ANG_VELOCITY) {\n        vec4 velocity = read_data(BUFFER_STATE, ADDR_CAMERA_ANG_VELOCITY);\n        \n        vec3 acceleration = vec3(\n            get_key(KEY_PAN_LEFT) - get_key(KEY_PAN_RIGHT),\n            get_key(KEY_TILT_UP) - get_key(KEY_TILT_DOWN),\n            get_key(KEY_ROLL_RIGHT) - get_key(KEY_ROLL_LEFT)\n        ) * 10.0;\n        velocity.xyz += acceleration * iTimeDelta;\n        \n        vec4 drag = velocity * 10.0;\n        velocity -= drag * iTimeDelta;\n        \n        fragColor = velocity;\n        return;\n    }\n    if (address == ADDR_CAMERA_LIN_VELOCITY) {\n        vec4 velocity = read_data(BUFFER_STATE, ADDR_CAMERA_LIN_VELOCITY);\n        \n        vec3 acceleration = vec3(\n            get_key(KEY_RIGHT) - get_key(KEY_LEFT),\n            get_key(KEY_UP) - get_key(KEY_DOWN),\n            get_key(KEY_FORWARD) - get_key(KEY_BACKWARD)\n        ) * 5.0;\n        velocity.xyz += acceleration * iTimeDelta;\n        \n        vec4 drag = velocity * 5.0;\n        velocity -= drag * iTimeDelta;\n        \n        fragColor = velocity;\n        return;\n    }\n}\n\n\n","name":"Buffer A","description":"","type":"buffer"},{"inputs":[],"outputs":[],"code":"const ivec2 ADDR_CAMERA_POSITION = ivec2(0,0);\nconst ivec2 ADDR_CAMERA_ORIENTATION = ivec2(0,1);\nconst ivec2 ADDR_CAMERA_ANG_VELOCITY = ivec2(0,2);\nconst ivec2 ADDR_CAMERA_LIN_VELOCITY = ivec2(0,3);\n\nconst float LENS = 0.5;\n\nconst int KEY_LEFT = 65;\nconst int KEY_UP   = 82;\nconst int KEY_RIGHT = 68;\nconst int KEY_DOWN = 70;\nconst int KEY_FORWARD = 87;\nconst int KEY_BACKWARD = 83;\n\nconst int KEY_TILT_UP = 38;\nconst int KEY_TILT_DOWN = 40;\nconst int KEY_PAN_LEFT = 37;\nconst int KEY_PAN_RIGHT = 39;\nconst int KEY_ROLL_LEFT = 81;\nconst int KEY_ROLL_RIGHT = 69;\n\nconst float MAX_TILES = 3.0;\n\n\n\n// Fetch a single pixe from a buffer\nvec4 read_data(sampler2D buffer, ivec2 address){\n    return texelFetch(buffer, address, 0);\n}\n\n\n\n// A surface is defined by a vec4. The rgb channels are the normal, the alpha\n// is the distance to the surface. This makes physics etc. very easy\nvec4 sphere_sdf(vec3 query_point, float sphere_radius) {\n    float l = length(query_point);\n    float df = l - sphere_radius;\n    \n    return vec4(\n        query_point / l,\n        df\n    );\n}\n\nvec4 surface_intersect(vec4 surface_1, vec4 surface_2) {\n    // Find the volume both surface occupy\n    if (surface_1.a > surface_2.a){\n        return surface_1;\n    } else {\n     \treturn surface_2;   \n    }\n}\nvec4 surface_difference(vec4 surface_1, vec4 surface_2) {\n    // subtract surface_2 from surface_1\n    // invert surface 2:\n    surface_2.a = -surface_2.a;\n    surface_2.xyz = -surface_2.xyz;\n    return surface_intersect(surface_1, surface_2);\n}\n\n\nvec4 distance_field(vec3 co) {\n    float rad = clamp(co.z, 0.0, 10.0) * 0.05 + 0.4;\n    //co = mod(co, vec3(1.0)) - 0.5;\n    co = (co - clamp(round(co), -5.0, 5.0));\n    return sphere_sdf(co, rad);\n}\n\n// A simple fractal-like made by iterative transformation\nvec4 sample_world(vec3 world_position) {\n\n    vec4 body = vec4(vec3(0.0), -9990.0);\n    vec3 co = world_position;\n\n    mat4 m = mat4(\n\t\tvec4(0.6373087, -0.0796581,  0.7664804, 0.0),\n  \t\tvec4(0.2670984,  0.9558195, -0.1227499, 0.0),\n  \t\tvec4(-0.7228389,  0.2829553,  0.6304286, 0.0),\n        vec4(0.3, 0.2, 0.2, 0.0)\n    );\n    \n    for (int i=0; i<3; i++) {\n        co = (m * vec4(co, float(i))).xyz;\n     \tbody = surface_difference(body, distance_field(co));\n    }\n    \n    return body;\n}\n\n\nvec4 raymarch(vec2 resolution, vec2 uv, vec4 start_data, mat4 camera_transform) {\n    int steps = 16;\n    \n    // Convert to range (-1, 1) and correct aspect ratio\n    vec2 screen_coords = (uv - 0.5) * 2.0;\n    screen_coords.x *= resolution.x / resolution.y;\n    \n    \n    vec3 ray_start_position = camera_transform[3].xyz;\n    \n    vec3 ray_direction = normalize(vec3(screen_coords * LENS, 1.0));\n    ray_direction = (camera_transform * vec4(ray_direction, 0.0)).xyz;\n    \n    \n    float dist = start_data.a * 0.9;\n    vec3 sample_point = ray_start_position + dist * ray_direction;\n    \n    vec4 results = sample_world(sample_point);\n    \n    float tolerance = 0.0;\n    \n    for (int i=0; i<steps; i += 1) {\n        dist += results.a;\n        sample_point += ray_direction * results.a;\n        results = sample_world(sample_point);\n        \n        // TODO: Derive from resolution, camera lens and distance\n    \ttolerance = LENS / resolution.x * dist;\n        \n        if (results.a < tolerance) {\n        \tbreak; \n        }\n    }\n    \n    vec4 data = vec4(\n        vec3(results) * 0.5 + 0.5,\n        dist\n    );\n    \n    return data;\n}\n\n\n\n// Create a quaternion from axis-angle notation\nvec4 quat_from_axis_angle(vec3 axis, float angle) {\n    float factor = sin(angle) / 2.0;\n    float w = cos(angle) / 2.0;\n    return normalize(vec4(axis*factor, w));\n}\n\n// Convert a quaternion into a transformation matrix\nmat4 quat_to_transform(vec4 quat, vec3 translation) {\n    float qx = quat.x;\n    float qy = quat.y;\n    float qz = quat.z;\n    float qw = quat.w;\n    float qx2 = qx * qx;\n    float qy2 = qy * qy;\n    float qz2 = qz * qz;\n    \n \treturn mat4(\n        1.0 - 2.0*qy2 - 2.0*qz2,\t2.0*qx*qy - 2.0*qz*qw,\t2.0*qx*qz + 2.0*qy*qw, 0.0,\n    \t2.0*qx*qy + 2.0*qz*qw,\t1.0 - 2.0*qx2 - 2.0*qz2,\t2.0*qy*qz - 2.0*qx*qw, 0.0,\n    \t2.0*qx*qz - 2.0*qy*qw,\t2.0*qy*qz + 2.0*qx*qw,\t1.0 - 2.0*qx2 - 2.0*qy2, 0.0,\n        translation, 0.0\n    );\n}\n\n// Multiply two quaternions\nvec4 quat_mul(vec4 a, vec4 b) {\n \treturn vec4(\n        a.w * b.x + a.x * b.w + a.y * b.z - a.z * b.y,\n        a.w * b.y - a.x * b.z + a.y * b.w + a.z * b.x,\n        a.w * b.z + a.x * b.y - a.y * b.x + a.z * b.w,\n        a.w * b.w - a.x * b.x - a.y * b.y - a.z * b.z\n    );   \n}\n\n\n\n\n\n\nvec2 tile_resolution(vec2 screen_resolution, float tile_id_f) {\n    float tile_height = 1.0 / pow(2.0, tile_id_f);\n    return screen_resolution * tile_height;\n}\n\n\nvec4 sample_tile(sampler2D buffer, vec2 resolution, vec2 uv, float tile_id_f) {\n    float tile_height = 1.0 / pow(2.0, tile_id_f);\n    \n    uv.x += 1.0;\n    vec2 area_uv = uv * tile_height;\n    \n    // Compensate for GL.LINEAR sampling - we need to sample the middle of the pixel\n    vec2 tile_resolution = resolution * tile_height;\n    vec2 inv_resolution = 1.0 / resolution.xy;\n    area_uv -= mod(area_uv, inv_resolution) - inv_resolution * 0.5;\n    \n    return texture(buffer, area_uv);\n}\n\n","name":"Common","description":"","type":"common"},{"inputs":[{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"XsXGR8","filepath":"/media/previz/buffer01.png","previewfilepath":"/media/previz/buffer01.png","type":"buffer","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"XsXGR8","channel":0}],"code":"// Render the world. \n//\n// Outputs:\n// - rgb = analytic normal\n// - a = depth\n//\n// Pretty much a single raymarcher\n\n#define BUFFER_STATE iChannel0\n#define BUFFER_SELF iChannel1\n\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    // Normalized pixel coordinates (from 0 to 1)\n    vec2 uv = fragCoord/iResolution.xy;\n    \n    float tile_id_f = floor(log2(1.0 / uv.x)) + 1.0;\n    float tile_height = 1.0 / pow(2.0, tile_id_f);\n\n\t// Convert to coordinates for this tile    \n    vec2 tile_uv = vec2(uv / tile_height);\n   \ttile_uv.x -= 1.0;\n    \n    if (uv.y > tile_height || tile_id_f > MAX_TILES) {\n        fragColor = vec4(0.0);\n        return;\n    }\n    \n    // sample previous tile\n    vec4 previous = sample_tile(BUFFER_SELF, iResolution.xy, tile_uv, tile_id_f + 1.0);\n    \n    \n    mat4 camera_transform = quat_to_transform(\n        read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION),\n        read_data(BUFFER_STATE, ADDR_CAMERA_POSITION).xyz\n    );\n    \n    \n    // Output to screen\n    \n    // Output to screen\n    fragColor = raymarch(\n        tile_resolution(iResolution.xy, tile_id_f),\n        tile_uv,\n        previous,\n        camera_transform\n    );\n}\n\n\n","name":"Buffer B","description":"","type":"buffer"}]}