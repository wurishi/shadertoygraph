{"ver":"0.1","info":{"id":"tlXfRX","date":"1633657431","viewed":883,"name":"Real-time GI Copy","username":"yuletian","description":"svgf real time GI","likes":30,"published":1,"flags":48,"usePreview":0,"tags":["svgf"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[{"id":"XdfGR8","filepath":"/media/previz/buffer03.png","previewfilepath":"/media/previz/buffer03.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dfGRr","channel":0}],"code":"// Copyright 2020 Alexander Dzhoganov\n//\n// MIT License\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy of this software\n// and associated documentation files (the \"Software\"), to deal in the Software without restriction,\n// including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,\n// and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,\n// subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n// OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n// LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n// CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n/*\n *\n * Real-time global illumination via Monte Carlo path tracing\n * with Spatiotemporal Variance-Guided Filtering.\n *\n * >>>\n * >>> Use the mouse and WASD (or arrow keys) to move the camera.\n * >>>\n *\n * The code is pretty well commented and should be easy to parse.\n * Various quality trade-offs were made due to the very thin gbuffer layout and not enough buffers to do a lot of filtering.\n *\n * > Spatiotemporal Variance-Guided Filtering:\n *\n * Dundr 2018, \"Progressive Spatiotemporal Variance-Guided Filtering\"\n * https://pdfs.semanticscholar.org/a81a/4eed7f303f7e7f3ca1914ccab66351ce662b.pdf\n *\n * NVIDIA 2017, \"Spatiotemporal Variance-Guided Filtering: Real-Time Reconstruction for Path-Traced Global Illumination\"\n * https://cg.ivd.kit.edu/publications/2017/svgf/svgf_preprint.pdf\n *\n * Dammertz, Sewtz, Hanika, Lensch 2010, \"Edge-Avoiding Ã€-Trous Wavelet Transform for fast Global Illumination Filtering\"\n * https://jo.dreggn.org/home/2010_atrous.pdf\n *\n * > Pseudorandom number generation for Monte Carlo integration:\n *\n * GPU Gems 3, \"Efficient Random Number Generation and Application Using CUDA\"\n * https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-37-efficient-random-number-generation-and-application\n *\n * > Branchless construction of an orthonormal basis:\n *\n * Pixar 2017, \"Building an Orthonormal Basis, Revisited\"\n * https://graphics.pixar.com/library/OrthonormalB/paper.pdf\n *\n */\n\n// different debug view modes\n#define DEBUG_MODE 0\n// 0 - debug off\n// 1 - albedo\n// 2 - normals\n// 3 - depth\n// 4 - irradiance\n// 5 - emittance\n// 6 - variance\n\nconst float AMBIENT = 0.01;\n\nbool debugDrawGbuffer(GBuffer gbuf, out vec4 fragColor) {\n  switch (DEBUG_MODE) {\n  case 1: // albedo\n    fragColor = vec4(gbuf.albedo, 1.0);\n    break;\n  case 2: // normals\n    fragColor = vec4(vec3(gbuf.normal + 1.0) * 0.5, 1.0);\n    break;\n  case 3: // depth\n    fragColor = vec4(vec3(gbuf.depth * 0.01), 1.0);\n    break;\n  case 4: // irradiance\n    fragColor = vec4(vec3(pow(gbuf.irradiance, 1.0 / 2.2)), 1.0);\n    break;\n  case 5: // emittance\n    fragColor = vec4(vec3(gbuf.emittance), 1.0);\n    break;\n  case 6: // variance\n    fragColor = vec4(vec3(gbuf.variance), 1.0);\n    break;\n  }\n\n  return DEBUG_MODE != 0;\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // fourth filtering pass (step size = 8)\n  GBuffer g = psvgf(iChannel0, fragCoord, 8.0);\n\n  // if any debug mode is active draw it and bail\n  if (debugDrawGbuffer(g, fragColor)) {\n    return;\n  }\n\n  // calculate the final color value of the pixel\n  // albedo * (irradiance + emittance + ambient)\n  vec3 color = g.albedo * (g.irradiance + g.emittance + AMBIENT);\n\n  // gamma correction\n  color = pow(color, vec3(1.0 / 2.2));\n\n  fragColor = vec4(color, 1.0);\n}\n","name":"Image","description":"","type":"image"},{"inputs":[],"outputs":[],"code":"#define UZERO uint(min(0, iFrame))\n#ifndef saturate\n#define saturate(X) clamp(X, 0.0, 1.0)\n#endif\n\n#define PI 3.14159265359\n\n// [0..1] float to byte\nuint f2b(float value) { return uint(saturate(value) * 255.0) & 0xFFu; }\n// byte to [0..1] float\nfloat b2f(uint value) { return float(value & 0xFFu) * (1.0 / 255.0); }\n\n// 128-bit gbuffer\n//\n// albedo r (8), albedo g (8), albedo b (8), emittance (8)\n// normal x (8), normal y (8) normal z (8), unused (8)\n// depth (16) variance (16)\n// irradiance (32)\n//\nstruct GBuffer {\n  vec3 albedo;\n  float irradiance;\n  vec3 normal;\n  float emittance;\n  float depth;\n  float variance;\n};\n\n// Pack the GBuffer struct into a vec4.\nvec4 packGBuffer(GBuffer gbuf) {\n  uvec4 p;\n  p.x = f2b(gbuf.albedo.r) | f2b(gbuf.albedo.g) << 8 |\n        f2b(gbuf.albedo.b) << 16 | f2b(gbuf.emittance) << 24;\n  vec3 normal = (gbuf.normal + 1.0) * 0.5;\n  p.y = f2b(normal.x) | f2b(normal.y) << 8 | f2b(normal.z) << 16;\n  p.z = packHalf2x16(vec2(gbuf.depth, gbuf.variance));\n  p.w = floatBitsToUint(gbuf.irradiance);\n  return uintBitsToFloat(p);\n}\n\n// Unpack the GBuffer struct from a vec4.\nGBuffer unpackGBuffer(vec4 packed) {\n  uvec4 p = floatBitsToUint(packed);\n\n  GBuffer gbuf;\n  gbuf.albedo.r = b2f(p.x);\n  gbuf.albedo.g = b2f(p.x >> 8);\n  gbuf.albedo.b = b2f(p.x >> 16);\n  gbuf.emittance = b2f(p.x >> 24);\n  gbuf.normal.x = b2f(p.y);\n  gbuf.normal.y = b2f(p.y >> 8);\n  gbuf.normal.z = b2f(p.y >> 16);\n  gbuf.normal = normalize(gbuf.normal * 2.0 - 1.0);\n  vec2 tmp = unpackHalf2x16(p.z);\n  gbuf.depth = tmp.x;\n  gbuf.variance = tmp.y;\n  gbuf.irradiance = uintBitsToFloat(p.w);\n  return gbuf;\n}\n\n// Sample a gbuffer texture.\nGBuffer sampleGBuffer(sampler2D tex, ivec2 uv) {\n  return unpackGBuffer(texelFetch(tex, uv, 0));\n}\n\n// Creates a 4x4 rotation matrix given an axis and and an angle.\nmat4 rotationMatrix(vec3 axis, float angle) {\n  axis = normalize(axis);\n  float s = sin(angle);\n  float c = cos(angle);\n  float oc = 1.0 - c;\n\n  return mat4(\n      oc * axis.x * axis.x + c, oc * axis.x * axis.y - axis.z * s,\n      oc * axis.z * axis.x + axis.y * s, 0.0, oc * axis.x * axis.y + axis.z * s,\n      oc * axis.y * axis.y + c, oc * axis.y * axis.z - axis.x * s, 0.0,\n      oc * axis.z * axis.x - axis.y * s, oc * axis.y * axis.z + axis.x * s,\n      oc * axis.z * axis.z + c, 0.0, 0.0, 0.0, 0.0, 1.0);\n}\n\n// Camera parameters.\nstruct CameraData {\n  vec3 position;\n  vec2 pitchYaw;\n  vec2 prevMouse;\n};\n\n// Pack the CameraData struct into a vec4.\nvec4 packCameraData(CameraData camera) {\n  uvec4 packed;\n  packed.x = packHalf2x16(camera.position.xy);\n  packed.y = packHalf2x16(vec2(camera.position.z));\n  packed.z = packHalf2x16(camera.pitchYaw);\n  packed.w = packHalf2x16(camera.prevMouse);\n  return uintBitsToFloat(packed);\n}\n\n// Unpack the CameraData struct from a vec4.\nCameraData unpackCameraData(vec4 packed) {\n  uvec4 p = floatBitsToUint(packed);\n\n  CameraData camera;\n  camera.position.xy = unpackHalf2x16(p.x);\n  camera.position.z = unpackHalf2x16(p.y).x;\n  camera.pitchYaw = unpackHalf2x16(p.z);\n  camera.prevMouse = unpackHalf2x16(p.w);\n  return camera;\n}\n\n// Returns the inverse view matrix for a camera.\nmat4 getInvViewMatrix(CameraData camera) {\n  mat4 pitch = rotationMatrix(vec3(1.0, 0.0, 0.0), camera.pitchYaw.x);\n  mat4 yaw = rotationMatrix(vec3(0.0, 1.0, 0.0), camera.pitchYaw.y);\n  mat4 translate = mat4(1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n                        0.0, camera.position, 1.0);\n\n  return yaw * pitch * translate;\n}\n\n// Returns the view matrix for a camera.\nmat4 getViewMatrix(CameraData camera) {\n  mat4 pitch = rotationMatrix(vec3(1.0, 0.0, 0.0), -camera.pitchYaw.x);\n  mat4 yaw = rotationMatrix(vec3(0.0, 1.0, 0.0), -camera.pitchYaw.y);\n  mat4 translate = mat4(1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n                        0.0, -camera.position, 1.0);\n\n  return pitch * yaw * translate;\n}\n\n// Returns a perspective projection matrix.\nmat4 getProjMatrix(float fov, vec2 size, float near, float far) {\n  float fn = far + near;\n  float f_n = far - near;\n  float r = size.x / size.y;\n  float t = -1.0 / tan(radians(fov) * 0.5);\n\n  return mat4(                                //\n      t / r, 0.0, 0.0, 0.0,                   //\n      0.0, t, 0.0, 0.0,                       //\n      0.0, 0.0, fn / f_n, 1.0,                //\n      0.0, 0.0, (2.0 * far * near) / f_n, 0.0 //\n  );\n}\n\n// Calculates the ray direction in view space for a pixel given the camera's\n// field of view and the screen size in pixels.\nvec3 rayDirection(float fov, vec2 size, vec2 fragCoord) {\n  vec2 xy = fragCoord - size * 0.5;\n  float z = size.y / tan(radians(fov) * 0.5);\n  return normalize(vec3(xy, -z));\n}\n\n// Projects a world-space position to screen-space given camera view and\n// projection matrices.\nvec3 project2Screen(const mat4 view, const mat4 proj, vec3 v) {\n  vec4 p = proj * (view * vec4(v, 1.0));\n  p /= p.w;\n  p.xy += 0.5;\n  p.z *= 2.0;\n  p.z -= 1.0;\n  return p.xyz;\n}\n\n// Normal-weighting function (4.4.1)\nfloat normalWeight(vec3 normal0, vec3 normal1) {\n  const float exponent = 64.0;\n  return pow(max(0.0, dot(normal0, normal1)), exponent);\n}\n\n// Depth-weighting function (4.4.2)\nfloat depthWeight(float depth0, float depth1, vec2 grad, vec2 offset) {\n  // paper uses eps = 0.005 for a normalized depth buffer\n  // ours is not but 0.1 seems to work fine\n  const float eps = 0.1;\n  return exp((-abs(depth0 - depth1)) / (abs(dot(grad, offset)) + eps));\n}\n\n// Luminance-weighting function (4.4.3)\nfloat luminanceWeight(float lum0, float lum1, float variance) {\n  const float strictness = 4.0;\n  const float eps = 0.01;\n  return exp((-abs(lum0 - lum1)) / (strictness * variance + eps));\n}\n\n// The next function implements the filtering method described in the two papers\n// linked below.\n//\n// \"Progressive Spatiotemporal Variance-Guided Filtering\"\n// https://pdfs.semanticscholar.org/a81a/4eed7f303f7e7f3ca1914ccab66351ce662b.pdf\n//\n// \"Edge-Avoiding Ã€-Trous Wavelet Transform for fast Global Illumination Filtering\"\n// https://jo.dreggn.org/home/2010_atrous.pdf\n//\n\nGBuffer psvgf(sampler2D buf, vec2 uv, float stepSize) {\n  // 3x3 kernel from the paper\n  const float filterKernel[] =\n      float[](0.0625, 0.125, 0.0625, 0.125, 0.25, 0.125, 0.0625, 0.125, 0.0625);\n\n  GBuffer g = sampleGBuffer(buf, ivec2(uv));\n\n  // depth-gradient estimation from screen-space derivatives\n  vec2 dgrad = vec2(dFdx(g.depth), dFdy(g.depth));\n\n  // total irradiance\n  float irradiance = 0.0;\n\n  // weights sum\n  float wsum = 0.0;\n\n  for (int y = -1; y <= 1; y++) {\n    for (int x = -1; x <= 1; x++) {\n      vec2 offset = vec2(x, y) * stepSize;\n      GBuffer s = sampleGBuffer(buf, ivec2(uv + offset));\n\n      // calculate the normal, depth and luminance weights\n      float nw = normalWeight(g.normal, s.normal);\n      float dw = depthWeight(g.depth, s.depth, dgrad, offset);\n      float lw = luminanceWeight(g.irradiance, s.irradiance, g.variance);\n\n      // combine the weights from above\n      float w = saturate(nw * dw * lw);\n\n      // scale by the filtering kernel\n      w *= filterKernel[(x + 1) + (y + 1) * 3];\n\n      // add to total irradiance\n      irradiance += s.irradiance * w;\n      wsum += w;\n    }\n  }\n\n  // scale total irradiance by the sum of the weights\n  g.irradiance = irradiance / wsum;\n  return g;\n}\n","name":"Common","description":"","type":"common"},{"inputs":[{"id":"4dXGRr","filepath":"/presets/tex00.jpg","previewfilepath":"/presets/tex00.jpg","type":"keyboard","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dXGR8","channel":0}],"code":"const float MOUSE_SENSITIVITY = 0.15; // mouse sensitivity\nconst uint NUM_SAMPLES = 32u; // number of traced paths per pixel per frame\nconst uint MAX_BOUNCES = 3u;  // max number of bounces per path\n\n// Ray-sphere intersection.\nfloat intersectRaySphere(vec3 ro, vec3 rd, vec3 sp, float rsq) {\n  vec3 n = ro - sp;\n  float a = dot(rd, rd);\n  float b = 2.0 * dot(rd, n);\n  float c = dot(n, n) - rsq;\n  float d = b * b - 4.0 * a * c;\n  return d < 0.0 ? -1.0 : (-b - sqrt(d)) / 2.0 * a;\n}\n\n// Ray-plane intersection.\nfloat intersectRayPlane(vec3 ro, vec3 rd, vec3 n) {\n  const float eps = 0.0001;\n  float denom = dot(rd, n);\n  return abs(denom) < eps ? -1.0 : dot(-ro, n) / denom;\n}\n\n// Efficient pseudorandom generator.\n// https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-37-efficient-random-number-generation-and-application\nuvec4 rngState;\n\nuint tausStep(uint z, uint S1, uint S2, uint S3, uint M) {\n  uint b = ((z << S1) ^ z) >> S2;\n  return ((z & M) << S3) ^ b;\n}\n\nuint lcgStep(uint z, uint A, uint C) { return A * z + C; }\n\n// Returns a random number [0..1].\nfloat random() {\n  const float c = 2.3283064365387e-10;\n  rngState.x = tausStep(rngState.x, 13u, 19u, 12u, 4294967294u);\n  rngState.y = tausStep(rngState.y, 2u, 25u, 4u, 4294967288u);\n  rngState.z = tausStep(rngState.z, 3u, 11u, 17u, 4294967280u);\n  rngState.w = lcgStep(rngState.w, 1664525u, 1013904223u);\n  return saturate(c * float(rngState.x ^ rngState.y ^ rngState.z ^ rngState.w));\n}\n\n// Seeds the random number generator.\nvoid seedRng(uvec4 seed) {\n  rngState.x = seed.x * 15178633u;\n  rngState.y = seed.y * 15178633u;\n  rngState.z = seed.z * 15178633u;\n  rngState.w = seed.w * 15178633u;\n}\n\n// Branchless construction of an orthonormal basis.\n// https://graphics.pixar.com/library/OrthonormalB/paper.pdf\nvoid orthonormalBasis(const vec3 n, out vec3 b1, out vec3 b2) {\n  float s = n.z >= 0.0 ? 1.0 : -1.0;\n  float a = -1.0 / (s + n.z);\n  float b = n.x * n.y * a;\n  b1 = vec3(1.0 + s * n.x * n.x * a, s * b, -s * n.x);\n  b2 = vec3(b, s + n.y * n.y * a, -n.y);\n}\n\n// Returns a cosine-weighted unit vector on a hemisphere centered around n.\nvec3 unitVectorOnHemisphere(vec3 n) {\n  float r = random();\n  float angle = random() * (2.0 * PI);\n  float sr = sqrt(r);\n  vec2 p = vec2(sr * cos(angle), sr * sin(angle));\n  vec3 ph = vec3(p.xy, sqrt(1.0 - dot(p, p)));\n\n  vec3 b1, b2;\n  orthonormalBasis(n, b1, b2);\n  return b1 * ph.x + b2 * ph.y + n * ph.z;\n}\n\nstruct Sphere {\n  vec3 position;\n  float radius;\n  vec4 color; // rgb - albedo, a - emissive\n};\n\nconst Sphere[] sceneSpheres =\n    Sphere[](Sphere(vec3(-1.0, 1.0, -2.5), 1.0, // red ball\n                    vec4(0.75, 0.05, 0.05, 0.0)),\n             Sphere(vec3(-3.5, 0.45, 4.0), 0.45, // orange ball\n                    vec4(0.95, 0.85, 0.05, 0.0)),\n             Sphere(vec3(-4.0, 0.35, 2.5), 0.35, // pink ball\n                    vec4(0.95, 0.71, 0.75, 0.0)),\n             Sphere(vec3(-2.5, 0.75, 1.5), 0.75, // green ball\n                    vec4(0.05, 0.75, 0.05, 0.0)),\n             Sphere(vec3(-1.25, 0.75, 3.5), 0.75, // blue ball\n                    vec4(0.05, 0.05, 0.75, 0.0)),\n             Sphere(vec3(-5.0, 0.75, 1.5), 0.75, // white ball 1\n                    vec4(1.0, 1.0, 1.0, 2.0)),\n             Sphere(vec3(0.0, 1.25, 0.0), 1.25, // white ball 2\n                    vec4(1.0, 1.0, 1.0, 2.0)),\n             Sphere(vec3(-4.0, 0.25, 5.0), 0.25, // white ball 3\n                    vec4(1.0, 1.0, 1.0, 2.0)));\n\n// Traces a ray (ro, rd) through the scene and returns the hit distance, normal\n// and color.\nfloat traceSceneRay(vec3 ro, vec3 rd, out vec3 normal, out vec4 color) {\n  const vec3 up = vec3(0.0, 1.0, 0.0);\n\n  float minT = 1e10;\n  color = vec4(1.0, 1.0, 1.0, 0.0);\n\n  float tFloor = intersectRayPlane(ro, rd, up);\n  if (tFloor > 0.0) {\n    normal = up;\n    minT = tFloor;\n  }\n\n  for (int i = 0; i < sceneSpheres.length(); i++) {\n    vec3 p = sceneSpheres[i].position;\n    float r2 = sceneSpheres[i].radius * sceneSpheres[i].radius;\n    float t = intersectRaySphere(ro, rd, p, r2);\n    if (t > 0.0 && t < minT) {\n      normal = (ro + rd * t) - p;\n      color = sceneSpheres[i].color;\n      minT = t;\n    }\n  }\n\n  return minT;\n}\n\n// Traces multiple paths for a primary ray and return the blended result.\nGBuffer tracePrimaryRay(vec3 ro, vec3 rd) {\n  GBuffer gbuf;\n\n  vec3 normal0;\n  vec4 color;\n  // get the depth, normal and color for this ray\n  float depth = traceSceneRay(ro, rd, normal0, color);\n  normal0 = normalize(normal0);\n\n  // fill the gbuffer with the material data\n  gbuf.albedo = color.rgb;\n  gbuf.emittance = color.a;\n  gbuf.depth = depth;\n  gbuf.normal = normal0;\n\n  // move the ray to the hit point\n  ro += rd * depth;\n  // slightly displace by the normal to prevent self-intersection\n  ro += normal0 * 0.00001;\n\n  vec3 ro0 = ro;\n  // summed irradiance for this pixel\n  gbuf.irradiance = 0.0;\n  // summed irradiance squared for this pixel\n  // used later on for variance estimation\n  float irradiance2 = 0.0;\n\n  for (uint q = UZERO; q < NUM_SAMPLES; q++) {\n    // get a random direction on the hemisphere around the normal\n    ro = ro0;\n    rd = unitVectorOnHemisphere(normal0);\n\n    // irradiance sum for the current path\n    float r = 0.0;\n\n    // keep bouncing and gathering light\n    for (uint i = 0u; i < MAX_BOUNCES; i++) {\n      vec3 normal;\n      depth = traceSceneRay(ro, rd, normal, color);\n      if (depth > 100.0) {\n        break;\n      }\n\n      // gather the emittance of whatever we hit\n      r += color.a;\n\n      // calculate the ray for the next bounce\n      normal = normalize(normal);\n      ro += rd * depth;\n      ro += normal * 0.00001;\n      rd = unitVectorOnHemisphere(normal);\n    }\n\n    // total irradiance = sum of irradience for each bounce\n    gbuf.irradiance += r;\n    // total squared irradiance\n    irradiance2 += r * r;\n  }\n\n  gbuf.irradiance = gbuf.irradiance / float(NUM_SAMPLES);\n  irradiance2 /= float(NUM_SAMPLES);\n\n  // variance = sum(x^2) - sum(x)^2\n  gbuf.variance = abs(irradiance2 - gbuf.irradiance * gbuf.irradiance);\n\n  return gbuf;\n}\n\n#define KEY_A 65\n#define KEY_D 68\n#define KEY_S 83\n#define KEY_W 87\n#define KEY_LEFT 37\n#define KEY_UP 38\n#define KEY_RIGHT 39\n#define KEY_DOWN 40\n\nbool isKeyPressed(int key) {\n  return texelFetch(iChannel1, ivec2(key, 0), 0).x != 0.0;\n}\n\n// Updates the camera according to user inputs.\nvoid updateCamera(inout CameraData camera, mat4 cameraMatrix) {\n  if (iFrame == 0) {\n    camera.position = vec3(-9.0, 2.0, 8.0);\n    camera.pitchYaw = vec2(0.15, PI * 0.25);\n    camera.prevMouse = iMouse.xy;\n  }\n\n  vec3 camFwd = (cameraMatrix * vec4(0.0, 0.0, -1.0, 0.0)).xyz;\n  vec3 camRight = (cameraMatrix * vec4(1.0, 0.0, 0.0, 0.0)).xyz;\n\n  if (isKeyPressed(KEY_W) || isKeyPressed(KEY_UP)) {\n    camera.position += camFwd * iTimeDelta * 4.0;\n  }\n  if (isKeyPressed(KEY_S) || isKeyPressed(KEY_DOWN)) {\n    camera.position -= camFwd * iTimeDelta * 4.0;\n  }\n  if (isKeyPressed(KEY_A) || isKeyPressed(KEY_LEFT)) {\n    camera.position -= camRight * iTimeDelta * 4.0;\n  }\n  if (isKeyPressed(KEY_D) || isKeyPressed(KEY_RIGHT)) {\n    camera.position += camRight * iTimeDelta * 4.0;\n  }\n\n  vec2 mouseDelta = iMouse.xy - camera.prevMouse;\n  mouseDelta = clamp(mouseDelta, -10.0, 10.0);\n  mouseDelta.x *= -1.0;\n  camera.pitchYaw -= mouseDelta.yx * iTimeDelta * MOUSE_SENSITIVITY;\n  camera.pitchYaw = clamp(camera.pitchYaw, -PI, PI);\n  camera.prevMouse = iMouse.xy;\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // seed the rng\n  seedRng(uvec4(fragCoord.xy, iFrame, iTime));\n\n  // fetch the camera data\n  vec4 cameraDataRaw = texelFetch(iChannel0, ivec2(0, 0), 0);\n  CameraData camera = unpackCameraData(cameraDataRaw);\n  mat4 cameraMatrix = getInvViewMatrix(camera);\n\n  if (uint(fragCoord.x) == 0u && uint(fragCoord.y) == 0u) {\n    // update the camera and store the updated data\n    updateCamera(camera, cameraMatrix);\n    fragColor = packCameraData(camera);\n    return;\n  } else if (uint(fragCoord.x) == 1u && uint(fragCoord.y) == 0u) {\n    // store previous frame's camera (used later on)\n    fragColor = cameraDataRaw;\n    return;\n  }\n\n  // get a camera ray\n  vec3 ro = camera.position;\n  vec3 rd = rayDirection(45.0, iResolution.xy, fragCoord);\n  rd = (cameraMatrix * vec4(rd, 0.0)).xyz;\n\n  // trace the ray for this pixel\n  GBuffer gbuf = tracePrimaryRay(ro, rd);\n\n  // store the result\n  fragColor = packGBuffer(gbuf);\n}\n","name":"Buffer A","description":"","type":"buffer"},{"inputs":[{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"XsXGR8","filepath":"/media/previz/buffer01.png","previewfilepath":"/media/previz/buffer01.png","type":"buffer","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"XsXGR8","channel":0}],"code":"const float DISOCCLUSION_EPS = 0.5;\nconst float HISTORY_BLEND_FACTOR = 0.1;\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // first filtering pass (step size = 1)\n  GBuffer g = psvgf(iChannel0, fragCoord, 1.0);\n\n  // recreate the ray for this pixel from the camera data\n  CameraData camera = unpackCameraData(texelFetch(iChannel0, ivec2(0, 0), 0));\n  mat4 cameraMatrix = getInvViewMatrix(camera);\n  vec3 ro = camera.position;\n  vec3 rd = rayDirection(45.0, iResolution.xy, fragCoord);\n  rd = (cameraMatrix * vec4(rd, 0.0)).xyz;\n\n  // fetch the camera data from the previous frame\n  // we'll use it to reproject the pixel onto the history buffer\n  CameraData prevCamera =\n      unpackCameraData(texelFetch(iChannel0, ivec2(1, 0), 0));\n\n  // view matrix from previous frame\n  mat4 prevView = getViewMatrix(prevCamera);\n\n  // projection matrix from previous frame\n  mat4 prevProj = getProjMatrix(45.0, iResolution.xy, 1.0, 100.0);\n\n  // reconstruct world-space position from ray and depth\n  vec3 worldPos = ro + rd * g.depth;\n\n  // project world-space position to screen-space\n  vec3 projPos = project2Screen(prevView, prevProj, worldPos);\n\n  // fetch the reprojected pixel from history\n  GBuffer prevG = unpackGBuffer(\n      texelFetch(iChannel1, ivec2(projPos.xy * iResolution.xy), 0));\n\n  // bounds check\n  bvec4 inside = bvec4(projPos.x > 0.0, projPos.y > 0.0,\n                       projPos.x < iResolution.x, projPos.y < iResolution.y);\n\n  // if in bounds and not the first frame blend between the current frame and\n  // history buffer (section 4.2 from \"Progressive Spatiotemporal\n  // Variance-Guided Filtering\")\n  if (all(inside) && iFrame != 0) {\n    // disocclusion factor\n    float disocclusion = abs(g.depth - prevG.depth);\n    if (disocclusion < DISOCCLUSION_EPS) {\n      g.irradiance = mix(prevG.irradiance, g.irradiance, HISTORY_BLEND_FACTOR);\n      g.variance = mix(prevG.variance, g.variance, HISTORY_BLEND_FACTOR);\n    }\n  }\n\n  fragColor = packGBuffer(g);\n}\n","name":"Buffer B","description":"","type":"buffer"},{"inputs":[{"id":"XsXGR8","filepath":"/media/previz/buffer01.png","previewfilepath":"/media/previz/buffer01.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4sXGR8","channel":0}],"code":"// second filtering pass (step size = 2)\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  GBuffer g = psvgf(iChannel0, fragCoord, 2.0);\n  fragColor = packGBuffer(g);\n}\n","name":"Buffer C","description":"","type":"buffer"},{"inputs":[{"id":"4sXGR8","filepath":"/media/previz/buffer02.png","previewfilepath":"/media/previz/buffer02.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"XdfGR8","channel":0}],"code":"// third filtering pass (step size = 4)\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  GBuffer g = psvgf(iChannel0, fragCoord, 4.0);\n  fragColor = packGBuffer(g);\n}\n","name":"Buffer D","description":"","type":"buffer"}]}