{"ver":"0.1","info":{"id":"sstBRH","date":"1656187500","viewed":259,"name":"HPG 2022 student competition AJ","username":"akshayjin","description":"https://www.highperformancegraphics.org/2022/student-competition/\nUnoptimized path tracer for the High Performance Graphics student competition.","likes":1,"published":1,"flags":32,"usePreview":1,"tags":["pathtracing","hpgconf"],"hasliked":0,"parentid":"ftXyD2","parentname":"HPG Student Competition 2022"},"renderpass":[{"inputs":[{"id":"XdfGR8","filepath":"/media/previz/buffer03.png","previewfilepath":"/media/previz/buffer03.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dfGRr","channel":0}],"code":"\n// different debug view modes\n#define DEBUG_MODE 0\n// 0 - debug off\n// 1 - normals\n// 2 - depth\n// 3 - variance\n\nbool debugDrawGbuffer(GBuffer gbuf, out vec4 fragColor) {\n  switch (DEBUG_MODE) {\n  case 1: // normals\n    fragColor = vec4(vec3(gbuf.normal + 1.0) * 0.5, 1.0);\n    break;\n  case 2: // depth\n    fragColor = vec4(vec3(gbuf.depth * 0.01), 1.0);\n    break;\n  case 3: // variance\n    fragColor = vec4(vec3(gbuf.variance), 1.0);\n    break;\n  }\n\n  return DEBUG_MODE != 0;\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // fourth filtering pass (step size = 8)\n  GBuffer g = psvgf(iChannel0, fragCoord, 8.0);\n\n  // if any debug mode is active draw it and bail\n  if (debugDrawGbuffer(g, fragColor)) {\n    return;\n  }\n\n  // calculate the final color value of the pixel\n  vec3 color = (g.radiance * EXPOSURE);\n\n  // gamma correction\n  color = pow(abs(color), vec3(1.0 / 2.2));\n  \n  if (isnan(color.r) || isnan(color.g) || isnan(color.b))\n     color = vec3(0.0);\n\n  fragColor = vec4(color, 1.0);\n}","name":"Image","description":"","type":"image"},{"inputs":[{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dXGR8","channel":0}],"code":"// The following built-in quality knobs are available:\n//\n// (1) Uncomment the preprocessor definition FULL_SCENE to get the full scene.\n// (2) Set NUM_SAMPLES to increase the number of rays per pixel.\n// \n// We will judge your submitted shader as follows:\n// \n//  - Shader run time must not exceed 2x of our baseline shader \n//    with FULL_SCENE and NUM_SAMPLES==1\n//\n//  - Final image quality will be measured using FLIP against a reference \n//    with FULL_SCENE and NUM_SAMPLES==100000\n// \n// Note: Changing these definitions will increase shader compilation times.\n// \n// Windows users will need to configure their browser to use the native OpenGL backend.\n//\n// If you are using Chrome:\n//  - Go to chrome://flags and search for \"Angle\"\n//  - Change backend from \"Default\" to \"OpenGL\"\n//  - Restart your browser\n//\n#define FULL_SCENE\n#define NUM_SAMPLES 2\n\n#define VERSION 2\n\n// Changelog:\n//\n// Update (2022-06-14): We have updated the quality evaluation metric to\n//                      FLIP (Andersson et al., HPG 2020) instead of SSIM\n//\n// Version 2:\n//   NEE computation: added epsilon for NdotL, commented in NdotL in BRDF term\n//   Thanks to Arthur Firmino for the bug report!\n\n\n// If you have developed your solution with the old code, and cannot update your\n// submission with reasonable effort, we will grade your solution against our version 1\n// renderer.\n\n#define INFINITY 9999999.0 // sorry, webgl doesn't allow to use proper float infinity :(\n//#define PI 3.141592653589\n\n#ifdef FULL_SCENE\n#define NUM_BOUNCES 6\n#else\n#define NUM_BOUNCES 2\n#endif\n\n#define MAT_LEFT        0\n#define MAT_RIGHT       1\n#define MAT_CEILING     2\n#define MAT_FLOOR       3\n#define MAT_BACK        4\n#define MAT_H           5\n#define MAT_P           6\n#define MAT_G           7\n#define MAT_2           8\n#define MAT_LIGHT0      9\n#define MAT_LIGHT1      10\n#define MAT_LIGHT2      11\n#define MAT_LIGHT3      12\n#define MAT_LIGHT4      13\n#define MAT_LIGHT5      14\n#define MAT_LIGHT6      15\n#define MAT_LIGHT7      16\n#define MAT_LIGHT8      17\n\nstruct Ray\n{\n\tvec3 origin, dir;\n};\n\nstruct AABB\n{\n\tvec3 min_, max_;\n};\n\nstruct MaterialSample\n{\n\tvec4 color;\n\tfloat roughness;\n\tbool is_light;\n};\n\nint seed;\nint flat_idx;\n\nconst float cube_light_size = 0.08;\nconst vec4 cube_light_pos[4] = vec4[4](\n\t\tvec4(  -0.9, -1.0 + cube_light_size * 0.495,  0.6, 0.4),\n\t\tvec4(  0.3, -1.0 + cube_light_size * 0.495,  0.2, 0.8),\n\t\tvec4(  1.0 - 2.0 * cube_light_size, -1.0 + 2.0 * cube_light_size, -1.0 + 5.0 * cube_light_size, 0.0),\n\t\tvec4(  -1.0 + 2.0 * cube_light_size, -1.0 + 2.0 * cube_light_size, -0.6, 0.0)\n\t\t);\n\nconst vec4 light_color[4] = vec4[4](\n\t\tvec4(1.0,1.0,1.0,5.0),\n\t\tvec4(1.0,1.0,1.0,5.0),\n\t\tvec4(1.0, .625, .375, 80.0),\n\t\tvec4(.375, .375, 1.0,80.0)\n\t\t);\n\n\n// H\nconst vec4 coordinates_H[3] = vec4[3](\nvec4(0.000, 0.000, 0.200, 0.750),\nvec4(0.427, 0.000, 0.627, 0.750),\nvec4(0.116, 0.310, 0.516, 0.450));\n// P\nconst vec4 coordinates_P[4] = vec4[4](\nvec4(0.000, 0.000, 0.200, 0.750),\nvec4(0.400, 0.360, 0.540, 0.675),\nvec4(0.044, 0.288, 0.471, 0.428),\nvec4(0.000, 0.610, 0.471, 0.750));\n// G\nconst vec4 coordinates_G[6] = vec4[6](\nvec4(0.000, 0.060, 0.200, 0.670),\nvec4(0.425, 0.060, 0.625, 0.265),\nvec4(0.425, 0.520, 0.625, 0.670),\nvec4(0.100, 0.000, 0.625, 0.140),\nvec4(0.315, 0.265, 0.625, 0.405),\nvec4(0.077, 0.610, 0.550, 0.750));\n// 2\nconst vec4 coordinates_2[5] = vec4[5](\nvec4(0.000, 0.000, 0.140, 0.365) * 0.5,\nvec4(0.474, 0.365, 0.614, 0.680) * 0.5,\nvec4(0.044, 0.000, 0.614, 0.140) * 0.5,\nvec4(0.044, 0.288, 0.544, 0.428) * 0.5,\nvec4(0.044, 0.610, 0.544, 0.750) * 0.5);\n\n// TNB, sorry! (Tangent normal basis?)\nmat3\nconstruct_ONB_frisvad(vec3 normal)\n{\n\tmat3 ret;\n\tret[1] = normal;\n\tif(normal.z < -0.999805696) {\n\t\tret[0] = vec3(0.0, -1.0, 0.0);\n\t\tret[2] = vec3(-1.0, 0.0, 0.0);\n\t}\n\telse {\n\t\tfloat a = 1.0 / (1.0 + normal.z);\n\t\tfloat b = -normal.x * normal.y * a;\n\t\tret[0] = vec3(1.0 - normal.x * normal.x * a, b, -normal.x);\n\t\tret[2] = vec3(b, 1.0 - normal.y * normal.y * a, -normal.y);\n\t}\n\treturn ret;\n}\n\nvoid\nencrypt_tea(inout uvec2 arg)\n{\n\tuvec4 key = uvec4(0xa341316c, 0xc8013ea4, 0xad90777d, 0x7e95761e);\n\tuint v0 = arg[0], v1 = arg[1];\n\tuint sum = 0u;\n\tuint delta = 0x9e3779b9u;\n\n\tfor(int i = 0; i < 32; i++) {\n\t\tsum += delta;\n\t\tv0 += ((v1 << 4) + key[0]) ^ (v1 + sum) ^ ((v1 >> 5) + key[1]);\n\t\tv1 += ((v0 << 4) + key[2]) ^ (v0 + sum) ^ ((v0 >> 5) + key[3]);\n\t}\n\targ[0] = v0;\n\targ[1] = v1;\n}\n\nvec2\nget_random()\n{\n  \tuvec2 arg = uvec2(flat_idx, seed++);\n  \tencrypt_tea(arg);\n  \treturn fract(vec2(arg) / vec2(0xffffffffu));\n}\n\n// adapted from https://cwyman.org/code/dxrTutors/tutors/Tutor14/tutorial14.md.html\nfloat\nggxNormalDistribution(float NdotH, float roughness)\n{\n\tfloat a2 = roughness * roughness;\n\tfloat d = ((NdotH * a2 - NdotH) * NdotH + 1.0);\n\treturn a2 / (d * d * PI);\n}\n\nfloat\nschlickMaskingTerm(float NdotL, float NdotV, float roughness)\n{\n\t// Karis notes they use alpha / 2 (or roughness^2 / 2)\n\tfloat k = roughness*roughness / 2.0;\n\n\t// Compute G(v) and G(l).  These equations directly from Schlick 1994\n\t//     (Though note, Schlick's notation is cryptic and confusing.)\n\tfloat g_v = NdotV / (NdotV*(1.0 - k) + k);\n\tfloat g_l = NdotL / (NdotL*(1.0 - k) + k);\n\treturn g_v * g_l;\n}\n\nvec3\nschlickFresnel(vec3 f0, float lDotH)\n{\n\treturn f0 + (vec3(1.0, 1.0, 1.0) - f0) * pow(1.0 - lDotH, 5.0);\n}\n\n// When using this function to sample, the probability density is:\n//      pdf = D * NdotH / (4 * HdotV)\nvec3\ngetGGXMicrofacetTS(vec2 randVal, float roughness, vec3 hitNorm)\n{\n\t// GGX NDF sampling\n\tfloat a2 = roughness * roughness;\n\tfloat cosThetaH = sqrt(max(0.0, (1.0 - randVal.x) / ((a2 - 1.0) * randVal.x + 1.0)));\n\tfloat sinThetaH = sqrt(max(0.0, 1.0 - cosThetaH * cosThetaH));\n\tfloat phiH = randVal.y * PI * 2.0;\n\n\t// Get our GGX NDF sample (i.e., the half vector)\n\treturn vec3(sinThetaH * cos(phiH), cosThetaH, sinThetaH * sin(phiH));\n}\n\nmat4\nrotate_y(float a)\n{\n\tmat4 ret = mat4(1.0);\n\tret[0][0] = ret[2][2] = cos(a);\n\tret[0][2] = sin(a);\n\tret[2][0] = -ret[0][2];\n\treturn ret;\n}\n\n\n\n\nvec3\nsample_light(vec3 rng, out vec3 normal, out float pdf, out vec4 Le)\n{\n\tint face_idx = int(rng.z * float(cube_light_pos.length()) * 6.0); //randomly select 1 of 24 cube light faces\n\tint cube_idx = face_idx / 6;\n\tface_idx %= 6;\n\n\tLe = light_color[cube_idx]; \n\n    // get random point on the cube face and corresponding normal\n\tvec3 p, n;\n\tswitch(face_idx) {\n\tdefault:\n\tcase 0: p = vec3(rng.x, 0, rng.y); n = vec3( 0, -1,  0); break; \n\tcase 1: p = vec3(rng.x, 1, rng.y); n = vec3( 0,  1,  0); break; \n\tcase 2: p = vec3(rng.x, rng.y, 0); n = vec3( 0,  0, -1); break; \n\tcase 3: p = vec3(rng.x, rng.y, 1); n = vec3( 0,  0,  1); break; \n\tcase 4: p = vec3(0, rng.x, rng.y); n = vec3(-1,  0,  0); break; \n\tcase 5: p = vec3(1, rng.x, rng.y); n = vec3( 1,  0,  0); break; \n\t}\n\n\tp -= vec3(0.5);\n\tp = (rotate_y(cube_light_pos[cube_idx].w) * vec4(p, 1.0)).xyz;\n\tn = (rotate_y(cube_light_pos[cube_idx].w) * vec4(n, 0.0)).xyz;\n\tp *= cube_light_size;\n\n\tnormal = n;\n\n\tpdf = 1.0 / (float(cube_light_pos.length()) * 6.0 * cube_light_size * cube_light_size); // Why not normalised?\n\n\treturn p + cube_light_pos[cube_idx].xyz; // p in world space\n}\n\nfloat\nget_light_pdf()\n{\n    return 1.0 / (float(cube_light_pos.length()) * 6.0 * cube_light_size * cube_light_size);\n}\n\n// convert pdf from area measure to solid angle\nfloat\npdf_a_to_w(float pdf, float dist2, float cos_theta)\n{\n    float abs_cos_theta = abs(cos_theta);\n    if(abs_cos_theta < 1e-8)\n        return 0.0;\n\n    return pdf * dist2 / abs_cos_theta;\n}\n\nbool\nintersect_aabb(in Ray ray, in AABB aabb, inout float t_min, inout float t_max)\n{\n\tvec3 div = 1.0 / ray.dir;\n\tvec3 t_1 = (aabb.min_ - ray.origin) * div;\n\tvec3 t_2 = (aabb.max_ - ray.origin) * div;\n\n\tvec3 t_min2 = min(t_1, t_2);\n\tvec3 t_max2 = max(t_1, t_2);\n\n\tt_min = max(max(t_min2.x, t_min2.y), max(t_min2.z, t_min));\n\tt_max = min(min(t_max2.x, t_max2.y), min(t_max2.z, t_max));\n\n\treturn t_min < t_max;\n}\n\nvec3\nray_at(in Ray ray, float t)\n{\n\treturn ray.origin + t * ray.dir;\n}\n\nfloat\nintersect_plane(\n\tRay ray,\n    vec3 center,\n    vec3 normal)\n{\n    float denom = dot(ray.dir, normal);\n    float t = dot(center - ray.origin, normal) / denom;\n\treturn t > 0.0 ? t : INFINITY;\n}\n\nfloat\nintersect_box(Ray ray, out vec3 normal, vec3 position_min, vec3 position_max)\n{\n\tfloat t_min = 0.0;\n\tfloat t_max = 999999999.0;\n\tif(intersect_aabb(ray, AABB(position_min, position_max), t_min, t_max)) {\n\t\tvec3 p = ray_at(ray, t_min);\n\n\t\tvec3 center = (position_min + position_max) * 0.5;\n\n\t\tnormal = p - center;\n\n\t\tvec3 an = abs(normal) / (position_max - position_min);\n\n\t\tif(an.x > an.y && an.x > an.z) {\n\t\t\tnormal = vec3(normal.x > 0.0 ? 1.0 : -1.0, 0, 0);\n\t\t}\n\t\tif(an.y > an.x && an.y > an.z) {\n\t\t\tnormal = vec3(0, normal.y > 0.0 ? 1.0 : -1.0, 0);\n\t\t}\n\t\tif(an.z > an.x && an.z > an.y) {\n\t\t\tnormal = vec3(0, 0, normal.z > 0.0 ? 1.0 : -1.0);\n\t\t}\n\n\t\treturn t_min;\n\t}\n\n\treturn INFINITY;\n}\n\n// Iterate through all scene objects and calculate first intersection. \nfloat\nintersect(Ray ray, inout vec3 p, inout vec3 normal, out MaterialSample ms)\n{\n\tfloat t_min = INFINITY; //assume ray doesn't hit anything\n\n\tint material = -1;\n\n    //test intersection with H\n\tfor(int i = 0; i < coordinates_H.length(); i++) {\n\t\tvec3 normal_tmp;\n\n        //define H's boxes tranformation\n\t\tRay ray_tmp = ray;\n\t\tmat4 r = rotate_y(-0.35);\n\t\tray_tmp.origin -= vec3(-0.9, -1, 0.0);\n\t\tray_tmp.dir = vec3(r * vec4(ray_tmp.dir, 0));\n\t\tray_tmp.origin = vec3(r * vec4(ray_tmp.origin, 1.0));\n\n\t\tvec3 box_origin = vec3(coordinates_H[i].xy, 0.0);\n\t\tvec3 box_size = vec3(coordinates_H[i].zw - coordinates_H[i].xy, 0.15);\n\t\tfloat t = intersect_box(ray_tmp, normal_tmp, box_origin, box_origin + box_size);//check ray intersection with box. t=how far along the ray\n\t\tif(t < t_min) {\n\t\t\tt_min = t;\n\t\t\tp = ray_at(ray, t); //intersection point; origin for next spawned ray\n\t\t\tmaterial = MAT_H; // material of intersection point\n\t\t\tnormal = vec3(transpose(r) * vec4(normal_tmp, 0.0)); // normal at intersection point\n\t\t}\n\t}\n\n\tfor(int i = 0; i < coordinates_P.length(); i++) {\n\t\tvec3 normal_tmp;\n\n\t\tRay ray_tmp = ray;\n\t\tmat4 r = rotate_y(0.75);\n\t\tray_tmp.origin -= vec3(-0.28, -1, 0.2);\n\t\tray_tmp.dir = vec3(r * vec4(ray_tmp.dir, 0));\n\t\tray_tmp.origin = vec3(r * vec4(ray_tmp.origin, 1.0));\n\n\t\tvec3 box_origin = vec3(coordinates_P[i].xy, 0.0);\n\t\tvec3 box_size = vec3(coordinates_P[i].zw - coordinates_P[i].xy, 0.15);\n\t\tfloat t = intersect_box(ray_tmp, normal_tmp, box_origin, box_origin + box_size);\n\t\tif(t < t_min) {\n\t\t\tt_min = t;\n\t\t\tp = ray_at(ray, t);\n\t\t\tmaterial = MAT_P;\n\t\t\tnormal = vec3(transpose(r) * vec4(normal_tmp, 0.0));\n\t\t}\n\t}\n\n\tfor(int i = 0; i < coordinates_G.length(); i++) {\n\t\tvec3 normal_tmp;\n\n\t\tRay ray_tmp = ray;\n\t\tmat4 r = rotate_y(-0.4);\n\t\tray_tmp.origin -= vec3(0.35, -1, -0.20);\n\t\tray_tmp.dir = vec3(r * vec4(ray_tmp.dir, 0));\n\t\tray_tmp.origin = vec3(r * vec4(ray_tmp.origin, 1.0));\n\n\t\tvec3 box_origin = vec3(coordinates_G[i].xy, 0.0);\n\t\tvec3 box_size = vec3(coordinates_G[i].zw - coordinates_G[i].xy, 0.15);\n\t\tfloat t = intersect_box(ray_tmp, normal_tmp, box_origin, box_origin + box_size);\n\t\tif(t < t_min) {\n\t\t\tt_min = t;\n\t\t\tp = ray_at(ray, t);\n\t\t\tmaterial = MAT_G;\n\t\t\tnormal = vec3(transpose(r) * vec4(normal_tmp, 0.0));\n\t\t}\n\t}\n\n#ifdef FULL_SCENE\n\tfor(int i = 0; i < coordinates_2.length(); i++) {\n\t\tvec3 normal_tmp;\n\n\t\tRay ray_tmp = ray;\n\t\tmat4 r = rotate_y(0.0);\n\t\tray_tmp.origin -= vec3(0.1, -0.2, -1.0);\n\t\tray_tmp.dir = vec3(r * vec4(ray_tmp.dir, 0));\n\t\tray_tmp.origin = vec3(r * vec4(ray_tmp.origin, 1.0));\n\n\t\tvec3 box_origin = vec3(coordinates_2[i].xy, 0.0);\n\t\tvec3 box_size = vec3(coordinates_2[i].zw - coordinates_2[i].xy, 0.125);\n\t\tfloat t = intersect_box(ray_tmp, normal_tmp, box_origin, box_origin + box_size);\n\t\tif(t < t_min) {\n\t\t\tt_min = t;\n\t\t\tp = ray_at(ray, t);\n\t\t\tmaterial = MAT_2;\n\t\t\tnormal = vec3(transpose(r) * vec4(normal_tmp, 0.0));\n\t\t}\n\t}\n\n\tfor(int i = 0; i < coordinates_2.length(); i++) {\n\t\tvec3 normal_tmp;\n\n\t\tRay ray_tmp = ray;\n\t\tmat4 r = rotate_y(0.0);\n\t\tray_tmp.origin -= vec3(0.45, -0.2, -1.0);\n\t\tray_tmp.dir = vec3(r * vec4(ray_tmp.dir, 0));\n\t\tray_tmp.origin = vec3(r * vec4(ray_tmp.origin, 1.0));\n\n\t\tvec3 box_origin = vec3(coordinates_2[i].xy, 0.0);\n\t\tvec3 box_size = vec3(coordinates_2[i].zw - coordinates_2[i].xy, 0.125);\n\t\tfloat t = intersect_box(ray_tmp, normal_tmp, box_origin, box_origin + box_size);\n\t\tif(t < t_min) {\n\t\t\tt_min = t;\n\t\t\tp = ray_at(ray, t);\n\t\t\tmaterial = MAT_2;\n\t\t\tnormal = vec3(transpose(r) * vec4(normal_tmp, 0.0));\n\t\t}\n\t}\n#endif\n\n\n\t// cube light sources\n\tfor(int i = 0; i < cube_light_pos.length(); i++) {\n\t\tvec3 normal_tmp;\n\t\tRay ray_tmp = ray;\n\t\t//mat4 r = rotate_y(scene_time);\n\t\tmat4 r = rotate_y(-cube_light_pos[i].w);\n\t\tray_tmp.origin -= cube_light_pos[i].xyz;\n\t\tray_tmp.dir = vec3(r * vec4(ray_tmp.dir, 0));\n\t\tray_tmp.origin = vec3(r * vec4(ray_tmp.origin, 1.0));\n\t\tfloat t = intersect_box(ray_tmp, normal_tmp,\n\t\t\t\tvec3(-cube_light_size * 0.5),\n\t\t\t\tvec3(cube_light_size * 0.5));\n\t\tif(t < t_min) {\n\t\t\tt_min = t;\n\t\t\tp = ray_at(ray, t);\n\t\t\tmaterial = MAT_LIGHT0 + i;\n\t\t\tnormal = vec3(transpose(r) * vec4(normal_tmp, 0.0));\n\t\t}\n\t}\n\t// left\n\t{\n\t\tvec3 n = vec3(1, 0, 0);\n\t\tfloat t = intersect_plane(ray, vec3(-1, 0, 0), n);\n\t\tif(t < t_min) {\n\t\t\tvec3 p_tmp = ray_at(ray, t);\n\t\t\tif(all(lessThanEqual(p_tmp.yz, vec2(1))) && all(greaterThanEqual(p_tmp.yz,\n\t\t\t\t\t\t\tvec2(-1))))\n\t\t\t{\n\t\t\t\tnormal = n;\n\t\t\t\tp = p_tmp;\n\n\t\t\t\tt_min = t;\n\n\t\t\t\tmaterial = MAT_LEFT;\n\t\t\t}\n\t\t}\n\t}\n\t// right\n\t{\n\t\tvec3 n = vec3(-1, 0, 0);\n\t\tfloat t = intersect_plane(ray, vec3(1, 0, 0), n);\n\t\tif(t < t_min) {\n\t\t\tvec3 p_tmp = ray_at(ray, t);\n\t\t\tif(all(lessThanEqual(p_tmp.yz, vec2(1))) && all(greaterThanEqual(p_tmp.yz,\n\t\t\t\t\t\t\tvec2(-1))))\n\t\t\t{\n\t\t\t\tnormal = n;\n\t\t\t\tp = p_tmp;\n\n\t\t\t\tt_min = t;\n\n\t\t\t\tmaterial = MAT_RIGHT;\n\t\t\t}\n\t\t}\n\t}\n\t// floor\n\t{\n\t\tvec3 n = vec3(0, 1, 0);\n\t\tfloat t = intersect_plane(ray, vec3(0, -1, 0), n);\n\t\tif(t < t_min) {\n\t\t\tvec3 p_tmp = ray_at(ray, t);\n\t\t\tif(all(lessThan(p_tmp.xz, vec2(1))) && all(greaterThan(p_tmp.xz,\n\t\t\t\t\t\t\tvec2(-1))))\n\t\t\t{\n\t\t\t\tnormal = n;\n\t\t\t\tp = p_tmp;\n\n\t\t\t\tt_min = t;\n\t\t\t\tmaterial = MAT_FLOOR;\n\t\t\t}\n\t\t}\n\t}\n\t// ceiling\n\t{\n\t\tvec3 n = vec3(0, -1, 0);\n\t\tfloat t = intersect_plane(ray, vec3(0, 1, 0), n);\n\t\tif(t < t_min) {\n\t\t\tvec3 p_tmp = ray_at(ray, t);\n\t\t\tif(all(lessThan(p_tmp.xz, vec2(1))) && all(greaterThan(p_tmp.xz,\n\t\t\t\t\t\t\tvec2(-1))))\n\t\t\t{\n\t\t\t\tnormal = n;\n\t\t\t\tp = p_tmp;\n\t\t\t\tmaterial = MAT_CEILING;\n\n\t\t\t\tt_min = t;\n\t\t\t}\n\t\t}\n\t}\n\t// back wall\n\t{\n\t\tvec3 n = vec3(0, 0, 1);\n\t\tfloat t = intersect_plane(ray, vec3(0, 0, -1), n);\n\t\tif(t < t_min) {\n\t\t\tvec3 p_tmp = ray_at(ray, t);\n\t\t\tif(all(lessThan(p_tmp.xy, vec2(1))) && all(greaterThan(p_tmp.xy,\n\t\t\t\t\t\t\tvec2(-1))))\n\t\t\t{\n\t\t\t\tnormal = n;\n\t\t\t\tp = p_tmp;\n\t\t\t\tmaterial = MAT_BACK;\n\n\t\t\t\tt_min = t;\n\t\t\t}\n\t\t}\n\t}\n\n    // define material for each objects\n\tswitch(material) {\n\tcase MAT_LEFT   : ms = MaterialSample(vec4(0.9, 0.1, 0.1, 0.0), 0.5,  false); break;\n\tcase MAT_RIGHT  : ms = MaterialSample(vec4(0.1, 0.9, 0.1, 0.0), 0.5,  false); break;\n\tcase MAT_CEILING: ms = MaterialSample(vec4(0.7, 0.7, 0.7, 0.0), 0.25, false); break;\n\tcase MAT_FLOOR  : ms = MaterialSample(vec4(0.7, 0.7, 0.7, 0.0), 0.12, false); break;\n\tcase MAT_BACK   : ms = MaterialSample(vec4(0.7, 0.7, 0.7, 0.0), 0.25, false); break;\n\tcase MAT_H      : ms = MaterialSample(vec4(1.0, 0.0, 0.0, 0.0), 0.10, false); break;\n\tcase MAT_P      : ms = MaterialSample(vec4(0.0, 0.7, 0.7, 0.0), 0.10, false); break;\n\tcase MAT_G      : ms = MaterialSample(vec4(0.1, 0.1, 0.7, 0.0), 0.10, false); break;\n\tcase MAT_2      : ms = MaterialSample(vec4(0.8, 0.8, 0.8, 0.0), 0.55, false); break;\n\tdefault         : ms = MaterialSample(light_color[material - MAT_LIGHT0], 0.0, true); break;\n\t}\n\n\tnormal = normalize(normal);\n\n\treturn t_min;\n}\n\nbool\ntest_visibility(vec3 p1, vec3 p2)\n{\n\tconst float eps = 1e-5;\n\n\tRay r = Ray(p1, normalize(p2 - p1));\n\tr.origin += eps * r.dir;\n\n\tvec3 n, p;\n\tMaterialSample ms;\n\tfloat t_shadow = intersect(r, p, n, ms);\n\n\treturn t_shadow > distance(p1, p2) - 2.0 * eps;\n}\n\nvec3\npt_mis(Ray ray, out GBuffer gbuf)\n{\n\tvec3 contrib = vec3(0); // final color of ray\n\tvec3 tp = vec3(1.0); // contribution of nth point\n\n\tvec3 position, normal;\n\tMaterialSample ms;\n\tfloat t = intersect(ray, position, normal, ms); // how far along the ray is first intersection\n    \n    gbuf.albedo = ms.color.rgb;\n    gbuf.emittance = ms.color.a;\n    gbuf.depth = t;\n    gbuf.normal = normal;\n\n\tif(t == INFINITY)\n\t\treturn vec3(0.0); // 0 radiance for not hitting anything\n\n\tif(ms.is_light) { /* hit light source; return radiance */\n\t\treturn ms.color.rgb * ms.color.a;\n\t}\n\n\n    //spawn new ray\n\tfor(int i = 0; i < NUM_BOUNCES; i++) {\n\t\tmat3 onb = construct_ONB_frisvad(normal);\n\n\t\tfloat NdotV = max(1e-4, dot(normal, -ray.dir)); // cos(incident angle)\n\n\t\t{ /* NEE */ \n\t\t\tvec3 light_normal;\n\t\t\tfloat light_pdf;\n\t\t\tvec4 Le;\n\t\t\tvec3 pos_ls = sample_light(vec3(get_random(), get_random().x), light_normal, light_pdf, Le); //Randomly pick a light source \n\t\t\tif(test_visibility(position, pos_ls)) {\n                vec3 l_nee = pos_ls - position;\n                float rr_nee = dot(l_nee, l_nee);\n                l_nee /= sqrt(rr_nee); // normalised direction of sampled light from intersection point\n\n                vec3 H = normalize(-ray.dir + l_nee);\n                float NdotH = max(0.0, dot(normal, H));\n                float LdotH = max(0.0, dot(l_nee, H));\n                float NdotL = max(1e-6, dot(normal, l_nee));\n\n                float D = ggxNormalDistribution(NdotH, ms.roughness);\n                float G = schlickMaskingTerm(NdotL, NdotV, ms.roughness);\n                vec3  F = schlickFresnel(ms.color.rgb, LdotH);\n\n                {\n                    vec3 brdf = D * G * F / (4.0 * NdotV  * NdotL );\n                    float brdf_pdf = D * NdotH / (4.0 * LdotH);\n\n                    float light_pdf_w = pdf_a_to_w(light_pdf, rr_nee, -dot(l_nee, light_normal));\n                    float w = light_pdf / (light_pdf_w + brdf_pdf);\n                    contrib += w * tp * ((Le.rgb*Le.a) * brdf) / light_pdf;                    \n                }\n                //gbuf.albedo = mix(Le.rgb,gbuf.albedo,tp);\n            }\n\t\t}\n\t\t\n\t\t{ /* brdf */\n\t\t\t// Randomly sample the NDF to get a microfacet in our BRDF \n\t\t\tvec3 H = onb * getGGXMicrofacetTS(get_random(), ms.roughness, normal);\n\n\t\t\t// Compute outgoing direction based on this (perfectly reflective) facet\n\t\t\tvec3 L = normalize(reflect(ray.dir, H));\n\n\t\t\t// Compute some dot products needed for shading\n\t\t\tfloat  NdotL = max(1e-6, dot(normal, L));\n\t\t\tfloat  NdotH = max(1e-6, dot(normal, H));\n\t\t\tfloat  LdotH = max(1e-6, dot(L, H));\n\n\t\t\t// Evaluate our BRDF using a microfacet BRDF model\n\t\t\tfloat D = ggxNormalDistribution(NdotH, ms.roughness);          \n\t\t\tfloat G = schlickMaskingTerm(NdotL, NdotV, ms.roughness); \n\t\t\tvec3  F = schlickFresnel(ms.color.rgb, LdotH);                 \n\t\t\tvec3  brdf = D * G * F / (4.0 * NdotL * NdotV);        \n\n\t\t\t// What's the probability of sampling vector H from getGGXMicrofacet()?\n\t\t\tfloat brdf_pdf = D * NdotH / (4.0 * LdotH);\n\n\t\t\tray = Ray(position + L * 1e-5, L); // move point slightly above surface\n\n            // get next intersection\n\t\t\tvec3 position_next, normal_next;\n\t\t\tMaterialSample ms_next;\n\t\t\tfloat t = intersect(ray, position_next, normal_next, ms_next);\n\n\t\t\tif(t == INFINITY)\n\t\t\t\tbreak;\n                \n            //gbuf.albedo = mix(ms.color.rgb,gbuf.albedo,tp);\n\n            if(ms_next.is_light) {\n\t\t\t\tfloat light_pdf_a = get_light_pdf();\n\t\t\t\tfloat light_pdf_w = pdf_a_to_w(light_pdf_a, t * t, -dot(ray.dir, normal_next));\n\t\t\t\tfloat w = brdf_pdf / (brdf_pdf + light_pdf_w);\n\t\t\t\tcontrib += tp * ((ms_next.color.rgb*ms_next.color.a) * w * brdf) / brdf_pdf;\n\t\t\t\tbreak;\n            }\n\n\t\t\ttp *= NdotL * brdf / brdf_pdf;\n\n\t\t\tposition = position_next;\n\t\t\tnormal = normal_next;\n\t\t\tms = ms_next;\n\t\t}\n\t}\n    //gbuf.albedo /= float(NUM_BOUNCES);\n\n\treturn contrib;\n}\n\nvoid\nmainImage(out vec4 fragColor, in vec2 fragCoord)\n{\n   // global variables for random number generation\n\tseed = iFrame * NUM_SAMPLES; \n\tflat_idx = int(dot(gl_FragCoord.xy, vec2(1, 4096)));\n\n\tvec2 p = fragCoord.xy / vec2(iResolution) - vec2(0.5); //fragCoord [-.5,.5]\n\tfloat a = float(iResolution.x) / float(iResolution.y); //aspect ratio\n\tif(a < 1.0)\n\t\tp.y /= a;\n\telse\n\t\tp.x *= a;\n\n    //define camera (dynamic)\n\t//vec3 cam_center = vec3(0, 0, 3.0);\n\tvec3 cam_center = vec3(sin(iTime) * 0.25, sin(iTime * 0.7345) * 0.4 + 0.2, 6.0);\n\tvec3 cam_target = vec3(0, -0.1, 0);\n\tmat4 cam = transpose(look_at(cam_center, cam_target, vec3(0, 1, 0)));\n    vec3 dir_vec = normalize(cam_target-cam_center);\n    float pitch = asin(-dir_vec.y);\n    float yaw = atan(dir_vec.x, dir_vec.z);\n    \n    // fetch the camera data\n    vec4 cameraDataRaw = texelFetch(iChannel0, ivec2(0, 0), 0);\n    CameraData camera = unpackCameraData(cameraDataRaw);\n    mat4 cameraMatrix = getInvViewMatrix(camera);\n\n    if (uint(fragCoord.x) == 0u && uint(fragCoord.y) == 0u) {\n        // update the camera and store the updated data\n        camera.position = cam_center;\n        camera.pitchYaw = vec2(pitch,yaw);\n        camera.prevMouse = vec2(0.0);\n        fragColor = packCameraData(camera);\n        return;\n    } else if (uint(fragCoord.x) == 1u && uint(fragCoord.y) == 0u) {\n        // store previous frame's camera (used later on)\n        fragColor = cameraDataRaw;\n        return;\n    }\n    \n\n\tvec3 s = vec3(0); //frag color\n    float exposure = 2.0;\n    GBuffer gbuf;\n    gbuf.irradiance = 0.0; // pardon the abuse of notation\n    float irradiance2 = 0.0;\n\tfor(int i = 0; i < NUM_SAMPLES; i++) {\n        //define ray\n\t\tRay ray;\n\t\tray.origin = cam_center;\n\t\tvec2 r = get_random(); // [0,1)\n\t\tvec3 ray_dir = normalize(vec3(p + r.x * dFdx(p) + r.y * dFdy(p), -2.5)); // randomly pick a point in pixel\n\t\tray.dir = vec3(cam * vec4(ray_dir, 0.0)); // tansform ray to camera coordinates\n        \n\t\tvec3 c = pt_mis(ray,gbuf); //trace ray\n\t\ts += c; //add color to pixel\n        float ir = rgb2luminance(c);\n        gbuf.irradiance += ir;\n        irradiance2 += ir*ir;\n\t}\n    gbuf.irradiance /= float(NUM_SAMPLES);\n    irradiance2 /= float(NUM_SAMPLES);\n    gbuf.radiance = s/float(NUM_SAMPLES);\n    // variance = sum(x^2) - sum(x)^2\n    gbuf.variance = abs(irradiance2 - gbuf.irradiance * gbuf.irradiance);\n    fragColor = packGBuffer(gbuf);\n\n}\n","name":"Buffer A","description":"","type":"buffer"},{"inputs":[],"outputs":[],"code":"\n//SVGF implementation adapted from https://www.shadertoy.com/view/tlXfRX\n\n#define EXPOSURE 4.0\n\nfloat rgb2luminance(vec3 rgb)\n{\n    const vec3 W = vec3(0.2125, 0.7154, 0.0721);\n    return dot(rgb, W);\n}\n\nmat4\nlook_at(vec3 eye, vec3 center, vec3 up)\n{\n\tmat4 ret;\n\n\tvec3 f = normalize(center - eye);\n\tvec3 s = normalize(cross(f, normalize(up)));\n\tvec3 u = cross(s, f);\n\n\tret[0][0] = s[0];\n\tret[1][0] = s[1];\n\tret[2][0] = s[2];\n\n\tret[0][1] = u[0];\n\tret[1][1] = u[1];\n\tret[2][1] = u[2];\n\n\tret[0][2] = -f[0];\n\tret[1][2] = -f[1];\n\tret[2][2] = -f[2];\n\n\tret[0][3] = ret[1][3] = ret[2][3] = 0.0;\n\n\tret[3][0] = -dot(s, eye);\n\tret[3][1] = -dot(u, eye);\n\tret[3][2] =  dot(f, eye);\n\n\tret[3][3] = 1.0;\n\treturn ret;\n}\n\n\n#define UZERO uint(min(0, iFrame))\n#ifndef saturate\n#define saturate(X) clamp(X, 0.0, 1.0)\n#endif\n\n#define PI 3.14159265359\n\n// [0..1] float to byte\nuint f2b(float value) { return uint(saturate(value) * 255.0) & 0xFFu; }\n// byte to [0..1] float\nfloat b2f(uint value) { return float(value & 0xFFu) * (1.0 / 255.0); }\n\n// 128-bit gbuffer\n//\n// albedo r (8), albedo g (8), albedo b (8), emittance (8)\n// normal x (8), normal y (8) normal z (8), unused (8)\n// depth (16) variance (16)\n// irradiance (32)\n//\nstruct GBuffer {\n  vec3 albedo;\n  float irradiance;\n  vec3 normal;\n  float emittance;\n  float depth;\n  float variance;\n  vec3 radiance;\n};\n\n// Pack the GBuffer struct into a vec4.\nvec4 packGBuffer(GBuffer gbuf) {\n  uvec4 p;\n  vec3 normal = (gbuf.normal + 1.0) * 0.5;\n  p.y = f2b(normal.x) | f2b(normal.y) << 8 | f2b(normal.z) << 16;\n  p.z = packHalf2x16(vec2(gbuf.depth, gbuf.variance));\n  p.x = packHalf2x16(vec2(gbuf.radiance.r, gbuf.radiance.g));\n  p.w = floatBitsToUint(gbuf.radiance.b);\n  return uintBitsToFloat(p);\n}\n\n// Unpack the GBuffer struct from a vec4.\nGBuffer unpackGBuffer(vec4 packed) {\n  uvec4 p = floatBitsToUint(packed);\n\n  GBuffer gbuf;\n  gbuf.normal.x = b2f(p.y);\n  gbuf.normal.y = b2f(p.y >> 8);\n  gbuf.normal.z = b2f(p.y >> 16);\n  gbuf.normal = normalize(gbuf.normal * 2.0 - 1.0);\n  vec2 tmp = unpackHalf2x16(p.z);\n  gbuf.depth = tmp.x;\n  gbuf.variance = tmp.y;\n  tmp = unpackHalf2x16(p.x);\n  gbuf.radiance.r = tmp.x;\n  gbuf.radiance.g = tmp.y;\n  gbuf.radiance.b = uintBitsToFloat(p.w);\n  gbuf.irradiance = 0.0;\n  gbuf.emittance = 0.0;\n  gbuf.albedo = vec3(0.0);\n  \n  return gbuf;\n}\n\n// Sample a gbuffer texture.\nGBuffer sampleGBuffer(sampler2D tex, ivec2 uv) {\n  return unpackGBuffer(texelFetch(tex, uv, 0));\n}\n\n// Creates a 4x4 rotation matrix given an axis and and an angle.\nmat4 rotationMatrix(vec3 axis, float angle) {\n  axis = normalize(axis);\n  float s = sin(angle);\n  float c = cos(angle);\n  float oc = 1.0 - c;\n\n  return mat4(\n      oc * axis.x * axis.x + c, oc * axis.x * axis.y - axis.z * s,\n      oc * axis.z * axis.x + axis.y * s, 0.0, oc * axis.x * axis.y + axis.z * s,\n      oc * axis.y * axis.y + c, oc * axis.y * axis.z - axis.x * s, 0.0,\n      oc * axis.z * axis.x - axis.y * s, oc * axis.y * axis.z + axis.x * s,\n      oc * axis.z * axis.z + c, 0.0, 0.0, 0.0, 0.0, 1.0);\n}\n\n// Camera parameters.\nstruct CameraData {\n  vec3 position;\n  vec2 pitchYaw;\n  vec2 prevMouse;\n};\n\n// Pack the CameraData struct into a vec4.\nvec4 packCameraData(CameraData camera) {\n  uvec4 packed;\n  packed.x = packHalf2x16(camera.position.xy);\n  packed.y = packHalf2x16(vec2(camera.position.z));\n  packed.z = packHalf2x16(camera.pitchYaw);\n  packed.w = packHalf2x16(camera.prevMouse);\n  return uintBitsToFloat(packed);\n}\n\n// Unpack the CameraData struct from a vec4.\nCameraData unpackCameraData(vec4 packed) {\n  uvec4 p = floatBitsToUint(packed);\n\n  CameraData camera;\n  camera.position.xy = unpackHalf2x16(p.x);\n  camera.position.z = unpackHalf2x16(p.y).x;\n  camera.pitchYaw = unpackHalf2x16(p.z);\n  camera.prevMouse = unpackHalf2x16(p.w);\n  return camera;\n}\n\n// Returns the inverse view matrix for a camera.\nmat4 getInvViewMatrix(CameraData camera) {\n  mat4 pitch = rotationMatrix(vec3(1.0, 0.0, 0.0), camera.pitchYaw.x);\n  mat4 yaw = rotationMatrix(vec3(0.0, 1.0, 0.0), camera.pitchYaw.y);\n  mat4 translate = mat4(1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n                        0.0, camera.position, 1.0);\n\n  return yaw * pitch * translate;\n}\n\n// Returns the view matrix for a camera.\nmat4 getViewMatrix(CameraData camera) {\n  mat4 pitch = rotationMatrix(vec3(1.0, 0.0, 0.0), -camera.pitchYaw.x);\n  mat4 yaw = rotationMatrix(vec3(0.0, 1.0, 0.0), -camera.pitchYaw.y);\n  mat4 translate = mat4(1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n                        0.0, -camera.position, 1.0);\n\n  return pitch * yaw * translate;\n}\n\n// Returns a perspective projection matrix.\nmat4 getProjMatrix(float fov, vec2 size, float near, float far) {\n  float fn = far + near;\n  float f_n = far - near;\n  float r = size.x / size.y;\n  float t = -1.0 / tan(radians(fov) * 0.5);\n\n  return mat4(                                //\n      t / r, 0.0, 0.0, 0.0,                   //\n      0.0, t, 0.0, 0.0,                       //\n      0.0, 0.0, fn / f_n, 1.0,                //\n      0.0, 0.0, (2.0 * far * near) / f_n, 0.0 //\n  );\n}\n\n// Calculates the ray direction in view space for a pixel given the camera's\n// field of view and the screen size in pixels.\nvec3 rayDirection(float fov, vec2 size, vec2 fragCoord) {\n  vec2 xy = fragCoord - size * 0.5;\n  float z = size.y / tan(radians(fov) * 0.5);\n  return normalize(vec3(xy, -z));\n}\n\n// Projects a world-space position to screen-space given camera view and\n// projection matrices.\nvec3 project2Screen(const mat4 view, const mat4 proj, vec3 v) {\n  vec4 p = proj * (view * vec4(v, 1.0));\n  p /= p.w;\n  p.xy += 0.5;\n  p.z *= 2.0;\n  p.z -= 1.0;\n  return p.xyz;\n}\n\n// Normal-weighting function (4.4.1)\nfloat normalWeight(vec3 normal0, vec3 normal1) {\n  const float exponent = 64.0;\n  return pow(max(0.0, dot(normal0, normal1)), exponent);\n}\n\n// Depth-weighting function (4.4.2)\nfloat depthWeight(float depth0, float depth1, vec2 grad, vec2 offset) {\n  // paper uses eps = 0.005 for a normalized depth buffer\n  // ours is not but 0.1 seems to work fine\n  const float eps = 0.1;\n  return exp((-abs(depth0 - depth1)) / (abs(dot(grad, offset)) + eps));\n}\n\n// Luminance-weighting function (4.4.3)\nfloat luminanceWeight(float lum0, float lum1, float variance) {\n  const float strictness = 4.0;\n  const float eps = 0.01;\n  return exp((-abs(lum0 - lum1)) / (strictness * variance + eps));\n}\n\n// The next function implements the filtering method described in the two papers\n// linked below.\n//\n// \"Progressive Spatiotemporal Variance-Guided Filtering\"\n// https://pdfs.semanticscholar.org/a81a/4eed7f303f7e7f3ca1914ccab66351ce662b.pdf\n//\n// \"Edge-Avoiding Ã€-Trous Wavelet Transform for fast Global Illumination Filtering\"\n// https://jo.dreggn.org/home/2010_atrous.pdf\n//\n\nGBuffer psvgf(sampler2D buf, vec2 uv, float stepSize) {\n  // 3x3 kernel from the paper\n  const float filterKernel[] =\n      float[](0.0625, 0.125, 0.0625, 0.125, 0.25, 0.125, 0.0625, 0.125, 0.0625);\n\n  GBuffer g = sampleGBuffer(buf, ivec2(uv));\n  g.irradiance = rgb2luminance(g.radiance);\n\n  // depth-gradient estimation from screen-space derivatives\n  vec2 dgrad = vec2(dFdx(g.depth), dFdy(g.depth));\n\n  // total irradiance\n  vec3 radiance = vec3(0.0);\n\n  // weights sum\n  float wsum = 0.0;\n\n  for (int y = -1; y <= 1; y++) {\n    for (int x = -1; x <= 1; x++) {\n      vec2 offset = vec2(x, y) * stepSize;\n      GBuffer s = sampleGBuffer(buf, ivec2(uv + offset));\n      s.irradiance = rgb2luminance(s.radiance);\n\n      // calculate the normal, depth and luminance weights\n      float nw = clamp(normalWeight(g.normal, s.normal),0.00,0.9999);\n      float dw = clamp(depthWeight(g.depth, s.depth, dgrad, offset),0.00,0.9999);\n      float lw = clamp(luminanceWeight(g.irradiance, s.irradiance, g.variance),0.00,0.9999);\n\n      // combine the weights from above\n      float w = saturate(nw * dw * lw);\n\n      // scale by the filtering kernel\n      w *= filterKernel[(x + 1) + (y + 1) * 3];\n\n      // add to total irradiance\n      if(!isnan(s.irradiance) && !isnan(w)) radiance += s.radiance * w;\n      if(!isnan(s.irradiance) && !isnan(w)) wsum += w;\n    }\n  }\n\n  // scale total irradiance by the sum of the weights\n  g.radiance = radiance;\n  if(wsum!=0.0) g.radiance /= wsum;\n  return g;\n}\n","name":"Common","description":"","type":"common"},{"inputs":[{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"XsXGR8","filepath":"/media/previz/buffer01.png","previewfilepath":"/media/previz/buffer01.png","type":"buffer","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"XsXGR8","channel":0}],"code":"const float DISOCCLUSION_EPS = 0.5;\nconst float HISTORY_BLEND_FACTOR = 0.1;\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // first filtering pass (step size = 1)\n  GBuffer g = psvgf(iChannel0, fragCoord, 1.0);\n\n  /* Unnecessary for tested velocities, commenting out\n  // Reproject previous frame\n  \n  // recreate the ray for this pixel from the camera data\n  CameraData camera = unpackCameraData(texelFetch(iChannel0, ivec2(0, 0), 0));\n  mat4 cameraMatrix = getInvViewMatrix(camera);\n  vec3 ro = camera.position;\n  vec3 rd = rayDirection(45.0, iResolution.xy, fragCoord);\n  rd = (cameraMatrix * vec4(rd, 0.0)).xyz;\n\n  // fetch the camera data from the previous frame\n  // we'll use it to reproject the pixel onto the history buffer\n  CameraData prevCamera =\n      unpackCameraData(texelFetch(iChannel0, ivec2(1, 0), 0));\n\n  // view matrix from previous frame\n  mat4 prevView = getViewMatrix(prevCamera);\n\n  // projection matrix from previous frame\n  mat4 prevProj = getProjMatrix(45.0, iResolution.xy, 1.0, 100.0);\n\n  // reconstruct world-space position from ray and depth\n  vec3 worldPos = ro + rd * g.depth;\n\n  // project world-space position to screen-space\n  vec3 projPos = project2Screen(prevView, prevProj, worldPos);\n\n  // fetch the reprojected pixel from history\n  GBuffer prevG = unpackGBuffer(\n      texelFetch(iChannel1, ivec2(projPos.xy * iResolution.xy), 0));\n  \n      \n  // bounds check\n  bvec4 inside = bvec4(projPos.x > 0.0, projPos.y > 0.0,\n                       projPos.x < iResolution.x, projPos.y < iResolution.y);\n  */\n                       \n  //assume last sample was in the same pixel\n  GBuffer prevG = unpackGBuffer(\n  texelFetch(iChannel1, ivec2(fragCoord), 0)); \n\n  // if in bounds and not the first frame blend between the current frame and\n  // history buffer (section 4.2 from \"Progressive Spatiotemporal\n  // Variance-Guided Filtering\")\n  if ( iFrame != 0) {\n    // disocclusion factor\n    float disocclusion = abs(g.depth - prevG.depth);\n    if (disocclusion < DISOCCLUSION_EPS) {\n      g.radiance = mix(prevG.radiance, g.radiance, HISTORY_BLEND_FACTOR);\n      g.variance = mix(prevG.variance, g.variance, HISTORY_BLEND_FACTOR);\n\n    }\n  }\n\n  fragColor = packGBuffer(g);\n}\n","name":"Buffer B","description":"","type":"buffer"},{"inputs":[{"id":"XsXGR8","filepath":"/media/previz/buffer01.png","previewfilepath":"/media/previz/buffer01.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4sXGR8","channel":0}],"code":"// second filtering pass (step size = 2)\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  GBuffer g = psvgf(iChannel0, fragCoord, 2.0);\n  fragColor = packGBuffer(g);\n}\n","name":"Buffer C","description":"","type":"buffer"},{"inputs":[{"id":"4sXGR8","filepath":"/media/previz/buffer02.png","previewfilepath":"/media/previz/buffer02.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"XdfGR8","channel":0}],"code":"// third filtering pass (step size = 4)\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  GBuffer g = psvgf(iChannel0, fragCoord, 4.0);\n  fragColor = packGBuffer(g);\n}\n","name":"Buffer D","description":"","type":"buffer"}]}