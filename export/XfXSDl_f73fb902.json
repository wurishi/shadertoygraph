{"ver":"0.1","info":{"id":"XfXSDl","date":"1705852564","viewed":26,"name":"[inspirnathan] 05 - raymarching","username":"hrst4","description":"[inspirnathan] 05 - raymarching","likes":0,"published":1,"flags":0,"usePreview":0,"tags":["inspirnathan"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[],"outputs":[{"id":"4dfGRr","channel":0}],"code":"// This a french translation of the highly educational Nathan Vaughn's tutorials.\n// Un grand merci à lui !\n// his website: https://inspirnathan.com/\n// original:\n// from https://inspirnathan.com/posts/52-shadertoy-tutorial-part-6\n\n#define PART 5\n/*\nSalutations, chers amis ! C'est le moment que vous attendiez tous ! Dans ce tutoriel, vous ferez les premiers pas pour apprendre à dessiner \ndes scènes 3D dans Shadertoy en utilisant le ray marching !\n\n# Introduction aux rayons\n\nAvez-vous déjà navigué sur Shadertoy, pour y voir des créations étonnantes qui vous laissent pantois ?\nComment font les gens pour créer des scènes aussi étonnantes avec seulement un pixel shader et aucun modèle 3D ?\nEst-ce de la magie ? Ont-ils un doctorat en mathématiques ou en graphisme ? \nCertains d'entre eux le sont peut-être, mais la plupart ne le sont pas !\n\nLa plupart des scènes 3D que vous voyez sur Shadertoy utilisent une forme d'algorithme de ray tracing ou de ray marching.\nCes algorithmes sont couramment utilisés dans le domaine de l'infographie.\nLa première étape de la création d'une scène 3D dans Shadertoy consiste à comprendre les rayons.\n\nVoici ! Le rayon !\nhttps://inspirnathan.com/_nuxt/img/img-1.02ce4ac.png\n\nC'est ça ? On dirait un point avec une flèche pointant vers lui.\nOui, c'est bien ça ! Le point noir représente l'origine du rayon et la flèche rouge indique qu'il pointe dans une direction. \nVous utiliserez beaucoup les rayons lors de la création de scènes en 3D, il est donc préférable de comprendre\nleur fonctionnement.\n\nUn rayon se compose d'une origine et d'une direction, mais qu'est-ce que je veux dire par là ?\n\nL'origine d'un rayon est simplement le point de départ du rayon.\nEn 2D, nous pouvons créer une variable en GLSL pour représenter une origine :\n\nvec2 rayOrigin = vec2(0, 0);\n\nSi vous avez suivi des cours d'algèbre linéaire ou de calcul,\nvous êtes peut-être perplexe.\nPourquoi assigner un point à un vecteur ? \nLes vecteurs n'ont-ils pas tous une direction ? \nD'un point de vue mathématique, les vecteurs ont à la fois une longueur et une direction,mais nous parlons\nici d'un type de données vectorielles.\n\nDans les langages de shaders tels que GLSL, nous pouvons utiliser un vec2 pour y stocker deux valeurs de notre choix, \ncomme s'il s'agissait d'un tableau (à ne pas confondre avec les tableaux réels dans la spécification du langage GLSL). \nLes variables de type vec3 permettent de stocker trois valeurs. Ces valeurs peuvent représenter différentes choses : \nune couleur, des coordonnées, le rayon d'un cercle, ou tout ce que vous voulez.\n\nPour l'origine d'un rayon, nous avons choisi nos valeurs pour représenter une coordonnée XY telle que (0, 0).\n\n\nLa direction d'un rayon est un vecteur normalisé de manière à ce que sa magnitude soit égale à un. \nEn 2D, nous pouvons créer une variable en GLSL pour représenter une direction :\n\nvec2 rayDirection = vec2(1, 0);\n\nEn fixant la direction du rayon à vec2(1, 0), nous disons que le rayon pointe d'une unité vers la droite.\nhttps://inspirnathan.com/_nuxt/img/img-2.a7425ed.png\n\nLes vecteurs 2D peuvent avoir une composante x et une composante y.\nVoici un exemple de rayon avec une direction vec2(2, 2) où la ligne noire représente le rayon. \nElle pointe en diagonale vers le haut et la droite à un angle de 45 degrés par rapport à l'origine.\n\nLa ligne horizontale rouge représente la composante x du rayon et la ligne verticale verte représente la composante y.\nVous pouvez vous amuser avec les vecteurs en utilisant un graphique que j'ai créé dans Desmos.\nhttps://www.desmos.com/calculator/mvdjc2mvp5\nhttps://inspirnathan.com/_nuxt/img/img-3.3a2fddc.png\n\nCe rayon n'est cependant pas normalisé. Si nous trouvons la magnitude de la direction du rayon, \nnous découvrirons qu'elle n'est pas égale à un.\nLa magnitude peut être calculée à l'aide de l'équation suivante pour les vecteurs 2D :\n\nsqrt(x^2+y^2)\n\nCalculons la magnitude (longueur) du rayon, vec2(2,2).\n\nlength(vec2(2,2)) = sqrt(x^2 + y^2) = sqrt(2^2 + 2^2) = sqrt(4 + 4) = sqrt(8)\n\nLa magnitude est égale à la racine carrée de huit. \nCette valeur n'est pas égale à un, nous devons donc la normaliser.\nEn GLSL, nous pouvons normaliser les vecteurs à l'aide de la fonction normalize :\n\nvec2 normalizedRayDirection = normalize(vec2(2, 2)) ;\n\nEn coulisses, la fonction de normalisation divise chaque composante du vecteur par la magnitude (longueur) du vecteur.\n\nGiven vec2(2,2):\nx = 2\ny = 2\n\nlength(vec2(2,2)) = sqrt(8)\n\nx / length(x) = 2 / sqrt(8) = 1 / sqrt(2) = 0.7071 (approximately)\ny / length(y) = 2 / sqrt(8) = 1 / sqrt(2) = 0.7071 (approximately)\n\nnormalize(vec2(2,2)) = vec2(0.7071, 0.7071)\n\nAprès normalisation, nous obtenons le nouveau vecteur, vec2(0,7071, 0,7071).\nSi nous calculons la longueur de ce vecteur, nous découvrirons qu'elle est égale à un.\n\nPar convention, nous utilisons des vecteurs normalisés pour représenter les directions. \nCertains des algorithmes que nous utiliserons ne s'intéressent qu'à la direction et non à la magnitude \n(ou à la longueur) d'un rayon. Nous ne nous soucions pas de la longueur du rayon.\n\nSi vous avez suivi des cours d'algèbre linéaire, vous devez savoir que vous pouvez utiliser une combinaison \nlinéaire de vecteurs de base pour former n'importe quel autre vecteur. \nDe même, nous pouvons multiplier un rayon normalisé par une valeur scalaire pour l'allonger, \nmais il reste dans la même direction.\n\n# L espace euclidien en 3D\n\nTout ce que nous avons dit sur les rayons en 2D s'applique également à la 3D. \nLa magnitude ou la longueur d'un rayon en 3D est définie par l'équation suivante.\n\nsqrt(x^2+y^2+z^2)\n\nDans l'espace euclidien 3D (l'espace 3D typique que vous avez probablement l'habitude d'utiliser à l'école),\nles vecteurs sont également une combinaison linéaire de vecteurs de base. \n\nVous pouvez utiliser une combinaison de vecteurs de base ou de vecteurs normalisés pour former un nouveau vecteur.\nhttps://inspirnathan.com/_nuxt/img/img-6.b5d04ce.png\n\nDans l'image ci-dessus, il y a trois axes, représentant l'axe des x (bleu), l'axe des y (rouge) et l'axe des z (vert).\nLes vecteurs i, j et k représentent les vecteurs de base fondamentaux (ou vecteurs unitaires) qui peuvent être combinés,\nrétrécis ou étirés pour créer un nouveau vecteur tel que le vecteur a qui a une composante x, une composante y et \nune composante z.\n\nN'oubliez pas que l'image ci-dessus n'est qu'une représentation de l'espace de coordonnées 3D.\nNous pouvons faire pivoter le système de coordonnées comme bon nous semble.\nTant que les trois axes restent perpendiculaires (ou orthogonaux) l'un à l'autre,\nl'arithmétique vectorielle reste la même.\n\nDans Shadertoy, il est très courant de créer un système de coordonnées dans lequel l'axe x suit l'axe horizontal du canvas,\nl'axe y suit l'axe vertical du canevas et l'axe z pointe vers vous ou s'éloigne de vous.\nhttps://inspirnathan.com/_nuxt/img/img-7.3f42e26.png\n\nRemarquez les couleurs que j'utilise dans l'image ci-dessus.\nL'axe des x est coloré en rouge, l'axe des y est coloré en vert et l'axe des z est coloré en bleu. \nC'est intentionnel. Comme indiqué dans la partie 1 de cette série de tutoriels, chaque axe correspond à \nune composante de couleur :\n\nvec3 someVariable = vec3(1, 2, 3);\n\nsomeVariable.r == someVariable.x\nsomeVariable.g == someVariable.y\nsomeVariable.b == someVariable.z\n\nDans l'image ci-dessus, l'axe des z est considéré comme positif lorsqu'il se rapproche de nous et négatif \nlorsqu'il s'en éloigne. \n\nCette convention utilise la règle de la main droite. En utilisant votre main droite, vous dirigez votre pouce vers la droite, \nvotre index vers le haut et votre majeur vers vous, de sorte que chacun de vos trois doigts pointe dans des directions\nperpendiculaires, comme dans un système de coordonnées. Chaque doigt pointe dans la direction positive.\n\nVous verrez parfois cette convention inversée le long de l'axe z lorsque vous lirez le code d'autres personnes \nou d'autres tutoriels en ligne. \nIls peuvent rendre l'axe z positif lorsqu'il s'éloigne de vous et négatif lorsqu'il se rapproche de vous,\nmais les axes x et y restent inchangés. C'est ce qu'on appelle la règle de la main gauche.\n\n\n# Les algorithmes de rayons\n\nParlons enfin des \"algorithmes de rayons\" tels que la marche et le traçage des rayons.\nLe raymarching est l'algorithme le plus couramment utilisé pour développer des scènes 3D dans Shadertoy,\nmais vous verrez que les gens utilisent également le ray tracing ou le path tracing.\n\nLe ray marching et le ray tracing sont tous deux des algorithmes utilisés pour dessiner des scènes 3D sur un écran 2D\nà l'aide de rayons. \nDans la vie réelle, les sources de lumière telles que le soleil projettent des rayons lumineux sous forme de photons \ndans des tonnes de directions différentes. Lorsqu'un photon touche un objet, l'énergie est absorbée par le réseau \ncristallin d'atomes de l'objet et un autre photon est libéré. \n\nEn fonction de la structure cristalline du réseau atomique du matériau, les photons peuvent être émis dans une \ndirection aléatoire (réflexion diffuse) ou sous le même angle qu'ils ont pénétré dans le matériau \n(réflexion spéculaire ou miroir).\n\nJe pourrais parler de physique toute la journée, mais ce qui nous intéresse,\nc'est le lien avec le raytracing et le raymarching. \n\nSi nous essayons de modéliser une scène en 3D en partant d'une source de lumière et en la traçant jusqu'à la caméra,\nnous aboutissons à un gaspillage de ressources informatiques. \n\nCette simulation \"vers l'avant\" ferait en sorte qu'un grand nombre de ces rayons n'atteindraient jamais la caméra.\n\nVous verrez surtout des simulations \"à l'envers\" où les rayons sont tirés à partir d'une caméra ou d'un \"œil\".\nNous travaillons à l'envers ! La lumière provient généralement d'une source lumineuse telle que le soleil, \nrebondit sur un certain nombre d'objets et frappe notre caméra.\n\nAu lieu de cela, notre caméra émet des rayons dans de nombreuses directions différentes.\nCes rayons rebondissent sur les objets de notre scène, y compris sur une surface telle que le sol, \net certains d'entre eux atteignent une source lumineuse. \n\nSi un rayon rebondit sur une surface et touche un objet au lieu de la source lumineuse,\nil est considéré comme un \"rayon d'ombre\" et nous indique que nous devons dessiner un pixel de couleur\nfoncée pour représenter l'ombre.\n\n\nhttps://inspirnathan.com/_nuxt/img/img-8.8da7582.png\n\nDans l'image ci-dessus, une caméra émet des rayons dans différentes directions.\nCombien de rayons ? Un pour chaque pixel de notre canvas ! \n\nNous utilisons chaque pixel du canvas Shadertoy pour générer un rayon. \nAstucieux, n'est-ce pas ? Chaque pixel possède une coordonnée le long de l'axe x et de l'axe y, \nalors pourquoi ne pas les utiliser pour créer des rayons avec une composante z ?\n\nCombien de directions différentes y aura-t-il ? Une pour chaque pixel également ! \nC'est pourquoi il est important de comprendre le fonctionnement des rayons.\n\nL'origine de chaque rayon émis par la caméra sera la même que la position de notre caméra. \nChaque rayon aura une direction avec une composante x, une composante y et une composante z.\n\nRemarquez l'origine des rayons d'ombre. L'origine des rayons d'ombre sera égale au point où le rayon de la caméra \na touché la surface. Chaque fois que le rayon touche une surface, nous pouvons simuler un \"rebond\" ou une \nréflexion en générant un nouveau rayon à partir de ce point.\n\nGardez cela à l'esprit lorsque nous parlerons de l'éclairage et des ombres.\n\n# Différence entre les algorithmes de rayons\n\nDiscutons de la différence entre tous les algorithmes de rayons que vous pouvez voir en ligne. \nIl s'agit notamment du ray casting, du ray tracing, du ray marching et du ray tracing.\n\nRay Casting :\nUne forme plus simple de ray tracing utilisée dans des jeux tels que Wolfenstein 3D et Doom,\nqui tire un seul rayon et s'arrête lorsqu'il atteint une cible.\n\nRay Marching : \nméthode de lancer de rayon qui utilise des champs de distance signés (SDF) et généralement un algorithme de traçage \nde sphère qui \"marche\" sur les rayons de façon incrémentielle jusqu'à ce qu'ils atteignent l'objet le plus proche.\n\nRay Tracing :\nUne version plus sophistiquée de la projection de rayons qui émet des rayons, \ncalcule les intersections rayon-surface et crée récursivement de nouveaux rayons à chaque réflexion.\n\nPath Tracing : \nUn type d'algorithme de traçage de rayon qui tire des centaines ou des milliers de rayons par pixel au lieu d'un seul.\nLes rayons sont tirés dans des directions aléatoires à l'aide de la méthode Monte Carlo,\net la couleur finale du pixel est déterminée à partir de l'échantillonnage des rayons qui atteignent la source lumineuse.\n\nSi vous voyez \"Monte Carlo\" quelque part, cela vous indique immédiatement que vous aurez probablement affaire\nà des mathématiques liées aux probabilités et aux statistiques.\n\nVous pouvez également entendre parler de \"sphere tracing\". \nIl y a une bonne discussion sur la différence entre le ray marching et le sphere tracing sur le Stack Exchange \nde l'infographie. \n\nFondamentalement, le sphere tracing est un type d'implémentation du ray marching. \n\nLa plupart des techniques de ray marching que vous voyez sur Shadertoy utilisent le sphere tracing,\nqui est toujours un type d'algorithme de ray marching.\nhttps://computergraphics.stackexchange.com/questions/161/what-is-ray-marching-is-sphere-tracing-the-same-thing/163\n\nAu cas où vous vous poseriez des questions sur l'orthographe, je vois souvent des personnes utiliser \"raymarching\" \nou \"raytracing\" comme un seul mot. \n\nLorsque vous recherchez des ressources sur ces sujets sur Google ou que vous utilisez Cmd+F (ou Ctrl+F) \npour rechercher toute référence à ray marching ou au ray tracing, gardez ceci à l'esprit.\n\n\n# Ray Marching\n\nDans la suite de cet article, je vais expliquer comment utiliser l'algorithme de ray marching dans Shadertoy. \nIl existe de nombreux excellents tutoriels en ligne qui enseignent le ray marching, comme ce tutoriel de Jamie Wong. \nhttp://jamie-wong.com/2016/07/15/ray-marching-signed-distance-functions/\n\nCe tutoriel sur Shadertoy est une ressource précieuse pour vous aider à visualiser le ray marching et à comprendre pourquoi \non l'appelle parfois le sphere tracing.\nhttps://www.shadertoy.com/view/4dSfRc\n\nJe vais vous aider à décomposer le processus de ray marching étape par étape, \nafin que vous puissiez commencer à créer des scènes 3D même avec très peu d'expérience dans le domaine de l'infographie.\n\nNous allons créer une caméra simple afin de pouvoir simuler une scène 3D dans le canevas Shadertoy.\nImaginons d'abord à quoi ressemblera notre scène. Nous commencerons par l'objet le plus basique : une sphère.\n\nhttps://inspirnathan.com/_nuxt/img/img-9.9a4b61e.png\n\nL'image ci-dessus montre une vue latérale de la scène 3D que nous allons créer dans Shadertoy. \nL'axe des x n'est pas représenté car il pointe vers l'observateur.\nNotre caméra sera traitée comme un point avec des coordonnées telles que (0, 0, 5), \nce qui signifie qu'elle se trouve à 5 unités du canevas le long de l'axe z.\nComme dans les tutoriels précédents, nous allons remapper les coordonnées UV de sorte que l'origine soit au \ncentre du canvas.\n\nhttps://inspirnathan.com/_nuxt/img/img-10.799a183.png\n\nL'image ci-dessus représente le canvas de notre point de vue avec un axe des x (rouge) et un axe des y (vert).\nNous observerons la scène du point de vue de la caméra. \n\nLe rayon qui sort directement de la caméra et qui passe par l'origine du canevas touchera notre sphère.\nLe rayon diagonal part de la caméra avec un angle et touche le sol (s'il existe dans la scène).\nSi le rayon ne touche rien, nous rendrons une couleur d'arrière-plan.\n\nMaintenant que nous comprenons ce que nous allons construire, commençons à coder ! \n\nCréez un nouveau shader Shadertoy et remplacez le contenu par ce qui suit pour configurer notre canvas :\n\n\n\n*/\n\n#if PART == 0\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n  vec2 uv = fragCoord/iResolution.xy; // <0, 1>\n  uv -= 0.5; // <-0.5,0.5>\n  uv.x *= iResolution.x/iResolution.y; // fix aspect ratio\n\n  vec3 col = vec3(0.);\n\n  // Output to screen\n  fragColor = vec4(col,1.0);\n}\n\n/*\nPour rendre notre code plus propre, nous pouvons remapper les coordonnées UV en une seule ligne au lieu de 3 lignes.\nNous sommes maintenant habitués à ce que fait ce code !\n*/\n\n#elif PART == 1\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n  vec2 uv = (fragCoord - .5 * iResolution.xy) / iResolution.y; // Condense 3 lines down to a single line!\n\n  vec3 col = vec3(0.);\n\n  // Output to screen\n  fragColor = vec4(col,1.0);\n}\n\n/*\nL'origine du rayon, ro, sera la position de notre caméra. \nNous la placerons à 5 unités derrière le \"canvas\" à travers lequel nous regardons.\n\nvec3 ro = vec3(0, 0, 5) ;\n\nEnsuite, nous ajouterons une direction de rayon, rd, qui changera en fonction des coordonnées des pixels.\nNous fixons la composante z à -1 pour que chaque rayon soit dirigé vers notre scène. \nNous allons ensuite normaliser l'ensemble du vecteur.\n\nvec3 rd = normalize(vec3(uv, -1));\n\nNous allons ensuite définir une variable qui renvoie la distance obtenue par l'algorithme de ray marching :\n\nfloat d = rayMarch(ro, rd, 0., 100.) ;\n\nCréons une fonction appelée rayMarch qui met en œuvre l'algorithme de ray marching :\n\nfloat rayMarch(vec3 ro, vec3 rd, float start, float end) {\n  float depth = start;\n  \n  for (int i = 0; i < 255; i++) {\n    vec3 p = ro + depth * rd;\n    float d = sdSphere(p, 1.);\n    depth += d;\n    if (d < 0.001 || depth > end) break;\n  }\n  \n  return depth;\n}\n\n\nExaminons de plus près l'algorithme de marche des rayons.\nNous commençons avec une profondeur de zéro et augmentons progressivement la profondeur. \nNotre point de test est égal à l'origine du rayon (la position de notre caméra) plus la profondeur multipliée \npar la direction du rayon. \n\nN'oubliez pas que l'algorithme de ray marching s'exécute pour chaque pixel et que chaque pixel détermine une direction \nde rayon différente.\n\nNous prenons le point de test, p, et le passons à la fonction sdSphere que nous définirons comme suit :\n\nfloat sdSphere(vec3 p, float r)\n{\n  return length(p) - r ; // p est le point de test et r le rayon de la sphère\n}\n\nNous incrémentons ensuite la profondeur de la valeur de la distance renvoyée par la fonction sdSphere. \nSi la distance se situe à moins de 0,001 unité de la sphère, nous considérons qu'elle est suffisamment proche de la sphère.\nCela représente une précision. Vous pouvez diminuer cette valeur si vous souhaitez obtenir une plus grande précision.\n\nSi la distance est supérieure à un certain seuil, 100 dans notre cas, le rayon est allé trop loin et nous devons arrêter\nla boucle de marche des rayons. \n\nNous ne voulons pas que le rayon continue jusqu'à l'infini, car c'est un gaspillage de ressources de calcul\net cela ferait tourner une boucle for à l'infini si le rayon ne touchait rien.\n\nEnfin, nous ajouterons une couleur selon que le rayon a touché quelque chose ou non :\n\nif (d > 100.0) {\n  col = vec3(0.6); // ray didn't hit anything\n} else {\n  col = vec3(0, 0, 1); // ray hit something\n}\n\nNotre code terminé devrait ressembler à ce qui suit :\n\n\n*/\n#elif PART == 2\n\nfloat sdSphere(vec3 p, float r )\n{\n  return length(p) - r;\n}\n\nfloat rayMarch(vec3 ro, vec3 rd, float start, float end) {\n  float depth = start;\n\n  for (int i = 0; i < 255; i++) {\n    vec3 p = ro + depth * rd;\n    float d = sdSphere(p, 1.);\n    depth += d;\n    if (d < 0.001 || depth > end) break;\n  }\n\n  return depth;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n  vec2 uv = (fragCoord-.5*iResolution.xy)/iResolution.y;\n\n  vec3 col = vec3(0);\n  vec3 ro = vec3(0, 0, 5); // ray origin that represents camera position\n  vec3 rd = normalize(vec3(uv, -1)); // ray direction\n\n  float d = rayMarch(ro, rd, 0., 100.); // distance to sphere\n\n  if (d > 100.0) {\n    col = vec3(0.6); // ray didn't hit anything\n  } else {\n    col = vec3(0, 0, 1); // ray hit something\n  }\n\n  // Output to screen\n  fragColor = vec4(col, 1.0);\n}\n\n/*\nNous semblons réutiliser certains chiffres, alors définissons quelques variables globales constantes.\nEn GLSL, nous pouvons utiliser le mot-clé const pour indiquer au compilateur que nous ne prévoyons pas\nde modifier ces variables :\n\nconst int MAX_MARCHING_STEPS = 255 ;\nconst float MIN_DIST = 0.0 ;\nconst float MAX_DIST = 100.0 ;\nconst float PRECISION = 0.001 ;\n\n\nIl est également possible d'utiliser les directives du préprocesseur.\nVous pouvez voir des personnes utiliser des directives de préprocesseur telles que #define lorsqu'elles définissent \ndes constantes. \n\nL'avantage d'utiliser #define est que vous pouvez utiliser #ifdef pour vérifier si une variable est définie plus loin \ndans votre code. \n\nIl existe des différences entre #define et const, alors choisissez celle que vous préférez et celle qui convient \nle mieux à votre scénario.\n\nSi nous réécrivions les variables constantes pour utiliser la directive #define du préprocesseur, nous aurions ce qui suit :\n\n#define MAX_MARCHING_STEPS 255\n#define MIN_DIST 0.0\n#define MAX_DIST 100.0\n#define PRECISION 0.001\n\nRemarquez que nous n'utilisons pas de signe égal ni de point-virgule à la fin de chaque ligne \nqui utilise une directive de préprocesseur.\n\nLe mot-clé #define nous permet de définir à la fois des variables et des fonctions, \nmais je préfère utiliser const pour des raisons de sécurité de type.\n\nEn utilisant ces variables globales constantes, le code devrait maintenant ressembler à ce qui suit :\n\n*/\n\n#elif PART == 3\n\nconst int MAX_MARCHING_STEPS = 255;\nconst float MIN_DIST = 0.0;\nconst float MAX_DIST = 100.0;\nconst float PRECISION = 0.001;\n\nfloat sdSphere(vec3 p, float r )\n{\n  return length(p) - r;\n}\n\nfloat rayMarch(vec3 ro, vec3 rd, float start, float end) {\n  float depth = start;\n\n  for (int i = 0; i < MAX_MARCHING_STEPS; i++) {\n    vec3 p = ro + depth * rd;\n    float d = sdSphere(p, 1.);\n    depth += d;\n    if (d < PRECISION || depth > end) break;\n  }\n\n  return depth;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n  vec2 uv = (fragCoord-.5*iResolution.xy)/iResolution.y;\n\n  vec3 col = vec3(0);\n  vec3 ro = vec3(0, 0, 5); // ray origin that represents camera position\n  vec3 rd = normalize(vec3(uv, -1)); // ray direction\n\n  float d = rayMarch(ro, rd, MIN_DIST, MAX_DIST); // distance to sphere\n\n  if (d > MAX_DIST) {\n    col = vec3(0.6); // ray didn't hit anything\n  } else {\n    col = vec3(0, 0, 1); // ray hit something\n  }\n\n  // Output to screen\n  fragColor = vec4(col, 1.0);\n}\n\n\n\n/*\nLorsque nous exécutons le code, nous devrions voir l'image d'une sphère. \nElle ressemble à un cercle, mais c'est bien une sphère !\nhttps://inspirnathan.com/_nuxt/img/img-11.52f5907.png\n\nSi nous changeons la position de la caméra, nous pouvons zoomer et dézoomer pour prouver que nous regardons un objet en 3D.\nEn faisant passer la distance entre la caméra et le canvas virtuelle de notre scène de 5 à 3, la sphère \ndevrait apparaître plus grande, comme si nous avions fait un pas en avant.\n\nvec3 ro = vec3(0, 0, 3); // ray origin\n\nhttps://inspirnathan.com/_nuxt/img/img-12.0934f44.png\n\nIl y a cependant un problème. Actuellement, le centre de notre sphère se trouve à la coordonnée (0, 0, 0),\nce qui est différent de l'image que j'ai présentée précédemment. \n\nNotre scène est configurée de telle sorte que la caméra est très proche de la sphère.\n\nhttps://inspirnathan.com/_nuxt/img/img-13.601def4.png\n\nAjoutons un décalage à la sphère, comme nous l'avons fait avec les cercles dans la partie 2 de ma série de tutoriels.\n\nfloat sdSphere(vec3 p, float r )\n{\n  vec3 offset = vec3(0, 0, -2);\n  return length(p - offset) - r;\n}\n\nLa sphère sera ainsi poussée de deux unités vers l'avant le long de l'axe z. \nLa sphère devrait apparaître plus petite puisqu'elle est maintenant plus éloignée de la caméra.\n\n\n# Lighting / éclairage\n\nPour que cette forme ressemble davantage à une sphère, nous devons ajouter de l'éclairage.\nDans le monde réel, les rayons lumineux sont diffusés par les objets dans des directions aléatoires.\n\nhttps://inspirnathan.com/_nuxt/img/img-14.796eb73.png\n\nLes objets apparaissent différemment selon qu'ils sont éclairés par une source lumineuse telle que le soleil.\n\nhttps://inspirnathan.com/_nuxt/img/img-15.7f2b697.png\n\nLes flèches noires dans l'image ci-dessus représentent quelques normales de surface de la sphère.\nSi la normale de la surface pointe vers la source lumineuse, cet endroit de la sphère apparaît plus clair \nque le reste de la sphère. \nSi la normale de la surface pointe complètement à l'opposé de la source lumineuse,\ncette partie de la sphère apparaît plus sombre.\n\nIl existe plusieurs types de modèles d'éclairage utilisés pour simuler le monde réel. \n\nNous allons examiner l'éclairage Lambert pour simuler la réflexion diffuse. \nPour ce faire, on utilise généralement le produit scalaire entre la direction du rayon d'une source lumineuse \net la direction d'une normale de surface.\n\nvec3 diffuseReflection = dot(normal, lightDirection) ; // (hrst4: pourquoi pas float?)\n\nUne normale de surface est généralement un vecteur normalisé, car seule la direction nous intéresse.\n\nPour trouver cette direction, nous devons utiliser le gradient. \n\nLa normale à la surface sera égale au gradient d'une surface en un point de la surface.\n\nTrouver le gradient revient à trouver la pente d'une ligne. \n\nÀ l'école, on vous a probablement appris à mémoriser l'expression \"s'élever plutôt que courir\" (?). \n\nDans l'espace de coordonnées 3D, nous pouvons utiliser le gradient pour trouver la \"direction\" vers laquelle pointe un\npoint de la surface.\n\nSi vous avez suivi un cours de calcul, vous avez probablement appris que la pente d'une ligne\nn'est en fait qu'une différence infiniment petite entre deux points de la ligne.\n\nhttps://inspirnathan.com/_nuxt/img/img-16.e54d45f.png\n\nTrouvons la pente en effectuant une \"montée sur la descente\" (?):\n\nPoint 1 = (1, 1)\nPoint 2 = (1.2, 1.2)\n\nRise / Run = (y2 - y1) / (x2 - x1) = (1.2 - 1) / (1.2 - 1) = 0.2 / 0.2 = 1\n\nTherefore, the slope is equal to one.\n\nPour trouver le gradient d'une surface, nous avons besoin de deux points.\nNous allons prendre un point sur la surface de la sphère et en soustraire un petit nombre pour obtenir le deuxième point.\n\nCela nous permettra de réaliser une astuce peu coûteuse pour trouver le gradient. \n\nNous pouvons ensuite utiliser la valeur de ce gradient comme normale à la surface.\n\nÉtant donné une surface, f(x,y,z), le gradient le long de la surface aura l'équation suivante :\n\nhttps://inspirnathan.com/_nuxt/img/img-17.19aef41.png\n\n\nLe symbole bouclé qui ressemble à la lettre \"e\" est la lettre grecque epsilon.\nIl représente une valeur minuscule à côté d'un point sur la surface de notre sphère.\n\nEn GLSL, nous allons créer une fonction appelée calcNormal qui prend en compte un point\nd'échantillonnage obtenu par la fonction rayMarch.\n\n\nvec3 calcNormal(vec3 p) {\n  float e = 0.0005; // epsilon\n  float r = 1.; // radius of sphere\n  return normalize(vec3(\n    sdSphere(vec3(p.x + e, p.y, p.z), r) - sdSphere(vec3(p.x - e, p.y, p.z), r),\n    sdSphere(vec3(p.x, p.y + e, p.z), r) - sdSphere(vec3(p.x, p.y - e, p.z), r),\n    sdSphere(vec3(p.x, p.y, p.z  + e), r) - sdSphere(vec3(p.x, p.y, p.z - e), r)\n  ));\n}\n\nNous pouvons en fait utiliser le Swizzling et l'arithmétique vectorielle pour créer une autre façon de calculer\nun petit gradient. \n\nN'oubliez pas que notre objectif est de créer un petit gradient entre deux points proches de la surface de la sphère \n(ou approximativement de la surface de la sphère). \n\nBien que cette nouvelle approche ne soit pas exactement la même que le code ci-dessus, \nelle fonctionne assez bien pour créer une petite valeur qui pointe approximativement dans la direction du vecteur normal.\nEn d'autres termes, elle permet de créer un gradient.\n\n\nvec3 calcNormal(vec3 p) {\n  vec2 e = vec2(1.0, -1.0) * 0.0005; // epsilon\n  float r = 1.; // radius of sphere\n  return normalize(\n    e.xyy * sdSphere(p + e.xyy, r) +\n    e.yyx * sdSphere(p + e.yyx, r) +\n    e.yxy * sdSphere(p + e.yxy, r) +\n    e.xxx * sdSphere(p + e.xxx, r));\n}\n\n\n## TIP\nSi vous souhaitez comparer les différences entre chaque implémentation de calcNormal,\nj'ai créé un petit programme JavaScript qui émule certains comportements du code GLSL.\nhttps://gist.github.com/inspirnathan/aec5d735194ba556cad69af15d76c831\n\n\nIl est important de comprendre que la fonction calcNormal renvoie une direction de rayon qui représente\nla direction vers laquelle un point de la sphère est orienté.\n\nEnsuite, nous devons définir la position de la source lumineuse.\n\nConsidérez-la comme un petit point dans l'espace 3D.\n\nvec3 lightPosition = vec3(2, 2, 4) ;\n\nPour l'instant, la source lumineuse est toujours dirigée vers la sphère. \nPar conséquent, la direction du rayon lumineux sera la différence entre la position de la lumière \net un point que nous obtenons en retour de la boucle de marche du rayon.\n\nvec3 lightDirection = normalize(lightPosition - p);\n\nPour connaître la quantité de lumière qui frappe la surface de notre sphère, \nnous devons calculer le produit scalaire. En GLSL, nous utilisons la fonction dot pour calculer cette valeur.\n\nfloat dif = dot(normal, lightDirection); // dif = diffuse reflection\n\nLorsque nous effectuons le produit point entre les vecteurs de la normale et de la direction de la lumière,\nil se peut que nous obtenions une valeur négative. \n\nPour maintenir la valeur entre zéro et un afin d'obtenir une plus grande plage de valeurs,\nnous pouvons utiliser la fonction clamp.\n\nfloat dif = clamp(dot(normal, lightDirection), 0., 1.);\n\n\nEn mettant tout cela bout à bout, nous obtenons le code suivant :\n\n*/\n\n#elif PART == 4\n\nconst int MAX_MARCHING_STEPS = 255;\nconst float MIN_DIST = 0.0;\nconst float MAX_DIST = 100.0;\nconst float PRECISION = 0.001;\n\nfloat sdSphere(vec3 p, float r )\n{\n  vec3 offset = vec3(0, 0, -2);\n  return length(p - offset) - r;\n}\n\nfloat rayMarch(vec3 ro, vec3 rd, float start, float end) {\n  float depth = start;\n\n  for (int i = 0; i < MAX_MARCHING_STEPS; i++) {\n    vec3 p = ro + depth * rd;\n    float d = sdSphere(p, 1.);\n    depth += d;\n    if (d < PRECISION || depth > end) break;\n  }\n\n  return depth;\n}\n\nvec3 calcNormal(vec3 p) {\n    vec2 e = vec2(1.0, -1.0) * 0.0005; // epsilon\n    float r = 1.; // radius of sphere\n    return normalize(\n      e.xyy * sdSphere(p + e.xyy, r) +\n      e.yyx * sdSphere(p + e.yyx, r) +\n      e.yxy * sdSphere(p + e.yxy, r) +\n      e.xxx * sdSphere(p + e.xxx, r));\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n  vec2 uv = (fragCoord-.5*iResolution.xy)/iResolution.y;\n\n  vec3 col = vec3(0);\n  vec3 ro = vec3(0, 0, 3); // ray origin that represents camera position\n  vec3 rd = normalize(vec3(uv, -1)); // ray direction\n\n  float d = rayMarch(ro, rd, MIN_DIST, MAX_DIST); // distance to sphere\n\n  if (d > MAX_DIST) {\n    col = vec3(0.6); // ray didn't hit anything\n  } else {\n    vec3 p = ro + rd * d; // point on sphere we discovered from ray marching\n    vec3 normal = calcNormal(p);\n    vec3 lightPosition = vec3(2, 2, 4);\n    vec3 lightDirection = normalize(lightPosition - p);\n\n    // Calculate diffuse reflection by taking the dot product of \n    // the normal and the light direction.\n    float dif = clamp(dot(normal, lightDirection), 0., 1.);\n\n    col = vec3(dif);\n  }\n\n  // Output to screen\n  fragColor = vec4(col, 1.0);\n}\n\n/*\nLorsque vous exécutez ce code, vous devriez voir une sphère éclairée !\nMaintenant, vous savez que je disais la vérité. Ça ressemble vraiment à une sphère maintenant !\nhttps://inspirnathan.com/_nuxt/img/img-18.6133fb6.png\n\nSi vous jouez avec la variable lightPosition, vous devriez pouvoir déplacer la lumière dans les coordonnées du monde 3D.\nLe déplacement de la lumière devrait influer sur l'intensité de l'ombrage de la sphère.\nSi vous déplacez la source lumineuse derrière la caméra, le centre de la sphère devrait être beaucoup plus lumineux.\n\nvec3 lightPosition = vec3(2, 2, 7);\n\nVous pouvez également modifier la couleur de la sphère en multipliant la valeur de la réflexion \ndiffuse par un vecteur de couleur :\n\ncol = vec3(dif) * vec3(1, 0.58, 0.29);\n\nhttps://inspirnathan.com/_nuxt/img/img-19.132e768.png\n\nSi vous souhaitez ajouter un peu de couleur à la lumière ambiante, \nvous pouvez ajuster la plage de clamp, de sorte que la sphère n'apparaisse pas complètement noire dans les zones ombrées :\n\nfloat dif = clamp(dot(normal, lightDirection), 0.3, 1.);\n\nVous pouvez également modifier la couleur de l'arrière-plan et ajouter un peu de cette couleur à celle de la sphère,\nafin qu'elle se fonde bien dans l'ensemble. \n\nCela ressemble un peu à l'image de référence que nous avons vue plus tôt dans ce tutoriel, non ?\n\nhttps://inspirnathan.com/_nuxt/img/img-20.b4069a7.png\n\nPour référence, voici le code complet que j'ai utilisé pour créer l'image ci-dessus.\n\n\n\n\n*/\n\n#elif PART == 5\n\nconst int MAX_MARCHING_STEPS = 255;\nconst float MIN_DIST = 0.0;\nconst float MAX_DIST = 100.0;\nconst float PRECISION = 0.001;\n\nfloat sdSphere(vec3 p, float r )\n{\n  vec3 offset = vec3(0, 0, -2);\n  return length(p - offset) - r;\n}\n\nfloat rayMarch(vec3 ro, vec3 rd, float start, float end) {\n  float depth = start;\n\n  for (int i = 0; i < MAX_MARCHING_STEPS; i++) {\n    vec3 p = ro + depth * rd;\n    float d = sdSphere(p, 1.);\n    depth += d;\n    if (d < PRECISION || depth > end) break;\n  }\n\n  return depth;\n}\n\nvec3 calcNormal(vec3 p) {\n    vec2 e = vec2(1.0, -1.0) * 0.0005; // epsilon\n    float r = 1.; // radius of sphere\n    return normalize(\n      e.xyy * sdSphere(p + e.xyy, r) +\n      e.yyx * sdSphere(p + e.yyx, r) +\n      e.yxy * sdSphere(p + e.yxy, r) +\n      e.xxx * sdSphere(p + e.xxx, r));\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n  vec2 uv = (fragCoord-.5*iResolution.xy)/iResolution.y;\n  vec3 backgroundColor = vec3(0.835, 1, 1);\n\n  vec3 col = vec3(0);\n  vec3 ro = vec3(0, 0, 3); // ray origin that represents camera position\n  vec3 rd = normalize(vec3(uv, -1)); // ray direction\n\n  float d = rayMarch(ro, rd, MIN_DIST, MAX_DIST); // distance to sphere\n\n  if (d > MAX_DIST) {\n    col = backgroundColor; // ray didn't hit anything\n  } else {\n    vec3 p = ro + rd * d; // point on sphere we discovered from ray marching\n    vec3 normal = calcNormal(p);\n    vec3 lightPosition = vec3(2, 2, 7);\n    vec3 lightDirection = normalize(lightPosition - p);\n\n    // Calculate diffuse reflection by taking the dot product of \n    // the normal and the light direction.\n    float dif = clamp(dot(normal, lightDirection), 0.3, 1.);\n\n    // Multiply the diffuse reflection value by an orange color and add a bit\n    // of the background color to the sphere to blend it more with the background.\n    col = dif * vec3(1, 0.58, 0.29) + backgroundColor * .2;\n  }\n\n  // Output to screen\n  fragColor = vec4(col, 1.0);\n}\n\n\n\n#endif\n\n\n","name":"Image","description":"","type":"image"}]}