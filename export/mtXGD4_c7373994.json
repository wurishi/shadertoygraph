{"ver":"0.1","info":{"id":"mtXGD4","date":"1671771156","viewed":84,"name":"backward_mapping_lkg_quilt _3","username":"holophone3d","description":"Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\nClick and hold mouse on the image and move left/right to view synthesized intermediate views\n\n","likes":0,"published":1,"flags":0,"usePreview":0,"tags":["backwardmapping"],"hasliked":0,"parentid":"mlXGW8","parentname":"backward_mapping_lkg_quilt _2"},"renderpass":[{"inputs":[],"outputs":[{"id":"4dfGRr","channel":0}],"code":"/*\nLOAD THE FOUR TEXTURES BELOW TO RUN THE SHADER:\n\nThe idea is to call directly the SetTexture function found in Shadertoy js code \n(https://www.shadertoy.com/view/lsGGDd)\n\nHere is how to loads the three textures needed for this shader:\n - Open the javascript console of your browser:\n\t\t\t\t   Mac      /     Windows\n\tChrome:  cmd + opt + J  /  ctrl + shift J\n\tFirefox: cmd + opt + K  /  ctrl + shift K\n    Edge:          na         /  ctrl + shift J   \n\n- Then copy the following lines in the console:\n\nHigh depth (inwards 3d):\ngShaderToy.SetTexture(0, {mSrc:'https://i.imgur.com/WyfxrfJ.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(1, {mSrc:'https://i.imgur.com/knCRQt5.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(2, {mSrc:'https://i.imgur.com/v961sp9.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\ngShaderToy.SetTexture(3, {mSrc:'https://i.imgur.com/xcgUn8T.png', mType:'texture', mID:1, mSampler:{ filter: 'mipmap', wrap: 'repeat', vflip:'true', srgb:'false', internal:'byte' }});\n\n- hit return to execute and load the textures.\n\n*/\n\n// DETAILS:\n// Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\n// POC shader to enable 3DS emulator Citra to natively render into Looking Glass hologram display\n// Uses a depth based warping and blending approach to create virtual frames\n// backward mapping - loosely based on the paper below\n// https://www.cc.gatech.edu/conferences/3DPVT08/Program/Papers/paper213.pdf\n\n// USAGE:\n// Synthesizes all views between L and R given a rectified stereo pair with depth maps (designed for 3ds)\n// Click and hold mouse on the image and move left/right to view synthesized intermediate views\n\n// NEED SOME MATH HELP/BRAINSTORMING HERE:\n// As you can see, I'm trying to determine the disparity between the rectified RGB+D stereo images\n// In theory, I have enough data to do this with the depth maps because the formula is: disparity = (focalLength * baseline) / depth\n// However, I think the normalized z data (0-1) isn't enough, I think I need it in world space\n// I think I just need a little math insight or brainstormin (maybe guessing at a projection matrix?)\n// Any thoughts or ideas are very welcome!\n\n// pixel depth offset values for high depth images\n// brute force mapping of offsets, need to determine how to generate these from depth information\n\nconst vec2 lut[96] = vec2[](vec2(37.0, 99), vec2(38.0, 99), vec2(39.0, 99), vec2(40.0, 99), vec2(41.0, 99), vec2(42.0, 99), vec2(43.0, 99), vec2(44.0, 99), vec2(45.0, 93), vec2(46.0, 93), vec2(47.0, 93), vec2(51.0, 93), vec2(55.0, 93), vec2(56.0, 93), vec2(60.0, 93), vec2(68.0, 93), vec2(70.0, 93), vec2(72.0, 93), vec2(75.0, 93), vec2(92.0, 93), vec2(99.0, 93), vec2(100.0, 93), vec2(102.0, 93), vec2(110.0, 85), vec2(116.0, 85), vec2(120.0, 85), vec2(121.0, 85), vec2(125.0, 85), vec2(128.0, 85), vec2(146.0, 85), vec2(149.0, 85), vec2(153.0, 85), vec2(155.0, 85), vec2(156.0, 85), vec2(157.0, 84), vec2(158.0, 83), vec2(159.0, 82), vec2(160.0, 82), vec2(161.0, 82), vec2(162.0, 82), vec2(163.0, 82), vec2(164.0, 82), vec2(165.0, 81), vec2(166.0, 81), vec2(167.0, 80), vec2(168.0, 80), vec2(169.0, 78), vec2(170.0, 78), vec2(171.0, 78), vec2(172.0, 78), vec2(173.0, 78), vec2(174.0, 78), vec2(175.0, 78), vec2(176.0, 75), vec2(177.0, 75), vec2(178.0, 73), vec2(179.0, 73), vec2(180.0, 73), vec2(181.0, 73), vec2(182.0, 73), vec2(183.0, 71), vec2(184.0, 71), vec2(185.0, 71), vec2(186.0, 71), vec2(187.0, 71), vec2(188.0, 71), vec2(189.0, 70), vec2(190.0, 70), vec2(191.0, 66), vec2(192.0, 66), vec2(193.0, 66), vec2(194.0, 66), vec2(195.0, 66), vec2(196.0, 62), vec2(197.0, 62), vec2(198.0, 62), vec2(199.0, 62), vec2(200.0, 62), vec2(201.0, 61), vec2(202.0, 61), vec2(203.0, 60), vec2(204.0, 58), vec2(205.0, 57), vec2(206.0, 57), vec2(207.0, 55), vec2(208.0, 54), vec2(209.0, 53), vec2(210.0, 51), vec2(211.0, 51), vec2(212.0, 50), vec2(213.0, 49), vec2(214.0, 47), vec2(215.0, 46), vec2(216.0, 45), vec2(217.0, 43), vec2(218.0, 42));\n\n\nfloat get_depth_shift(float depth)\n{  \n   bool use_lut = true;\n   float offset = 0.0;\n   if (use_lut)\n   {\n        for(int i=0; i < lut.length(); i++)\n        {\n            if (lut[i].x <= depth*255.0)\n            {\n                offset = float(lut[i].y);\n            }\n        }\n   }\n   else\n   {\n       depth = (1.0 - depth)*100.0;\n       offset = -0.25*(depth*depth) + 90.0*depth + 4000.0;\n       offset /= 100.0;\n   }\n   offset /= iChannelResolution[0].x;\n   return offset;\n}\n\nvec3 get_Im(vec2 normalized_coords, float alpha)\n{\n    float depth_l = texture(iChannel1, normalized_coords).x;\n    float depth_r = texture(iChannel3, normalized_coords).x;\n    float depth_shifted_x_l = normalized_coords.x - alpha*get_depth_shift(depth_l);\n    float depth_shifted_x_r = normalized_coords.x + (1.0-alpha)*get_depth_shift(depth_r);\n    // fix screen edge occlusion\n    if (depth_shifted_x_l < 0.0)\n    {\n        alpha = 1.0;\n    }\n    else if (depth_shifted_x_r > 1.0)\n    {\n        alpha = 0.0;\n    }\n    vec3 left_rgb = (1.0-alpha)*texture(iChannel0, vec2(depth_shifted_x_l, normalized_coords.y)).xyz;\n    vec3 right_rgb = alpha*texture(iChannel2, vec2(depth_shifted_x_r, normalized_coords.y)).xyz;\n    return left_rgb + right_rgb;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{        \n    vec2 normalized_coords = fragCoord.xy/iResolution.xy;\n    float alpha = iMouse.x/iResolution.x; // coefficient 0-1 of camera angle (L-R) DEBUG: use mouse to see warping and blending\n    \n    bool generate_quilt = false; // Looking Glass Mode    \n    bool use_expanded_alpha = false; // compresses the middle field of view to have more depth and creates more virtual frames outside of normal alpha range\n    \n    if (generate_quilt)\n    {\n        // quilt config\n        float cols = 8.0; \n        float rows = 6.0;\n        \n        // alter virtual camera angle for each quilt image\n        float alpha_delta = 1.0/(rows*cols); //step per quilt view\n        float active_row = floor(normalized_coords.y/(1.0/rows));\n        float active_col = floor(normalized_coords.x/(1.0/cols));\n        alpha = active_row*cols*alpha_delta + active_col*alpha_delta; // update alpha virtual camera angle based on quilt view\n              \n        // update coordinates for quilt space\n        normalized_coords = vec2(normalized_coords.x*cols - active_col, normalized_coords.y * rows);\n\n    }\n    \n    if(use_expanded_alpha)\n    {\n        alpha *= 2.0;\n        alpha -= 0.5;\n    }\n   \n    fragColor = vec4(get_Im(normalized_coords, alpha), 1.0);\n   \n}\n","name":"Image","description":"","type":"image"}]}