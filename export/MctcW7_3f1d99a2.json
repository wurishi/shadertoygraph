{"ver":"0.1","info":{"id":"MctcW7","date":"1730759350","viewed":99,"name":"Information Entropy Doodle","username":"rikr","description":"Just a doodle. Made by Rik Riesmeijer, 2024.","likes":3,"published":3,"flags":0,"usePreview":0,"tags":["cartoon","grass","field","doodle","theory","information"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[],"outputs":[{"id":"4dfGRr","channel":0}],"code":"// Information Entropy Doodle By Rik Riesmeijer - No rights reserved (CC0).\n\n//information measuring curve:\n    // Shanon Information Entropy Measure, for the notion of a bit of information:\n    //   1. Take the expected precision needed to encode the bit of information\n    //      by taking log2(1 / p) for probability p of encountering the information.\n    //   2. Then take the probability (p) that you will actually encounter that information.\n    //   3. Do then same, but for the equivalent version but from the negative perspective.\n    //      So reasoning about the same but for encoding the 0 bit instead of the 1.\n    //      Here we take binary so then other options can only give us (1-p) if not taking 1.\n    //   4. Calculate the average difference the bit makes when added some an encoding\n    //      of other bits that are already programmed to be encoded perfectly to convery\n    //      the bits of information.\n    //   5. Then sum this together to get the total utility of the new bit of information.\n    //   6. This is the entropy of the bit, which is to say that the entropy will be\n    //      measure as 1.0 if the new bit is perfectly random in the sense that encoding\n    //      it will only take a single new bit in the encoding that is almost always\n    //      used to encode something useful.\n    //      So this will mix maximized at p = 0.5, and very low near 0.0 and 1.0, as\n    //      0.0 means we will never encounter the information and 1.0 because the bit\n    //      is always 1, therefore wasting memory in the encoding.\n    //   7. So this means that we can also use this measure as guiding principle in the\n    //      sense that it can be a shaping function in designing information like in\n    //      the case of a shader here, where I wanted to shape visuals with it.\n    //      of this bit inside of the expected of the encoding.\n    \nfloat entropy(float p) {\n    p = max(0.001, min(0.999, p));\n    return p * -log2(p) + (1.0 - p) * -log2(1.0 - p);\n}\n\n// Demo doodle.\nfloat demofunc(float x, float t, float y) {\n    float aniheight =  cos(x + x - 1.4 * t);\n    float something =  entropy(fract(0.8 * x - 0.1 * t + 0.2 * aniheight));\n    float somethinq = -entropy(fract(1.2 * x - 0.1 * t - 0.2 * aniheight));\n    return smoothstep(0.3, 0.5, something + somethinq - y);\n}\n\n// Main entry function.\nvoid mainImage(out vec4 c, vec2 v) {\n    // Normalize screen coordinates.\n    v = v - 0.5 * iResolution.xy;\n    v = 3.3 * v / iResolution.y;\n    \n    // Doodle demo visual.\n    float t = iTime;\n    float x = v.x;\n    float y = v.y;\n    \n    // Final colors.\n    c =        vec4(0.000, 1.000, 0.800, 1.0);\n    c = mix(c, vec4(0.188, 0.769, 0.110, 1.0), demofunc(x * 7.0, t, 11.0 * y - 1.0));\n    c = mix(c, vec4(0.000, 0.741, 0.051, 1.0), demofunc(x * 4.0, t, 3.5 * y + 1.0));\n    c = mix(c, vec4(0.173, 0.627, 0.110, 1.0), demofunc(x * 3.0, t, 3.0 * y + 2.5));\n}","name":"Image","description":"","type":"image"}]}