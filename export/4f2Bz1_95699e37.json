{"ver":"0.1","info":{"id":"4f2Bz1","date":"1724660319","viewed":32,"name":"Programmatic SDF Transparency","username":"Quazerix","description":"A way to define points on the surface of an SDF that don't get rendered and have \"transparency\". This also allows for determining if a point lies on the inside or outside of a surface and color it based off of that.","likes":2,"published":1,"flags":48,"usePreview":0,"tags":["raymarching","noise","sdf"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dfGRr","channel":0}],"code":"/* USE ARROW KEYS TO CONTROL THE DIRECTION OF THE CAMERA (MODE 1) OR ORIENTATION OF THE SCENE (MODE 2)\n *\n * USE \"W\" AND \"S\" TO MOVE THE CAMERA FORWARD AND BACKWARD\n * (\"WALKS\" THE CAMERA IN MODE 1, CONTROLS \"ZOOM\" IN MODE 2)\n *\n * USE \"R\" TO RESET THE CAMERA POSITION\n * USE \"T\" TO TOGGLE BETWEEN CAMERA MODES (POSITIONS ARE PRESERVED BETWEEN MODES)\n *\n * SEE BUFFER A FOR MORE DETAILS\n */\n\n/* This started when I had the idea to try and render only half of an SDF and get a hollow shell.\n * I eventually settled on taking an extra step past the SDF if we were passing through a point that we don't want to render.\n * By keeping track of the number of times we do this, we can also detect if we're looking at the inside or outside surface\n * (even vs. odd number of steps through a surface), and have different colors based off of that.\n * This does mean that the count flips once we pass the boundary of an SDF, so colors can get swapped when moving the camera.\n * I have checks to correct that when passing through the outer sphere, but not the inner sphere or cylinder.\n *\n * FUTURE WORK: This is based on a global function that takes a point p and returns whether or not that point should\n *              be rendered. Using different functions per-object means I need to know something about where those\n *              objects are. It would be cool to be able to be able to specify something like a \"material\" for a given SDF\n *              to say \"this object should use this function\" and not have to worry about the location of that object.\n *              I need to brush up on the normal way to specify materials before trying to come up with ideas, though.\n *\n *              It would also be fun to play around and come up with functions that take in more than just p, such as the\n *              distance to the scene, the number of steps, etc.\n *\n *              It would be cool to use this idea to generate clouds (or similar), or to extend this so that the pixels that\n *              don't get rendered have actual transparency and affect the color of objects behind them.\n */\n \n \n /* Feel free to use this in your own projects with appropriate credits. */\n\n\n//\tClassic Perlin 3D Noise \n//\tby Stefan Gustavson (https://github.com/stegu/webgl-noise)\n//\nvec4 permute(vec4 x){return mod(((x*34.0)+1.0)*x, 289.0);}\nvec4 taylorInvSqrt(vec4 r){return 1.79284291400159 - 0.85373472095314 * r;}\nvec4 fade(vec4 t) {return t*t*t*(t*(t*6.0-15.0)+10.0);}\n\nfloat cnoise(vec4 P){\n  vec4 Pi0 = floor(P); // Integer part for indexing\n  vec4 Pi1 = Pi0 + 1.0; // Integer part + 1\n  Pi0 = mod(Pi0, 289.0);\n  Pi1 = mod(Pi1, 289.0);\n  vec4 Pf0 = fract(P); // Fractional part for interpolation\n  vec4 Pf1 = Pf0 - 1.0; // Fractional part - 1.0\n  vec4 ix = vec4(Pi0.x, Pi1.x, Pi0.x, Pi1.x);\n  vec4 iy = vec4(Pi0.yy, Pi1.yy);\n  vec4 iz0 = vec4(Pi0.zzzz);\n  vec4 iz1 = vec4(Pi1.zzzz);\n  vec4 iw0 = vec4(Pi0.wwww);\n  vec4 iw1 = vec4(Pi1.wwww);\n\n  vec4 ixy = permute(permute(ix) + iy);\n  vec4 ixy0 = permute(ixy + iz0);\n  vec4 ixy1 = permute(ixy + iz1);\n  vec4 ixy00 = permute(ixy0 + iw0);\n  vec4 ixy01 = permute(ixy0 + iw1);\n  vec4 ixy10 = permute(ixy1 + iw0);\n  vec4 ixy11 = permute(ixy1 + iw1);\n\n  vec4 gx00 = ixy00 / 7.0;\n  vec4 gy00 = floor(gx00) / 7.0;\n  vec4 gz00 = floor(gy00) / 6.0;\n  gx00 = fract(gx00) - 0.5;\n  gy00 = fract(gy00) - 0.5;\n  gz00 = fract(gz00) - 0.5;\n  vec4 gw00 = vec4(0.75) - abs(gx00) - abs(gy00) - abs(gz00);\n  vec4 sw00 = step(gw00, vec4(0.0));\n  gx00 -= sw00 * (step(0.0, gx00) - 0.5);\n  gy00 -= sw00 * (step(0.0, gy00) - 0.5);\n\n  vec4 gx01 = ixy01 / 7.0;\n  vec4 gy01 = floor(gx01) / 7.0;\n  vec4 gz01 = floor(gy01) / 6.0;\n  gx01 = fract(gx01) - 0.5;\n  gy01 = fract(gy01) - 0.5;\n  gz01 = fract(gz01) - 0.5;\n  vec4 gw01 = vec4(0.75) - abs(gx01) - abs(gy01) - abs(gz01);\n  vec4 sw01 = step(gw01, vec4(0.0));\n  gx01 -= sw01 * (step(0.0, gx01) - 0.5);\n  gy01 -= sw01 * (step(0.0, gy01) - 0.5);\n\n  vec4 gx10 = ixy10 / 7.0;\n  vec4 gy10 = floor(gx10) / 7.0;\n  vec4 gz10 = floor(gy10) / 6.0;\n  gx10 = fract(gx10) - 0.5;\n  gy10 = fract(gy10) - 0.5;\n  gz10 = fract(gz10) - 0.5;\n  vec4 gw10 = vec4(0.75) - abs(gx10) - abs(gy10) - abs(gz10);\n  vec4 sw10 = step(gw10, vec4(0.0));\n  gx10 -= sw10 * (step(0.0, gx10) - 0.5);\n  gy10 -= sw10 * (step(0.0, gy10) - 0.5);\n\n  vec4 gx11 = ixy11 / 7.0;\n  vec4 gy11 = floor(gx11) / 7.0;\n  vec4 gz11 = floor(gy11) / 6.0;\n  gx11 = fract(gx11) - 0.5;\n  gy11 = fract(gy11) - 0.5;\n  gz11 = fract(gz11) - 0.5;\n  vec4 gw11 = vec4(0.75) - abs(gx11) - abs(gy11) - abs(gz11);\n  vec4 sw11 = step(gw11, vec4(0.0));\n  gx11 -= sw11 * (step(0.0, gx11) - 0.5);\n  gy11 -= sw11 * (step(0.0, gy11) - 0.5);\n\n  vec4 g0000 = vec4(gx00.x,gy00.x,gz00.x,gw00.x);\n  vec4 g1000 = vec4(gx00.y,gy00.y,gz00.y,gw00.y);\n  vec4 g0100 = vec4(gx00.z,gy00.z,gz00.z,gw00.z);\n  vec4 g1100 = vec4(gx00.w,gy00.w,gz00.w,gw00.w);\n  vec4 g0010 = vec4(gx10.x,gy10.x,gz10.x,gw10.x);\n  vec4 g1010 = vec4(gx10.y,gy10.y,gz10.y,gw10.y);\n  vec4 g0110 = vec4(gx10.z,gy10.z,gz10.z,gw10.z);\n  vec4 g1110 = vec4(gx10.w,gy10.w,gz10.w,gw10.w);\n  vec4 g0001 = vec4(gx01.x,gy01.x,gz01.x,gw01.x);\n  vec4 g1001 = vec4(gx01.y,gy01.y,gz01.y,gw01.y);\n  vec4 g0101 = vec4(gx01.z,gy01.z,gz01.z,gw01.z);\n  vec4 g1101 = vec4(gx01.w,gy01.w,gz01.w,gw01.w);\n  vec4 g0011 = vec4(gx11.x,gy11.x,gz11.x,gw11.x);\n  vec4 g1011 = vec4(gx11.y,gy11.y,gz11.y,gw11.y);\n  vec4 g0111 = vec4(gx11.z,gy11.z,gz11.z,gw11.z);\n  vec4 g1111 = vec4(gx11.w,gy11.w,gz11.w,gw11.w);\n\n  vec4 norm00 = taylorInvSqrt(vec4(dot(g0000, g0000), dot(g0100, g0100), dot(g1000, g1000), dot(g1100, g1100)));\n  g0000 *= norm00.x;\n  g0100 *= norm00.y;\n  g1000 *= norm00.z;\n  g1100 *= norm00.w;\n\n  vec4 norm01 = taylorInvSqrt(vec4(dot(g0001, g0001), dot(g0101, g0101), dot(g1001, g1001), dot(g1101, g1101)));\n  g0001 *= norm01.x;\n  g0101 *= norm01.y;\n  g1001 *= norm01.z;\n  g1101 *= norm01.w;\n\n  vec4 norm10 = taylorInvSqrt(vec4(dot(g0010, g0010), dot(g0110, g0110), dot(g1010, g1010), dot(g1110, g1110)));\n  g0010 *= norm10.x;\n  g0110 *= norm10.y;\n  g1010 *= norm10.z;\n  g1110 *= norm10.w;\n\n  vec4 norm11 = taylorInvSqrt(vec4(dot(g0011, g0011), dot(g0111, g0111), dot(g1011, g1011), dot(g1111, g1111)));\n  g0011 *= norm11.x;\n  g0111 *= norm11.y;\n  g1011 *= norm11.z;\n  g1111 *= norm11.w;\n\n  float n0000 = dot(g0000, Pf0);\n  float n1000 = dot(g1000, vec4(Pf1.x, Pf0.yzw));\n  float n0100 = dot(g0100, vec4(Pf0.x, Pf1.y, Pf0.zw));\n  float n1100 = dot(g1100, vec4(Pf1.xy, Pf0.zw));\n  float n0010 = dot(g0010, vec4(Pf0.xy, Pf1.z, Pf0.w));\n  float n1010 = dot(g1010, vec4(Pf1.x, Pf0.y, Pf1.z, Pf0.w));\n  float n0110 = dot(g0110, vec4(Pf0.x, Pf1.yz, Pf0.w));\n  float n1110 = dot(g1110, vec4(Pf1.xyz, Pf0.w));\n  float n0001 = dot(g0001, vec4(Pf0.xyz, Pf1.w));\n  float n1001 = dot(g1001, vec4(Pf1.x, Pf0.yz, Pf1.w));\n  float n0101 = dot(g0101, vec4(Pf0.x, Pf1.y, Pf0.z, Pf1.w));\n  float n1101 = dot(g1101, vec4(Pf1.xy, Pf0.z, Pf1.w));\n  float n0011 = dot(g0011, vec4(Pf0.xy, Pf1.zw));\n  float n1011 = dot(g1011, vec4(Pf1.x, Pf0.y, Pf1.zw));\n  float n0111 = dot(g0111, vec4(Pf0.x, Pf1.yzw));\n  float n1111 = dot(g1111, Pf1);\n\n  vec4 fade_xyzw = fade(Pf0);\n  vec4 n_0w = mix(vec4(n0000, n1000, n0100, n1100), vec4(n0001, n1001, n0101, n1101), fade_xyzw.w);\n  vec4 n_1w = mix(vec4(n0010, n1010, n0110, n1110), vec4(n0011, n1011, n0111, n1111), fade_xyzw.w);\n  vec4 n_zw = mix(n_0w, n_1w, fade_xyzw.z);\n  vec2 n_yzw = mix(n_zw.xy, n_zw.zw, fade_xyzw.y);\n  float n_xyzw = mix(n_yzw.x, n_yzw.y, fade_xyzw.x);\n  return 2.2 * n_xyzw;\n}\n\n\nfloat smin(float a, float b, float k) {\n    k *= 1.0/(1.0-sqrt(0.5));\n    float h = max( k-abs(a-b), 0.0 )/k;\n    return min(a,b) - k*0.5*(1.0+h-sqrt(1.0-h*(h-2.0)));\n}\n\nfloat map(vec3 p) {\n    vec3 q = p;\n    float time = iTime*.2;\n    q.x -= sin(time*4.)*6.;\n    float d = length(q) - 2.;\n    float c = length(p.xz) - 1.;\n    d = smin(d, c, 1.);\n    d = max(-d, length(p) - 10.);\n    return d;\n}\n\n// Should this pixel be included in the surface?\nbool transparent(vec3 p) {\n    // Use Perlin noise for the large sphere, use sin(p.xy) for the small sphere and cylinder\n    return (length(p) > 9.9 && cnoise(vec4(p,iTime*.1+.6)) > sin(iTime*.5)*.2-.1) ||\n           (length(p) < 9.9 && (sin(p.x*10.+5.*iTime)<0. || sin(p.y*10.)<0.));\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    vec2 uv = (fragCoord * 2. - iResolution.xy) / iResolution.y;\n\n    // Initialization\n    float T = texelFetch(iChannel0, ivec2(6, 0), 0).r;                                   // Camera mode\n    vec3 rd = texelFetch(iChannel0, ivec2(fragCoord), 0).gba;                            // ray direction\n    vec3 ro = texelFetch(iChannel0, ivec2(1.-T, 0), 0).gba + vec3(0.,0.,(1.-T)*-3.1);    // ray origin\n\n    const float MAX_DIST = 30.;\n    float t = 0.; // total distance travelled\n    float i; // Number of steps\n    float d = MAX_DIST; // Distance to our scene\n    vec3 p;\n    \n    vec4 col=vec4(1.,1.,1.,float(length(ro)>9.99)); // Correct \"color-flipping\" when moving the camera into the larger sphere\n    \n    // Raymarching\n    for (i = 0.; i < 100. && t < MAX_DIST; i++) {\n        p = ro + rd * t;\n\n        d = map(p);\n        d = abs(d);  // To prevent the marching from backstepping once we've tricked it to go past the surface\n\n        t += d;\n        \n        if (d<0.01) {             // If we hit the surface...\n            if (transparent(p)) { // ...check if we should go through it\n\n                t+=.1;   // Step through the surface\n                \n                // Use the alpha channel to determine if we're inside an object. (Odd vs. even number of intersections.)\n                // This is used for knowing if we should shade it with the \"front\" or \"back\" color.\n                // (This also means that if the camera goes inside the boundary of one of the SDFs, the colors get flipped.)\n                col = vec4(1,1,1,1.-col.a); \n            } else {\n                break;\n            }\n        }\n        \n        // Epsilon and jump distance above are kept larger than one might normally use for raymarching applications\n        // because I got some odd artifacts when they were too small.\n        // They were cool in certain rendering styles, so I'll probably make a couple shaders showing that,\n        // but it's not what I was going for with this one.\n    }\n\n    // Coloring\n    if (d < .01) { // If we hit our scene, color it\n        // When the camera switches from inside to outside the large sphere, don't flip the \"back\" and \"front\" coloring.\n        bool ins = length(p) < 9.99;\n        if (ins) {\n            col.a = 1.-col.a;\n        }\n        \n        // Color based on if we're on the inside or outside of the surface (and which surface)\n        col = vec4(sin(p.x*2.), cos(p.z*3.), tan(p.y), 1.)*col.a + vec4(.5+.3*float(!ins),.6*float(!ins),0,1)*(1.-col.a);\n        col *= length(p)<9.99?1.:.4;\n    } else { // Otherwise, color the background\n        col = vec4(0.1);\n    }\n    \n    fragColor = vec4(col);\n}","name":"Image","description":"","type":"image"},{"inputs":[{"id":"4dXGRr","filepath":"/presets/tex00.jpg","previewfilepath":"/presets/tex00.jpg","type":"keyboard","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dXGR8","channel":0}],"code":"// Concept of storing keyboard-modified data in a buffer came from Assossa\n// https://www.shadertoy.com/view/ltsyRS\n//\n// But it is not a unique concept.\n// (E.g. \"Not today\" by ODtian - https://www.shadertoy.com/view/lcXyDj)\n\n/* This buffer stores the ray direction and ray origin for the camera in 2 different modes\n * 1. Free-cam (or 1st-person) has the camera rotate in place\n * 2. Arcball (or 3rd-person) has the camera rotate around (and always look at) a central point\n * Most of the pixels will store the ray direction, since that's different per-pixel\n * but we need to reserve 2 pixels to store data for the camera position for each mode.\n * I put these at the bottom right for minimum visual disturbance in the final scene.\n \n * CONTROLS:\n * \"W\" - Move the camera forward along its direction vector.\n * \"S\" - Move the camera backward along its direction vector.\n * UP ARROW - Increase the azimuthal angle of the camera. Capped at 89.9 degrees.\n * DOWN ARROW - Decrease the azimuthal angle of the camera. Capped at -89.9 degrees.\n * RIGHT ARROW - Increase the polar angle of the camera.\n * LEFT ARROW - Decrease the polar angle of the camera.\n * \"T\" - Toggle between the camera modes.\n * \"R\" - Reset the camera angle/position for the current camera mode.\n */\n \n \n \n /* Feel free to use this buffer in your own projects with appropriate credits. */\n\n\nconst int KEY_LEFT  = 37;\nconst int KEY_UP    = 38;\nconst int KEY_RIGHT = 39;\nconst int KEY_DOWN  = 40;\nconst int KEY_W     = 87;\nconst int KEY_S     = 83;\nconst int KEY_R     = 82;\nconst int KEY_T     = 84;\n\nconst float rotateSpeed = 100.0;\nconst float zoomSpeed = 20.0;\n\n// Offset along the negative z-axis for initial camera position.\n// Also dictates minumum radius for the arcball camera.\nconst float STARTING_CAMERA_DIST = 12.1;\nconst float MIN_CAMERA_DIST = 3.1;\n\nvec3 rotateVector(vec3 v, float theta, float phi) {\n    float thetaRad = -radians(theta); // Convert to radians (really just a scaling factor to limit the rotation speed)\n    float phiRad = radians(phi);     // Convert to radians (really just a scaling factor to limit the rotation speed)\n\n    // Rotation matrix around the Y-axis\n    mat3 R_y = mat3(\n        cos(thetaRad), 0, sin(thetaRad),\n        0, 1, 0,\n        -sin(thetaRad), 0, cos(thetaRad)\n    );\n\n    // Rotation matrix around the X-axis\n    mat3 R_x = mat3(\n        1, 0, 0,\n        0, cos(phiRad), -sin(phiRad),\n        0, sin(phiRad), cos(phiRad)\n    );\n\n    // Combined rotation matrix\n    mat3 R = R_y * R_x;\n\n    // Apply rotation to vector\n    return R * v;\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n    // Read data from the previous frame\n    float tf = texelFetch(iChannel0, ivec2(2, 0), 0).r; // Theta (free-cam)\n    float pf = texelFetch(iChannel0, ivec2(3, 0), 0).r; // Phi (free-cam)\n    float ta = texelFetch(iChannel0, ivec2(4, 0), 0).r; // Theta (arcball)\n    float pa = texelFetch(iChannel0, ivec2(5, 0), 0).r; // Phi (free-cam)\n\n    float t = texelFetch(iChannel1, ivec2(KEY_T, 2), 0).r; // Camera mode\n    t = 1.-t; // Uncomment this line to switch the starting camera mode on first load. Default is free-cam mode.\n\n    float theta = (tf*(1.-t))+(ta*t); // Theta for current camera mode\n    float phi   = (pf*(1.-t))+(pa*t); // Phi for current camera mode\n    \n    vec2 uv = (fragCoord * 2. - iResolution.xy) / iResolution.y;\n    vec3 rd = normalize(vec3(uv, 1));\n    vec3 currentPosition = rotateVector(rd, (1.-2.*t)*theta, (1.-2.*t)*phi); // Ray direction\n    float outData = 0.0; // Theta, phi, or radius\n    \n    if (int(fragCoord.y) == 0){\n        // Read data from the previous frame\n        float rad = texelFetch(iChannel0, ivec2(0, 0), 0).r; // Arcball radius/position\n        vec3 fp = texelFetch(iChannel0, ivec2(1, 0), 0).gba; // Free-cam position\n        \n        float r = texelFetch(iChannel1, ivec2(KEY_R, 1), 0).r; // If we need to reset the position/rotation\n        \n        // Calculate camera angle and position based on keyboard presses\n        switch(int(fragCoord.x)) {\n            case 0:\n                // Radius (arcball)\n                outData = (1.-r*t)*(rad +\n                    t*((iTimeDelta * zoomSpeed) * texelFetch(iChannel1, ivec2(KEY_S, 0), 0).r -\n                    (iTimeDelta * zoomSpeed) * texelFetch(iChannel1, ivec2(KEY_W, 0), 0).r));\n                outData = max(outData, MIN_CAMERA_DIST-STARTING_CAMERA_DIST);\n\n                // Position (arcball)\n                currentPosition = rotateVector(vec3(0, 0, -1), -theta, -phi)*(outData+STARTING_CAMERA_DIST);\n\n                break;\n            case 1:\n                // Position (free-cam)\n                currentPosition = fp + (1.-t)*(rotateVector(vec3(0.,0.,1.), theta, phi)*zoomSpeed/2.*\n                                     ((iTimeDelta) * texelFetch(iChannel1, ivec2(KEY_W, 0), 0).r -\n                                      (iTimeDelta) * texelFetch(iChannel1, ivec2(KEY_S, 0), 0).r));                      \n                                \n                if (r==1. && t==0.) { // Reset position if 'R' is pressed\n                    currentPosition = vec3(0,0,-STARTING_CAMERA_DIST);\n                }\n                \n                break;\n            case 2:\n                // Theta (freecam)\n                outData = (1.-r*(1.-t))*(tf +\n                    ((iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_RIGHT, 0), 0).r -\n                    (iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_LEFT, 0), 0).r) * (1.-t));\n                break;\n\n            case 3:\n                // Phi (freecam)\n                outData = (1.-r*(1.-t))*(pf +\n                    ((iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_UP, 0), 0).r -\n                    (iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_DOWN, 0), 0).r) * (1.-t));\n                outData = clamp(outData, -89.9, 89.9); // Used for camera angle, clamp to prevent camera from going upside-down\n                break;\n            case 4:\n                // Theta (arcball)\n                outData = (1.-r*t)*(ta +\n                    ((iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_RIGHT, 0), 0).r -\n                    (iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_LEFT, 0), 0).r) * t);\n                break;\n            case 5:\n                // Phi (arcball)\n                outData = (1.-r*t)*(pa +\n                    ((iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_UP, 0), 0).r -\n                    (iTimeDelta * rotateSpeed) * texelFetch(iChannel1, ivec2(KEY_DOWN, 0), 0).r) * t);\n                outData = clamp(outData, -89.9, 89.9); // Used for camera angle, clamp to prevent camera from going upside-down\n                break;\n            case 6:\n                outData = t;\n                break;\n        }\n    }\n    \n    // outData is theta, phi, or radius.\n    // currentPosition is ray direction or ray origin.\n    fragColor = vec4(outData, currentPosition);\n}","name":"Buffer A","description":"","type":"buffer"}]}