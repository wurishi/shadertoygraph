{"ver":"0.1","info":{"id":"MccyRs","date":"1731014703","viewed":41,"name":"Temporal Edges","username":"thedarkbunny","description":"Tried combining Gaussian blur with a time-causal limit kernel to make an edge-detection filter that works in time as well as space.  Results were mixed, but at least it looks cool.","likes":0,"published":1,"flags":32,"usePreview":0,"tags":["edgedetection","practice"],"hasliked":0,"parentid":"","parentname":""},"renderpass":[{"inputs":[{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dfGRr","channel":0}],"code":"\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    /*\n    // Normalized pixel coordinates (from 0 to 1)\n    vec2 uv = fragCoord/iResolution.xy;\n\n    // Time varying pixel color\n    vec3 col = 0.5 + 0.5*cos(iTime+uv.xyx+vec3(0,2,4));\n\n    // Output to screen\n    fragColor = (abs(vec4( hash(uv+iTime),hash(uv*iTime))));\n    */\n    \n    vec4 now = T(fragCoord);\n    \n    fragColor = pow(now*100.,vec4(2.2));\n    \n    \n}","name":"Image","description":"","type":"image"},{"inputs":[{"id":"4sX3Rn","filepath":"/media/a/c3a071ecf273428bc72fc72b2dd972671de8da420a2d4f917b75d20e1c24b34c.ogv","previewfilepath":"/media/ap/c3a071ecf273428bc72fc72b2dd972671de8da420a2d4f917b75d20e1c24b34c.ogv","type":"video","channel":1,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1},{"id":"4dXGR8","filepath":"/media/previz/buffer00.png","previewfilepath":"/media/previz/buffer00.png","type":"buffer","channel":0,"sampler":{"filter":"linear","wrap":"clamp","vflip":"true","srgb":"false","internal":"byte"},"published":1}],"outputs":[{"id":"4dXGR8","channel":0}],"code":"float dot2(vec2 a){\n    return dot(a,a);\n}\n\nvec4 blurred(vec2 coord){\n    coord /= iResolution.xy;\n    vec2 hstep = vec2(1./iResolution.x,0.);\n    vec2 vstep = vec2(0.,1./iResolution.y);\n    \n    return abs(\n          ( 273.*V(coord) \n            +37.*(\n                   V(coord-hstep)\n                  +V(coord+hstep)\n                  +V(coord-vstep)\n                  +V(coord+vstep)\n            )\n             +5.*(\n                   V(coord-hstep-vstep)\n                  +V(coord+hstep-vstep)\n                  +V(coord-hstep+vstep)\n                  +V(coord+hstep+vstep)\n            )\n           )/441. - V(coord) );\n\n\n}\n\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n\n    vec4 prev = (iFrame<3)?vec4(0):T(fragCoord);\n    //float signal = dot2(hash(fragCoord+iTime*vec2(47,31)));\n    float signal = dot(blurred(fragCoord).rgb,vec3(.2126,.7152,.0722));\n    vec4 now = vec4(0);\n    now.a = 0.395133093*prev.a + 0.604866907*signal;\n    now.b = 0.326547528*prev.b + 0.673452472*now.a;\n    now.r = 0.482343274*prev.r + 0.517656726*now.b;\n    now.g = 0.5        *prev.g + 0.5        *now.r;\n    \n    \n    \n    \n    fragColor = now;\n    \n    \n    \n}","name":"Buffer A","description":"","type":"buffer"},{"inputs":[],"outputs":[],"code":"#define T(I) texelFetch(iChannel0, ivec2(I) , 0)\n#define V(I) pow(texture(iChannel1,I),vec4(0.45))\n#define hash(p) ( 2.* fract(sin( (p) * mat2(127.1,311.7,269.5,183.3))*43758.5453123) -1.)","name":"Common","description":"","type":"common"}]}